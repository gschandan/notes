{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /jupyter/maths/linear-algebra/tensors/matrices/2022/04/30/intro-to-linear-algebra\n",
    "badges: true\n",
    "branch: main\n",
    "categories:\n",
    "- jupyter\n",
    "- maths\n",
    "- linear-algebra\n",
    "- tensors\n",
    "- matrices\n",
    "date: '2022-04-30'\n",
    "description: Examining some basic concepts of liner algebra using python libraries\n",
    "output-file: 2022-04-30-intro-to-linear-algebra.html\n",
    "title: Basic Linear Algebra Concepts\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    html-math-method: katex\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Linear Algebra\n",
    "\n",
    "References:\n",
    "1. https://github.com/jonkrohn/ML-foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar Tensors\n",
    "\n",
    "\n",
    "Pytorch tensors can be used for operations on the GPU or CPU, unlike numpy arrays, by casting the tensor to a CUDA data type. \n",
    "\n",
    "Tensorflow tensors are also immutable, and created using wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.int64, Shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Example scalar pytorch tensor\n",
    "x = torch.tensor(10, dtype=torch.int64)\n",
    "print(f\"Type: {x.dtype}, Shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 12:45:13.935875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 12:45:13.978973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 12:45:13.979100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 12:45:13.979916: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-02 12:45:13.980452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 12:45:13.980572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 12:45:13.980671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 12:45:14.569116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 12:45:14.569252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 12:45:14.569354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 12:45:14.569643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6540 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example scalar tensorflow tensor\n",
    "x = tf.constant(10, dtype=tf.int64)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Transposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [[5 6 7]] \n",
      "Shape = (1, 3)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[5,6,7]]) #Need extra brackets else shape will be (,3) (1D, not 2D)\n",
    "print(f\"x = {x} \\nShape = {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x transposed:\n",
      "[[5]\n",
      " [6]\n",
      " [7]] \n",
      "Shape = (3, 1)\n"
     ]
    }
   ],
   "source": [
    "x_t = x.T\n",
    "print(f\"x transposed:\\n{x_t} \\nShape = {x_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([5, 6, 7]) \n",
      "Shape = torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5,6,7])\n",
    "print(f\"x = {x} \\nShape = {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.488088481701515"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L2 Norm - euclidean distance\n",
    "x = np.array([[5,6,7]])\n",
    "np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 Norm\n",
    "np.sum(np.absolute(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Squared L2 Norm\n",
    "x = np.array([5,6,7])\n",
    "np.dot(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max norm\n",
    "np.max(np.abs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Orthogonal\n",
    "i = np.asarray([1,0])\n",
    "j = np.asarray([0,1])\n",
    "np.dot(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices (2-Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  3]\n",
      " [ 6  8]\n",
      " [ 2 10]]\n",
      " Shape: (3, 2) Size: 6\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[5,3],[6,8],[2,10]])\n",
    "print(f\"{X}\\n Shape: {X.shape} Size: {X.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0] # left column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1,:] # middle row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  3],\n",
      "        [ 6,  8],\n",
      "        [ 2, 10]])\n",
      " Shape: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "#pytorch\n",
    "X = torch.tensor([[5,3],[6,8],[2,10]])\n",
    "print(f\"{X}\\n Shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 2 Shape: [3 2]\n"
     ]
    }
   ],
   "source": [
    "#Tensorflow\n",
    "X = tf.constant([[5,3],[6,8],[2,10]])\n",
    "print(f\"Rank: {tf.rank(X)} Shape: {tf.shape(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 4, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.zeros([5,2,4,2])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 4\n"
     ]
    }
   ],
   "source": [
    "X = tf.zeros([5,2,4,2])\n",
    "print(f\"Rank: {tf.rank(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X*2 = \n",
      "[[10  6]\n",
      " [12 16]\n",
      " [ 4 20]]\n",
      "  X+2 =\n",
      "[[ 7  5]\n",
      " [ 8 10]\n",
      " [ 4 12]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[5,3],[6,8],[2,10]])\n",
    "print(f\"X*2 = \\n{X*2}\\n  X+2 =\\n{X+2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X*2 = \n",
      "tensor([[10,  6],\n",
      "        [12, 16],\n",
      "        [ 4, 20]])\n",
      "  X+2 =\n",
      "tensor([[ 7,  5],\n",
      "        [ 8, 10],\n",
      "        [ 4, 12]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[5,3],[6,8],[2,10]])\n",
    "print(f\"X*2 = \\n{torch.mul(X,2)}\\n  X+2 =\\n{torch.add(X,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elementwise Product  \n",
    "\n",
    "If tensors have the same size, operations can be aplied elementwise, this is the Hadamard product: $$\\boldsymbol X \\odot \\boldsymbol Y$$ It produces a tensor that is the same shape as the input, unlike the dot product which will produce a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 50  24]\n",
      " [ 66 104]\n",
      " [ 14 150]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[5,3],[6,8],[2,10]])\n",
    "Y = X + 5\n",
    "print(X*Y) ##dot product, not matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 tensor(34)\n",
      "tf.Tensor(34, shape=(), dtype=int32)\n",
      "[13 21] [ 8 14 12]\n",
      "tensor([13, 21])\n",
      "tensor([ 8, 14, 12])\n",
      "tf.Tensor([13 21], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 8 14 12], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(X.sum(), torch.sum(torch.tensor([[5,3],[6,8],[2,10]])))\n",
    "print(tf.reduce_sum(tf.constant([[5,3],[6,8],[2,10]])))\n",
    "print(X.sum(axis=0), X.sum(axis=1)) #along specific axes\n",
    "print(torch.sum(torch.tensor([[5,3],[6,8],[2,10]]),0))\n",
    "print(torch.sum(torch.tensor([[5,3],[6,8],[2,10]]),1))\n",
    "print(tf.reduce_sum(tf.constant([[5,3],[6,8],[2,10]]),0))\n",
    "print(tf.reduce_sum(tf.constant([[5,3],[6,8],[2,10]]),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Product\n",
    "\n",
    "The vectors must be of the same length or shape for the element-wise multiplication to occurr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272\n"
     ]
    }
   ],
   "source": [
    "X = np.array([5,3,6,8,2,10])\n",
    "Y = X + 1\n",
    "print(np.dot(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(272)\n"
     ]
    }
   ],
   "source": [
    "print(torch.dot(torch.tensor([5,3,6,8,2,10]),torch.add(torch.tensor([5,3,6,8,2,10]),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(272, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_sum(tf.multiply(tf.constant([5,3,6,8,2,10]),tf.add(tf.constant([5,3,6,8,2,10]),1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "#### Frobenius Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.427248620541512\n",
      "tensor(15.4272)\n",
      "tf.Tensor(15.427248, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[5,3],[6,8],[2,10]])\n",
    "print(np.linalg.norm(X))\n",
    "print(torch.norm(torch.tensor([[5.,3],[6,8],[2,10]])))\n",
    "print(tf.norm(tf.constant([[5.,3],[6,8],[2,10]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 17, 23])\n",
      "tf.Tensor([11 17 23], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[3,4],[5,6],[7,8]])\n",
    "b = np.array([1,2])\n",
    "print(np.dot(A,b)) #infers dot product vs matrix multiplication based on shape\n",
    "C = torch.tensor([[3,4],[5,6],[7,8]])\n",
    "d = torch.tensor([1,2])\n",
    "print(torch.matmul(C,d))\n",
    "E = tf.constant([[3,4],[5,6],[7,8]])\n",
    "f = tf.constant([1,2])\n",
    "print(tf.linalg.matvec(E,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 27]\n",
      " [17 45]\n",
      " [23 63]]\n",
      "tensor([[11, 27],\n",
      "        [17, 45],\n",
      "        [23, 63]])\n",
      "tf.Tensor(\n",
      "[[11 27]\n",
      " [17 45]\n",
      " [23 63]], shape=(3, 2), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[3,4],[5,6],[7,8]])\n",
    "b = np.array([[1,9],[2,0]])\n",
    "print(np.dot(A,b)) \n",
    "C = torch.tensor([[3,4],[5,6],[7,8]])\n",
    "d = torch.tensor([[1,9],[2,0]])\n",
    "print(torch.matmul(C,d))\n",
    "E = tf.convert_to_tensor(A)\n",
    "f = tf.convert_to_tensor(b)\n",
    "print(tf.matmul(E,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  4.]\n",
      "[ 4. -7.]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[4,2],[-5,-3]])\n",
    "X_inv = np.linalg.inv(X)\n",
    "y = np.array([4,-7])\n",
    "print(w:= np.dot(X_inv,y))\n",
    "print(np.dot(X,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
