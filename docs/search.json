[
  {
    "objectID": "posts/2024-09-01-binary-tree-inversion.html",
    "href": "posts/2024-09-01-binary-tree-inversion.html",
    "title": "Inverting a Binary Tree",
    "section": "",
    "text": "The intuition for inverting a binary tree is to swap the left and right children of each node recursively or iteratively. We can visualize this as flipping the tree horizontally, where each node‚Äôs left child becomes its right child and vice versa. The key part for me, which may seem obvious, was realising that a binary tree is a data structure where each node has at most two child nodes.\n\n\n\nInitially solved with a recursive approach based on the above intuition:\n1. Start with the root node and define the base cases.\n1. If the root is None (empty tree), return None.\n2. If the root does not have 2 children, it is a leaf node -&gt; return the root as is.\n2. For non-leaf nodes, recursively invert the left and right subtrees.\n3. Swap the inverted left and right subtrees.\n4. Return the root of the inverted tree.\n\n\n\n\nTime complexity: O(n)\nWe visit each node in the tree exactly once, where n is the number of nodes in the tree. Therefore, the time complexity is linear.\nSpace complexity: O(h)\nThe space complexity is O(h), where h is the height of the tree. This is due to the recursive call stack. In the worst case (a completely unbalanced tree), this could be O(n), but for a balanced tree, it would be O(\\log n).\n\n\n\n\n\nfrom typing import Optional\n\nclass TreeNode:\n    def __init__(self, val: int=0, left: int|None = None, right: int|None =None):\n        self.val = val\n        self.left = left\n        self.right = right\n\nclass Solution:\n    def invert_tree_recursive(self, root: Optional[TreeNode]) -&gt; Optional[TreeNode]:\n        \n        if not root:\n            return None\n        if not root.right and not root.left:\n            return root\n        \n        root.left, root.right = self.invert_tree_recursive(root.right), self.invert_tree_recursive(root.left)\n        return root\n\ndef print_tree(node: TreeNode, level: int=0, prefix: str=\"Root: \"):\n    if node is not None:\n        print(\" \" * (level * 4) + prefix + str(node.val))\n        if node.left or node.right:\n            print_tree(node.left, level + 1, \"L--- \")\n            print_tree(node.right, level + 1, \"R--- \")\n\nroot = TreeNode(4)\nroot.left = TreeNode(2)\nroot.right = TreeNode(7)\nroot.left.left = TreeNode(1)\nroot.left.right = TreeNode(3)\nroot.right.left = TreeNode(6)\nroot.right.right = TreeNode(9)\n\nprint(\"Original Tree:\")\nprint_tree(root)\n\nsolution = Solution()\ninverted_root = solution.invert_tree_recursive(root)\n\n\nprint(\"\\nInverted Tree:\")\nprint_tree(inverted_root)\n\nOriginal Tree:\nRoot: 4\n    L--- 2\n        L--- 1\n        R--- 3\n    R--- 7\n        L--- 6\n        R--- 9\n\nInverted Tree:\nRoot: 4\n    L--- 7\n        L--- 9\n        R--- 6\n    R--- 2\n        L--- 3\n        R--- 1\n\n\n\n\n\nFor an interative appraoch, a stack or queue works nicely as we can do the following:\n1. Start with the root node and push it onto the stack\n2. Take a node from the stack\n3. Swap the left and right children\n4. Add the non-None children to the stack for processing\n\n\n\n\nTime complexity: O(n)\nWe visit each node in the tree exactly once, where n is the number of nodes in the tree, performing a constant time operation (swapping the nodes), and the while loop runs n times. Therefore, the time complexity is linear.\nSpace complexity: O(n)\n\nThis occurs in a ‚Äúperfect‚Äù binary tree, where the last level is completely full.\n\nAt its maximum, the queue could contain all the leaf nodes, which in a perfect binary tree is (n+1)/2 nodes (slightly more than half of all nodes).\n\nFor a balanced tree, it could be O(w), where w is the maximum width of the tree (which is typically O(\\log n) for a balanced tree).\nFor a skewed tree (e.g.¬†a linked list), it would be O(1) as we‚Äôd only ever have one node in the queue at a time.\n\n\n\n\n\n\nfrom typing import Optional\n\nclass TreeNode:\n    def __init__(self, val: int=0, left: int|None = None, right: int|None =None):\n        self.val = val\n        self.left = left\n        self.right = right\n\nclass Solution:\n    def invert_tree_iterative(self, root: Optional[TreeNode]) -&gt; Optional[TreeNode]:\n        if not root:\n            return None\n        \n        nodes_to_process = [root]\n        while nodes_to_process:\n            current = nodes_to_process.pop(0)\n            current.left, current.right = current.right, current.left\n            if current.left:\n                nodes_to_process.append(current.left)\n            if current.right:\n                nodes_to_process.append(current.right)\n        \n        return root\n\ndef print_tree(node: TreeNode, level: int=0, prefix: str=\"Root: \"):\n    if node is not None:\n        print(\" \" * (level * 4) + prefix + str(node.val))\n        if node.left or node.right:\n            print_tree(node.left, level + 1, \"L--- \")\n            print_tree(node.right, level + 1, \"R--- \")\n\nroot = TreeNode(4)\nroot.left = TreeNode(2)\nroot.right = TreeNode(7)\nroot.left.left = TreeNode(1)\nroot.left.right = TreeNode(3)\nroot.right.left = TreeNode(6)\nroot.right.right = TreeNode(9)\n\nprint(\"Original Tree:\")\nprint_tree(root)\n\nsolution = Solution()\ninverted_root = solution.invert_tree_iterative(root)\n\n\nprint(\"\\nInverted Tree:\")\nprint_tree(inverted_root)\n\nOriginal Tree:\nRoot: 4\n    L--- 2\n        L--- 1\n        R--- 3\n    R--- 7\n        L--- 6\n        R--- 9\n\nInverted Tree:\nRoot: 4\n    L--- 7\n        L--- 9\n        R--- 6\n    R--- 2\n        L--- 3\n        R--- 1\n\n\n\n\n\n[^1] LeetCode 226 Invert Binary Tree"
  },
  {
    "objectID": "posts/2024-09-01-binary-tree-inversion.html#intuition",
    "href": "posts/2024-09-01-binary-tree-inversion.html#intuition",
    "title": "Inverting a Binary Tree",
    "section": "",
    "text": "The intuition for inverting a binary tree is to swap the left and right children of each node recursively or iteratively. We can visualize this as flipping the tree horizontally, where each node‚Äôs left child becomes its right child and vice versa. The key part for me, which may seem obvious, was realising that a binary tree is a data structure where each node has at most two child nodes."
  },
  {
    "objectID": "posts/2024-09-01-binary-tree-inversion.html#recursive-approach",
    "href": "posts/2024-09-01-binary-tree-inversion.html#recursive-approach",
    "title": "Inverting a Binary Tree",
    "section": "",
    "text": "Initially solved with a recursive approach based on the above intuition:\n1. Start with the root node and define the base cases.\n1. If the root is None (empty tree), return None.\n2. If the root does not have 2 children, it is a leaf node -&gt; return the root as is.\n2. For non-leaf nodes, recursively invert the left and right subtrees.\n3. Swap the inverted left and right subtrees.\n4. Return the root of the inverted tree."
  },
  {
    "objectID": "posts/2024-09-01-binary-tree-inversion.html#complexity",
    "href": "posts/2024-09-01-binary-tree-inversion.html#complexity",
    "title": "Inverting a Binary Tree",
    "section": "",
    "text": "Time complexity: O(n)\nWe visit each node in the tree exactly once, where n is the number of nodes in the tree. Therefore, the time complexity is linear.\nSpace complexity: O(h)\nThe space complexity is O(h), where h is the height of the tree. This is due to the recursive call stack. In the worst case (a completely unbalanced tree), this could be O(n), but for a balanced tree, it would be O(\\log n)."
  },
  {
    "objectID": "posts/2024-09-01-binary-tree-inversion.html#code",
    "href": "posts/2024-09-01-binary-tree-inversion.html#code",
    "title": "Inverting a Binary Tree",
    "section": "",
    "text": "from typing import Optional\n\nclass TreeNode:\n    def __init__(self, val: int=0, left: int|None = None, right: int|None =None):\n        self.val = val\n        self.left = left\n        self.right = right\n\nclass Solution:\n    def invert_tree_recursive(self, root: Optional[TreeNode]) -&gt; Optional[TreeNode]:\n        \n        if not root:\n            return None\n        if not root.right and not root.left:\n            return root\n        \n        root.left, root.right = self.invert_tree_recursive(root.right), self.invert_tree_recursive(root.left)\n        return root\n\ndef print_tree(node: TreeNode, level: int=0, prefix: str=\"Root: \"):\n    if node is not None:\n        print(\" \" * (level * 4) + prefix + str(node.val))\n        if node.left or node.right:\n            print_tree(node.left, level + 1, \"L--- \")\n            print_tree(node.right, level + 1, \"R--- \")\n\nroot = TreeNode(4)\nroot.left = TreeNode(2)\nroot.right = TreeNode(7)\nroot.left.left = TreeNode(1)\nroot.left.right = TreeNode(3)\nroot.right.left = TreeNode(6)\nroot.right.right = TreeNode(9)\n\nprint(\"Original Tree:\")\nprint_tree(root)\n\nsolution = Solution()\ninverted_root = solution.invert_tree_recursive(root)\n\n\nprint(\"\\nInverted Tree:\")\nprint_tree(inverted_root)\n\nOriginal Tree:\nRoot: 4\n    L--- 2\n        L--- 1\n        R--- 3\n    R--- 7\n        L--- 6\n        R--- 9\n\nInverted Tree:\nRoot: 4\n    L--- 7\n        L--- 9\n        R--- 6\n    R--- 2\n        L--- 3\n        R--- 1"
  },
  {
    "objectID": "posts/2024-09-01-binary-tree-inversion.html#iterative-approach",
    "href": "posts/2024-09-01-binary-tree-inversion.html#iterative-approach",
    "title": "Inverting a Binary Tree",
    "section": "",
    "text": "For an interative appraoch, a stack or queue works nicely as we can do the following:\n1. Start with the root node and push it onto the stack\n2. Take a node from the stack\n3. Swap the left and right children\n4. Add the non-None children to the stack for processing"
  },
  {
    "objectID": "posts/2024-09-01-binary-tree-inversion.html#complexity-1",
    "href": "posts/2024-09-01-binary-tree-inversion.html#complexity-1",
    "title": "Inverting a Binary Tree",
    "section": "",
    "text": "Time complexity: O(n)\nWe visit each node in the tree exactly once, where n is the number of nodes in the tree, performing a constant time operation (swapping the nodes), and the while loop runs n times. Therefore, the time complexity is linear.\nSpace complexity: O(n)\n\nThis occurs in a ‚Äúperfect‚Äù binary tree, where the last level is completely full.\n\nAt its maximum, the queue could contain all the leaf nodes, which in a perfect binary tree is (n+1)/2 nodes (slightly more than half of all nodes).\n\nFor a balanced tree, it could be O(w), where w is the maximum width of the tree (which is typically O(\\log n) for a balanced tree).\nFor a skewed tree (e.g.¬†a linked list), it would be O(1) as we‚Äôd only ever have one node in the queue at a time."
  },
  {
    "objectID": "posts/2024-09-01-binary-tree-inversion.html#code-1",
    "href": "posts/2024-09-01-binary-tree-inversion.html#code-1",
    "title": "Inverting a Binary Tree",
    "section": "",
    "text": "from typing import Optional\n\nclass TreeNode:\n    def __init__(self, val: int=0, left: int|None = None, right: int|None =None):\n        self.val = val\n        self.left = left\n        self.right = right\n\nclass Solution:\n    def invert_tree_iterative(self, root: Optional[TreeNode]) -&gt; Optional[TreeNode]:\n        if not root:\n            return None\n        \n        nodes_to_process = [root]\n        while nodes_to_process:\n            current = nodes_to_process.pop(0)\n            current.left, current.right = current.right, current.left\n            if current.left:\n                nodes_to_process.append(current.left)\n            if current.right:\n                nodes_to_process.append(current.right)\n        \n        return root\n\ndef print_tree(node: TreeNode, level: int=0, prefix: str=\"Root: \"):\n    if node is not None:\n        print(\" \" * (level * 4) + prefix + str(node.val))\n        if node.left or node.right:\n            print_tree(node.left, level + 1, \"L--- \")\n            print_tree(node.right, level + 1, \"R--- \")\n\nroot = TreeNode(4)\nroot.left = TreeNode(2)\nroot.right = TreeNode(7)\nroot.left.left = TreeNode(1)\nroot.left.right = TreeNode(3)\nroot.right.left = TreeNode(6)\nroot.right.right = TreeNode(9)\n\nprint(\"Original Tree:\")\nprint_tree(root)\n\nsolution = Solution()\ninverted_root = solution.invert_tree_iterative(root)\n\n\nprint(\"\\nInverted Tree:\")\nprint_tree(inverted_root)\n\nOriginal Tree:\nRoot: 4\n    L--- 2\n        L--- 1\n        R--- 3\n    R--- 7\n        L--- 6\n        R--- 9\n\nInverted Tree:\nRoot: 4\n    L--- 7\n        L--- 9\n        R--- 6\n    R--- 2\n        L--- 3\n        R--- 1"
  },
  {
    "objectID": "posts/2024-09-01-binary-tree-inversion.html#references",
    "href": "posts/2024-09-01-binary-tree-inversion.html#references",
    "title": "Inverting a Binary Tree",
    "section": "",
    "text": "[^1] LeetCode 226 Invert Binary Tree"
  },
  {
    "objectID": "posts/2022-04-30-linear-algebra-intro.html",
    "href": "posts/2022-04-30-linear-algebra-intro.html",
    "title": "Introduction to Linear Algebra",
    "section": "",
    "text": "Linear algebra deals with linear equations (no polynomial terms). However, it can still be used for approximations for non-linear systems. Uses in ML include solving for unknowns e.g.¬†deep learning, dimensionality reduction e.g.¬†PCA, ranking e.g.¬†eigenvectors, associated recommendations and NLP e.g.¬†SVD or matrix factorisation.\nAn example linear equation with many unknowns in a regression model of house prices:\n\\large y = \\alpha + \\beta x{_1} + \\gamma x{_2} + ... + mx{_m}\ny = price, \\alpha = y-intercept, \\beta = number of rooms, \\gamma = area, x = feature\nWhen dealing with linear systems of equations, there can only be three types of solutions, none, one or infinite. Plotting the equations, if the lines are parallel, they will never intersect i.e.¬†no solution, if the lines are overlapping, there are infinite solutions, else the lines will intersect at one point only (it is impossible for straight lines to intersect multiple times).\nUsing the house price regression model, we will have as many rows(n) of these equations as we have house prices, each with its m_{-1} features(x) (m-1 due to alpha), and m+1 unknowns(\\alpha,\\beta, \\gamma) which is what we want to solve for:\n y = \\alpha + \\beta x{_1} + \\gamma x{_2} + ... + m_{-1}x_{m_{-1}} \n\n\\begin{bmatrix}\ny{_1} |\\alpha + \\beta x{_{1,1}} + \\gamma x{_{1,2}} + ... + m_{-1}x{_{1,m_{-1}}} \\\\\ny{_2} | \\alpha + \\beta x{_{2,1}} + \\gamma x{_{2,2}} + ... + m_{-1}x{_{2,m_{-1}}} \\\\\n\\dots \\\\\ny{_n} |  \\alpha + \\beta x{_{n,1}} + \\gamma x{_{n,2}} + ... + m_{-1}x{_{n,m_{-1}}}\n\\end{bmatrix}\n\nThis is a set (denoted by square brackets) of linear equations.\n\n\nTensors are a generalisation of vectors and matrices to any number of dimensions.\n\n\n\nDimension\nName\nDescription\n\n\n\n\n0\nScalar\nmagnitude (value)\n\n\n1\nVector\narray\n\n\n2\nMatrix\n2d array (square)\n\n\n3\n3-Tensor\n3d array (cube)\n\n\nn\nn-Tensor\nmultidimensional\n\n\n\n\n\nSingle value, no dimensions, denoted by lower case italic variable names e.g.¬†x\n\n\n\n1-Dimensional ordered array of values, lower case bold italic name e.g.¬†\\large \\boldsymbol{x}\nElements are scalars, denoted non-bold lower case x with their index e.g.¬†x{_1}\n\n\n\\begin{bmatrix}  x{_1} & x{_2} & x{_3} \\end{bmatrix}{^T} = $\n\\begin{bmatrix}\nx{_1} \\\\\nx{_2} \\\\\nx{_3}\n\\end{bmatrix}\n$\nRow vector of shape (1,3) transposed to a column vector shape (3,1).\n\n\n\nNorms are functions that quantify vector magnitude (length).\n\\begin{bmatrix}  x{_1} & x{_2} \\end{bmatrix} = \\begin{bmatrix}  5 & 7 \\end{bmatrix} also represents a magnitude and direction from the origin.\n\\large L{^2} Norm ||\\boldsymbol{x}||{_2} = \\sqrt{\\sum_{\\substack{i}}x{_i}{^2}}\nSquare each element in the vector, sum them, then take the square root. Measures euclidean distance from the origin. Also commonly denoted \\large||\\boldsymbol{x}||.\n\\large L{^1} Norm ||\\boldsymbol{x}||{_1} = \\sum_{\\substack{i}}|x{_i}|\nSum the absolute values in the vector. Varies linearly at all locations (not origin dependent). Used when differences between zero and non-zero are needed.\nSquared \\large  L{^2} Norm ||\\boldsymbol{x}||{_2^2} = \\sum_{\\substack{i}}x{_i}{^2}\nSimilar to \\large L{^2} Norm, except we don‚Äôt take the root, just return the squared value. Computationally cheaper than \\large L{^2} Norm as sqaured \\large L{^2} Norm = \\boldsymbol{x}{^T}\\boldsymbol{x}. Derivative of element x only requires that element, vs \\large L{^2} Norm which requires the vector. However, squared \\large  L{^2} Norm grows slowly near the origin so can‚Äôt be used if zero/near-zero distinguishing is required.\n\\large L{^\\infin} Norm (or Max Norm) ||\\boldsymbol{x}||{_\\infin} = max{_i}|x{_i}|\nReturn maximum absolute value of the vector.\nGeneralised \\large L{^p} Norm ||\\boldsymbol{x}||{_p} = (\\sum_{\\substack{i}}|x{_i}|{^p}){^\\frac{1}{p}} for p being a real number \\geqslant 1\nNorms, particularly \\large L{^1} and \\large L{^2} norms, are used to regularise objective functions\n\n\n\nUnit vector is a special case where the L2 norm (length) is equal to one i.e.¬†\\boldsymbol{x} is a unit vector with unit norm. \\large||\\boldsymbol{x}|| = 1\n\n\n\nBasis vectors are vectors that can be scaled to represent any vector in a given vector space, typically unit vectors are used along the axes of the vector space. For example, the basis vectors i = (1,0) and j=(0,1), then we can represent a vector \\boldsymbol v as follows: \\boldsymbol v = 0.5 i + 3j\n\n\n\nSets of vectors where the dot product of the vectors are zero: \\boldsymbol{x}{^T}\\boldsymbol{y} = 0 i.e.¬†assuming they are of non-zero length, they are at 90\\degree angles i.e.¬†perpendicular. For an n-dimensional space, there are a maximum of n mutually orthogonal vectors.\n\n\nOrthonormal vectors are orthogonal and have unit norm e.g.¬†basis vectors.\n\n\n\n\n\nDenoted uppercase, italics and bold \\boldsymbol X = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\nNotation is (n_{row},n_{col})\nIndividual scalar elements denoted upper case italics only, e.g. \\boldsymbol X = \\begin{bmatrix} X_{1,1} & X_{1,2} \\\\ X_{2,1} & X_{2,2} \\end{bmatrix}\n\n\n\nUpper-case, bold, italic, sans-serif font e.g.¬†ùôì For a 4-tensor ùôì, an element at position (i,j,k,l) would be denoted as ùôì_{(i,j,k,l)}"
  },
  {
    "objectID": "posts/2022-04-30-linear-algebra-intro.html#tensors",
    "href": "posts/2022-04-30-linear-algebra-intro.html#tensors",
    "title": "Introduction to Linear Algebra",
    "section": "",
    "text": "Tensors are a generalisation of vectors and matrices to any number of dimensions.\n\n\n\nDimension\nName\nDescription\n\n\n\n\n0\nScalar\nmagnitude (value)\n\n\n1\nVector\narray\n\n\n2\nMatrix\n2d array (square)\n\n\n3\n3-Tensor\n3d array (cube)\n\n\nn\nn-Tensor\nmultidimensional\n\n\n\n\n\nSingle value, no dimensions, denoted by lower case italic variable names e.g.¬†x\n\n\n\n1-Dimensional ordered array of values, lower case bold italic name e.g.¬†\\large \\boldsymbol{x}\nElements are scalars, denoted non-bold lower case x with their index e.g.¬†x{_1}\n\n\n\\begin{bmatrix}  x{_1} & x{_2} & x{_3} \\end{bmatrix}{^T} = $\n\\begin{bmatrix}\nx{_1} \\\\\nx{_2} \\\\\nx{_3}\n\\end{bmatrix}\n$\nRow vector of shape (1,3) transposed to a column vector shape (3,1).\n\n\n\nNorms are functions that quantify vector magnitude (length).\n\\begin{bmatrix}  x{_1} & x{_2} \\end{bmatrix} = \\begin{bmatrix}  5 & 7 \\end{bmatrix} also represents a magnitude and direction from the origin.\n\\large L{^2} Norm ||\\boldsymbol{x}||{_2} = \\sqrt{\\sum_{\\substack{i}}x{_i}{^2}}\nSquare each element in the vector, sum them, then take the square root. Measures euclidean distance from the origin. Also commonly denoted \\large||\\boldsymbol{x}||.\n\\large L{^1} Norm ||\\boldsymbol{x}||{_1} = \\sum_{\\substack{i}}|x{_i}|\nSum the absolute values in the vector. Varies linearly at all locations (not origin dependent). Used when differences between zero and non-zero are needed.\nSquared \\large  L{^2} Norm ||\\boldsymbol{x}||{_2^2} = \\sum_{\\substack{i}}x{_i}{^2}\nSimilar to \\large L{^2} Norm, except we don‚Äôt take the root, just return the squared value. Computationally cheaper than \\large L{^2} Norm as sqaured \\large L{^2} Norm = \\boldsymbol{x}{^T}\\boldsymbol{x}. Derivative of element x only requires that element, vs \\large L{^2} Norm which requires the vector. However, squared \\large  L{^2} Norm grows slowly near the origin so can‚Äôt be used if zero/near-zero distinguishing is required.\n\\large L{^\\infin} Norm (or Max Norm) ||\\boldsymbol{x}||{_\\infin} = max{_i}|x{_i}|\nReturn maximum absolute value of the vector.\nGeneralised \\large L{^p} Norm ||\\boldsymbol{x}||{_p} = (\\sum_{\\substack{i}}|x{_i}|{^p}){^\\frac{1}{p}} for p being a real number \\geqslant 1\nNorms, particularly \\large L{^1} and \\large L{^2} norms, are used to regularise objective functions\n\n\n\nUnit vector is a special case where the L2 norm (length) is equal to one i.e.¬†\\boldsymbol{x} is a unit vector with unit norm. \\large||\\boldsymbol{x}|| = 1\n\n\n\nBasis vectors are vectors that can be scaled to represent any vector in a given vector space, typically unit vectors are used along the axes of the vector space. For example, the basis vectors i = (1,0) and j=(0,1), then we can represent a vector \\boldsymbol v as follows: \\boldsymbol v = 0.5 i + 3j\n\n\n\nSets of vectors where the dot product of the vectors are zero: \\boldsymbol{x}{^T}\\boldsymbol{y} = 0 i.e.¬†assuming they are of non-zero length, they are at 90\\degree angles i.e.¬†perpendicular. For an n-dimensional space, there are a maximum of n mutually orthogonal vectors.\n\n\nOrthonormal vectors are orthogonal and have unit norm e.g.¬†basis vectors.\n\n\n\n\n\nDenoted uppercase, italics and bold \\boldsymbol X = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\nNotation is (n_{row},n_{col})\nIndividual scalar elements denoted upper case italics only, e.g. \\boldsymbol X = \\begin{bmatrix} X_{1,1} & X_{1,2} \\\\ X_{2,1} & X_{2,2} \\end{bmatrix}\n\n\n\nUpper-case, bold, italic, sans-serif font e.g.¬†ùôì For a 4-tensor ùôì, an element at position (i,j,k,l) would be denoted as ùôì_{(i,j,k,l)}"
  },
  {
    "objectID": "posts/2024-11-25-distributed-systems.html",
    "href": "posts/2024-11-25-distributed-systems.html",
    "title": "Introduction to Distributed Systems",
    "section": "",
    "text": "Parallelism: Divide computationally intensive tasks across multiple machines\n\nFault Tolerance: Ensure system resilience through redundancy\n\nPerformance Scaling: Horizontal scaling of computational resources\n\nGeographic Distribution: Leverage physical separation for security and latency optimization\n\n\n\n\n\nConcurrency Management: Coordinating simultaneous operations\n\nPartial Failure Handling: Responding to individual component failures\n\nPerformance Optimization: Maintaining efficiency across distributed infrastructure\n\nConsistency Maintenance: Synchronizing state across multiple nodes\n\n\n\n\n\n\n\n\nRemote Procedure Calls (RPC): Synchronous method invocation across network\n\nMessage Passing: Asynchronous communication between distributed components\n\nPublish-Subscribe Models: Event-driven communication patterns\n\n\n\n\n\nLocks and Semaphores\n\nMutual exclusion\n\nResource synchronisation\n\nAtomic Transactions\n\nACID properties (Atomicity, Consistency, Isolation, Durability)\n\nEnsuring data integrity\n\n\n\n\n\n\n\n\nGuarantees immediate, synchronized view across all nodes\n\nHigh overhead, lower performance\n\nSuitable for financial, transactional systems\n\n\n\n\n\nAllows temporary divergence between node states\n\nLower overhead, higher performance\n\nAppropriate for eventually consistent systems like caches\n\n\n\n\n\n\n\n\n\nPrimary-Backup Model\nPeer-to-Peer Replication\nQuorum-Based Replication\n\n\n\n\n\nNetwork latency\n\nConflict resolution\n\nMaintaining replica coherence\n\n\n\n\n\n\n\n\nLinear speedup goals\n\nHorizontal scaling\n\nResource partitioning\n\n\n\n\n\nMinimizing inter-node communication\n\nOptimizing data locality\n\nCaching strategies\n\n\n\n\n\nMap reduce is an example of divide-and-conquer parallel processing: 1. Map - 2. Reduce -\n\n\n\n\n\n\n\n\n\nclass DistributedKeyValueStore:\n    def __init__(self, nodes):\n        self.nodes = nodes\n        self.replication_factor = 3\n    \n    def put(self, key, value):\n        \"\"\"Distribute key-value across nodes with replication\"\"\"\n        target_nodes = self._select_nodes(key)\n        for node in target_nodes:\n            node.store(key, value)\n    \n    def get(self, key):\n        \"\"\"Retrieve value with consistency mechanism\"\"\"\n        nodes_with_key = self._find_nodes_with_key(key)\n        return self._resolve_conflicts(nodes_with_key)\n    \n    def _select_nodes(self, key):\n        \"\"\"Deterministic node selection strategy\"\"\"\n        hash_value = hash(key)\n        return [self.nodes[i % len(self.nodes)] \n                for i in range(self.replication_factor)]"
  },
  {
    "objectID": "posts/2024-11-25-distributed-systems.html#core-concepts",
    "href": "posts/2024-11-25-distributed-systems.html#core-concepts",
    "title": "Introduction to Distributed Systems",
    "section": "",
    "text": "Parallelism: Divide computationally intensive tasks across multiple machines\n\nFault Tolerance: Ensure system resilience through redundancy\n\nPerformance Scaling: Horizontal scaling of computational resources\n\nGeographic Distribution: Leverage physical separation for security and latency optimization\n\n\n\n\n\nConcurrency Management: Coordinating simultaneous operations\n\nPartial Failure Handling: Responding to individual component failures\n\nPerformance Optimization: Maintaining efficiency across distributed infrastructure\n\nConsistency Maintenance: Synchronizing state across multiple nodes"
  },
  {
    "objectID": "posts/2024-11-25-distributed-systems.html#system-architecture",
    "href": "posts/2024-11-25-distributed-systems.html#system-architecture",
    "title": "Introduction to Distributed Systems",
    "section": "",
    "text": "Remote Procedure Calls (RPC): Synchronous method invocation across network\n\nMessage Passing: Asynchronous communication between distributed components\n\nPublish-Subscribe Models: Event-driven communication patterns\n\n\n\n\n\nLocks and Semaphores\n\nMutual exclusion\n\nResource synchronisation\n\nAtomic Transactions\n\nACID properties (Atomicity, Consistency, Isolation, Durability)\n\nEnsuring data integrity\n\n\n\n\n\n\n\n\nGuarantees immediate, synchronized view across all nodes\n\nHigh overhead, lower performance\n\nSuitable for financial, transactional systems\n\n\n\n\n\nAllows temporary divergence between node states\n\nLower overhead, higher performance\n\nAppropriate for eventually consistent systems like caches"
  },
  {
    "objectID": "posts/2024-11-25-distributed-systems.html#replication-strategies",
    "href": "posts/2024-11-25-distributed-systems.html#replication-strategies",
    "title": "Introduction to Distributed Systems",
    "section": "",
    "text": "Primary-Backup Model\nPeer-to-Peer Replication\nQuorum-Based Replication\n\n\n\n\n\nNetwork latency\n\nConflict resolution\n\nMaintaining replica coherence"
  },
  {
    "objectID": "posts/2024-11-25-distributed-systems.html#performance-considerations",
    "href": "posts/2024-11-25-distributed-systems.html#performance-considerations",
    "title": "Introduction to Distributed Systems",
    "section": "",
    "text": "Linear speedup goals\n\nHorizontal scaling\n\nResource partitioning\n\n\n\n\n\nMinimizing inter-node communication\n\nOptimizing data locality\n\nCaching strategies"
  },
  {
    "objectID": "posts/2024-11-25-distributed-systems.html#mapreduce",
    "href": "posts/2024-11-25-distributed-systems.html#mapreduce",
    "title": "Introduction to Distributed Systems",
    "section": "",
    "text": "Map reduce is an example of divide-and-conquer parallel processing: 1. Map - 2. Reduce -"
  },
  {
    "objectID": "posts/2024-11-25-distributed-systems.html#replicated-kv-store",
    "href": "posts/2024-11-25-distributed-systems.html#replicated-kv-store",
    "title": "Introduction to Distributed Systems",
    "section": "",
    "text": "class DistributedKeyValueStore:\n    def __init__(self, nodes):\n        self.nodes = nodes\n        self.replication_factor = 3\n    \n    def put(self, key, value):\n        \"\"\"Distribute key-value across nodes with replication\"\"\"\n        target_nodes = self._select_nodes(key)\n        for node in target_nodes:\n            node.store(key, value)\n    \n    def get(self, key):\n        \"\"\"Retrieve value with consistency mechanism\"\"\"\n        nodes_with_key = self._find_nodes_with_key(key)\n        return self._resolve_conflicts(nodes_with_key)\n    \n    def _select_nodes(self, key):\n        \"\"\"Deterministic node selection strategy\"\"\"\n        hash_value = hash(key)\n        return [self.nodes[i % len(self.nodes)] \n                for i in range(self.replication_factor)]"
  },
  {
    "objectID": "posts/2024-09-11-postgres-introduction.html",
    "href": "posts/2024-09-11-postgres-introduction.html",
    "title": "Introduction to PostgreSQL",
    "section": "",
    "text": "Various models developed to solve data storage and retrieval problems:\n\nHierarchical e.g.¬†IMS (IBM) - each record composed of other records - efficient storage for particular questions\n\nNetwork Database Model - records can contain references to other records via pointers - end up with a linked list - difficult to maintain pointers, and queries need to follow the linked lists\n\nRelational models (used in RDMSs) - emphasis on data integrity\n\nrecords are called tuples - an ordered group of components or attributes which each have a defined type\n\ntyples are ordered, there cannot be duplicates, and attributes must be atomic i.e.¬†a single piece of data\n\nthe attribute(s) used to differentiate a record from another are usually the primary key\n\n\n\n\n\nSQL (Structured Query Language):\n- current standard is SQL:2023\n- three types of commands:\n- DML (Data Manipulation Language) e.g.¬†inserting/updating/deleting - DDL (Data Definition Language) e.g.¬†creating tables/altering the structure or relationships of tables - DCL (Data Control Language) e.g.granting permissions or access\n\n\n\n\nManagement and storage of the database(s)\n\nQuery facilities\n\nConcurrent use\nAuditing/Logging - for auditing, investigation and restoration\n\nSecurity and access management\n\nMaintain referential integrity\n\n\n\n\nA relational DBMS that supports SQL\n\n\n\n[^1] Beginning Databases with PostgreSQL"
  },
  {
    "objectID": "posts/2024-09-11-postgres-introduction.html#database-models",
    "href": "posts/2024-09-11-postgres-introduction.html#database-models",
    "title": "Introduction to PostgreSQL",
    "section": "",
    "text": "Various models developed to solve data storage and retrieval problems:\n\nHierarchical e.g.¬†IMS (IBM) - each record composed of other records - efficient storage for particular questions\n\nNetwork Database Model - records can contain references to other records via pointers - end up with a linked list - difficult to maintain pointers, and queries need to follow the linked lists\n\nRelational models (used in RDMSs) - emphasis on data integrity\n\nrecords are called tuples - an ordered group of components or attributes which each have a defined type\n\ntyples are ordered, there cannot be duplicates, and attributes must be atomic i.e.¬†a single piece of data\n\nthe attribute(s) used to differentiate a record from another are usually the primary key"
  },
  {
    "objectID": "posts/2024-09-11-postgres-introduction.html#language",
    "href": "posts/2024-09-11-postgres-introduction.html#language",
    "title": "Introduction to PostgreSQL",
    "section": "",
    "text": "SQL (Structured Query Language):\n- current standard is SQL:2023\n- three types of commands:\n- DML (Data Manipulation Language) e.g.¬†inserting/updating/deleting - DDL (Data Definition Language) e.g.¬†creating tables/altering the structure or relationships of tables - DCL (Data Control Language) e.g.granting permissions or access"
  },
  {
    "objectID": "posts/2024-09-11-postgres-introduction.html#responsibilities-of-an-rdbms",
    "href": "posts/2024-09-11-postgres-introduction.html#responsibilities-of-an-rdbms",
    "title": "Introduction to PostgreSQL",
    "section": "",
    "text": "Management and storage of the database(s)\n\nQuery facilities\n\nConcurrent use\nAuditing/Logging - for auditing, investigation and restoration\n\nSecurity and access management\n\nMaintain referential integrity"
  },
  {
    "objectID": "posts/2024-09-11-postgres-introduction.html#postgresql",
    "href": "posts/2024-09-11-postgres-introduction.html#postgresql",
    "title": "Introduction to PostgreSQL",
    "section": "",
    "text": "A relational DBMS that supports SQL"
  },
  {
    "objectID": "posts/2024-09-11-postgres-introduction.html#references",
    "href": "posts/2024-09-11-postgres-introduction.html#references",
    "title": "Introduction to PostgreSQL",
    "section": "",
    "text": "[^1] Beginning Databases with PostgreSQL"
  },
  {
    "objectID": "posts/2024-08-27-linked-lists.html",
    "href": "posts/2024-08-27-linked-lists.html",
    "title": "Introduction to Linked Lists",
    "section": "",
    "text": "A linked list is a linear data structure where elements are stored in nodes. Each node contains a data field and a reference (or link) to the next node in the sequence.\n\n\n[Data|Next] -&gt; [Data|Next] -&gt; [Data|Next] -&gt; null\n\n\n\n\nSingly Linked List\nDoubly Linked List\nCircular Linked List\n\n\n\n\nIn a singly linked list, each node points to the next node in the sequence. In the following examples, the dot (‚Ä¢) is a pointer to the relevant node:\nExample:\n[3|‚Ä¢] -&gt; [7|‚Ä¢] -&gt; [1|‚Ä¢] -&gt; [9|‚Ä¢] -&gt; [4|‚Ä¢] -&gt; null\nOperations: - Access: O(n) - Search: O(n) - Insertion at beginning: O(1) - Insertion at end: O(n) - Deletion at beginning: O(1) - Deletion at end: O(n)\n\n\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass SinglyLinkedList:\n    def __init__(self):\n        self.head = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n            return\n        last = self.head\n        while last.next:\n            last = last.next\n        last.next = new_node\n    \n    def __str__(self):\n        if not self.head:\n            return \"empty\"\n        current = self.head\n        result = []\n        while current:\n            result.append(f\"[{current.data}|‚Ä¢]\")\n            current = current.next\n        result.append(\"null\")\n        return \" -&gt; \".join(result)\n\nsll = SinglyLinkedList()\nsll.append(3)\nsll.append(7)\nsll.append(1)\nsll.append(9)\nsll.append(4)\nprint(\"Singly Linked List:\", sll)\n\nSingly Linked List: [3|‚Ä¢] -&gt; [7|‚Ä¢] -&gt; [1|‚Ä¢] -&gt; [9|‚Ä¢] -&gt; [4|‚Ä¢] -&gt; null\n\n\n\n\n\n\nIn a doubly linked list, each node contains references to both the next and previous nodes.\nExample:\nnull &lt;-&gt; [3|‚Ä¢|‚Ä¢] &lt;-&gt; [7|‚Ä¢|‚Ä¢] &lt;-&gt; [1|‚Ä¢|‚Ä¢] &lt;-&gt; [9|‚Ä¢|‚Ä¢] &lt;-&gt; [4|‚Ä¢|‚Ä¢] &lt;-&gt; null\nOperations: - Access: O(n) - Search: O(n) - Insertion at beginning/end: O(1) - Deletion at beginning/end: O(1)\n\n\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\nclass DoublyLinkedList:\n    def __init__(self):\n        self.head = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n            return\n        last = self.head\n        while last.next:\n            last = last.next\n        last.next = new_node\n        new_node.prev = last\n\n    def __str__(self):\n        if not self.head:\n            return \"empty\"\n        current = self.head\n        result = [\"null\"]\n        while current:\n            result.append(f\"[{current.data}|‚Ä¢|‚Ä¢]\")\n            current = current.next\n        result.append(\"null\")\n        return \" &lt;-&gt; \".join(result)\n\n\ndll = DoublyLinkedList()\ndll.append(3)\ndll.append(7)\ndll.append(1)\ndll.append(9)\ndll.append(4)\nprint(\"Doubly Linked List:\", dll)\n\nDoubly Linked List: null &lt;-&gt; [3|‚Ä¢|‚Ä¢] &lt;-&gt; [7|‚Ä¢|‚Ä¢] &lt;-&gt; [1|‚Ä¢|‚Ä¢] &lt;-&gt; [9|‚Ä¢|‚Ä¢] &lt;-&gt; [4|‚Ä¢|‚Ä¢] &lt;-&gt; null\n\n\n\n\n\n\nIn a circular linked list, the last node points back to the first node, forming a circle.\nExample (Singly Circular):\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ                                ‚ñº\n[3|‚Ä¢] -&gt; [7|‚Ä¢] -&gt; [1|‚Ä¢] -&gt; [9|‚Ä¢] -&gt; [4|‚Ä¢]\nOperations: - Similar to singly/doubly linked lists - No null end: facilitates continuous traversal\n\n\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass CircularLinkedList:\n    def __init__(self):\n        self.head = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n            new_node.next = self.head\n            return\n        last = self.head\n        while last.next != self.head:\n            last = last.next\n        last.next = new_node\n        new_node.next = self.head\n\n    def __str__(self):\n        if not self.head:\n            return \"empty\"\n        current = self.head\n        result = []\n        while True:\n            result.append(f\"[{current.data}|‚Ä¢]\")\n            current = current.next\n            if current == self.head:\n                break\n        return \" -&gt; \".join(result) + \" (circular)\"\n\ncll = CircularLinkedList()\ncll.append(3)\ncll.append(7)\ncll.append(1)\ncll.append(9)\ncll.append(4)\nprint(\"Circular Linked List:\", cll)\n\nCircular Linked List: [3|‚Ä¢] -&gt; [7|‚Ä¢] -&gt; [1|‚Ä¢] -&gt; [9|‚Ä¢] -&gt; [4|‚Ä¢] (circular)\n\n\n\n\n\n\n\nDynamic size\nEase of insertion/deletion\nEfficient memory utilization\n\n\n\n\n\nRandom access is not allowed - must be traversed\nExtra memory space for pointers at each node\nNot cache friendly - less cache locality for CPU than a contiguous array - more likely to be randomly scattered around and therefore get cache misses\n\n\n\n\n\nImplementation of stacks and queues\n\nImplementing mutable arrays or collections"
  },
  {
    "objectID": "posts/2024-08-27-linked-lists.html#linked-lists",
    "href": "posts/2024-08-27-linked-lists.html#linked-lists",
    "title": "Introduction to Linked Lists",
    "section": "",
    "text": "A linked list is a linear data structure where elements are stored in nodes. Each node contains a data field and a reference (or link) to the next node in the sequence.\n\n\n[Data|Next] -&gt; [Data|Next] -&gt; [Data|Next] -&gt; null\n\n\n\n\nSingly Linked List\nDoubly Linked List\nCircular Linked List\n\n\n\n\nIn a singly linked list, each node points to the next node in the sequence. In the following examples, the dot (‚Ä¢) is a pointer to the relevant node:\nExample:\n[3|‚Ä¢] -&gt; [7|‚Ä¢] -&gt; [1|‚Ä¢] -&gt; [9|‚Ä¢] -&gt; [4|‚Ä¢] -&gt; null\nOperations: - Access: O(n) - Search: O(n) - Insertion at beginning: O(1) - Insertion at end: O(n) - Deletion at beginning: O(1) - Deletion at end: O(n)\n\n\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass SinglyLinkedList:\n    def __init__(self):\n        self.head = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n            return\n        last = self.head\n        while last.next:\n            last = last.next\n        last.next = new_node\n    \n    def __str__(self):\n        if not self.head:\n            return \"empty\"\n        current = self.head\n        result = []\n        while current:\n            result.append(f\"[{current.data}|‚Ä¢]\")\n            current = current.next\n        result.append(\"null\")\n        return \" -&gt; \".join(result)\n\nsll = SinglyLinkedList()\nsll.append(3)\nsll.append(7)\nsll.append(1)\nsll.append(9)\nsll.append(4)\nprint(\"Singly Linked List:\", sll)\n\nSingly Linked List: [3|‚Ä¢] -&gt; [7|‚Ä¢] -&gt; [1|‚Ä¢] -&gt; [9|‚Ä¢] -&gt; [4|‚Ä¢] -&gt; null\n\n\n\n\n\n\nIn a doubly linked list, each node contains references to both the next and previous nodes.\nExample:\nnull &lt;-&gt; [3|‚Ä¢|‚Ä¢] &lt;-&gt; [7|‚Ä¢|‚Ä¢] &lt;-&gt; [1|‚Ä¢|‚Ä¢] &lt;-&gt; [9|‚Ä¢|‚Ä¢] &lt;-&gt; [4|‚Ä¢|‚Ä¢] &lt;-&gt; null\nOperations: - Access: O(n) - Search: O(n) - Insertion at beginning/end: O(1) - Deletion at beginning/end: O(1)\n\n\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\nclass DoublyLinkedList:\n    def __init__(self):\n        self.head = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n            return\n        last = self.head\n        while last.next:\n            last = last.next\n        last.next = new_node\n        new_node.prev = last\n\n    def __str__(self):\n        if not self.head:\n            return \"empty\"\n        current = self.head\n        result = [\"null\"]\n        while current:\n            result.append(f\"[{current.data}|‚Ä¢|‚Ä¢]\")\n            current = current.next\n        result.append(\"null\")\n        return \" &lt;-&gt; \".join(result)\n\n\ndll = DoublyLinkedList()\ndll.append(3)\ndll.append(7)\ndll.append(1)\ndll.append(9)\ndll.append(4)\nprint(\"Doubly Linked List:\", dll)\n\nDoubly Linked List: null &lt;-&gt; [3|‚Ä¢|‚Ä¢] &lt;-&gt; [7|‚Ä¢|‚Ä¢] &lt;-&gt; [1|‚Ä¢|‚Ä¢] &lt;-&gt; [9|‚Ä¢|‚Ä¢] &lt;-&gt; [4|‚Ä¢|‚Ä¢] &lt;-&gt; null\n\n\n\n\n\n\nIn a circular linked list, the last node points back to the first node, forming a circle.\nExample (Singly Circular):\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ                                ‚ñº\n[3|‚Ä¢] -&gt; [7|‚Ä¢] -&gt; [1|‚Ä¢] -&gt; [9|‚Ä¢] -&gt; [4|‚Ä¢]\nOperations: - Similar to singly/doubly linked lists - No null end: facilitates continuous traversal\n\n\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass CircularLinkedList:\n    def __init__(self):\n        self.head = None\n\n    def append(self, data):\n        new_node = Node(data)\n        if not self.head:\n            self.head = new_node\n            new_node.next = self.head\n            return\n        last = self.head\n        while last.next != self.head:\n            last = last.next\n        last.next = new_node\n        new_node.next = self.head\n\n    def __str__(self):\n        if not self.head:\n            return \"empty\"\n        current = self.head\n        result = []\n        while True:\n            result.append(f\"[{current.data}|‚Ä¢]\")\n            current = current.next\n            if current == self.head:\n                break\n        return \" -&gt; \".join(result) + \" (circular)\"\n\ncll = CircularLinkedList()\ncll.append(3)\ncll.append(7)\ncll.append(1)\ncll.append(9)\ncll.append(4)\nprint(\"Circular Linked List:\", cll)\n\nCircular Linked List: [3|‚Ä¢] -&gt; [7|‚Ä¢] -&gt; [1|‚Ä¢] -&gt; [9|‚Ä¢] -&gt; [4|‚Ä¢] (circular)\n\n\n\n\n\n\n\nDynamic size\nEase of insertion/deletion\nEfficient memory utilization\n\n\n\n\n\nRandom access is not allowed - must be traversed\nExtra memory space for pointers at each node\nNot cache friendly - less cache locality for CPU than a contiguous array - more likely to be randomly scattered around and therefore get cache misses\n\n\n\n\n\nImplementation of stacks and queues\n\nImplementing mutable arrays or collections"
  },
  {
    "objectID": "posts/2024-08-27-linked-lists.html#references-for-linked-lists",
    "href": "posts/2024-08-27-linked-lists.html#references-for-linked-lists",
    "title": "Introduction to Linked Lists",
    "section": "References for Linked Lists",
    "text": "References for Linked Lists\n[^1] HackerRank - Linked Lists Data Structure\n[^2] Stanford CS Education Library - Linked List Basics\n[^3] VisuAlgo - Linked List Visualization\n[^4] leetcode"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html",
    "href": "posts/2024-08-24-DSA_1.html",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "Data structures provide an organised format for data storage, management and access. Most of the following data structures are used in four basic ways:\n\nAccess - provided a location, return the item at that location, or alternatively, return all the items\nSearch - look for a specific item or value, and it‚Äôs location\nInsert - add an item into the collection i.e.¬†at the start, end or in between\nDelete - remove an item from the collection\n\nTransformations like sorting and filtering can also be performed. The following are some common and basic data structures:\n\n\n\n\n\nArrays are collections of items, typically stored in contiguous memory:\n| Index | 0 | 1 | 2 | 3 | 4 | 5 |\n|-------|---|---|---|---|---|---|\n| Value | 1 | 2 | 3 | 4 | 5 | 6 |\nEfficiencies:\n\nAccess: O(1)\nSearch: O(n)\nInsertion: O(n)\nDeletion: O(n)\n\n\n\n\nElements are stored in no particular order.\n| Index | 0 | 1 | 2 | 3 | 4 | 5 |\n|-------|---|---|---|---|---|---|\n| Value | 7 | 2 | 9 | 4 | 5 | 1 |\n\nInsertion: O(1) (add to the end)\n\nDeletion: O(n) (need to shift elements)\n\nSearch: O(n) (linear search)\n\n\n\n\nElements are stored in a specific order, typically ascending or descending.\n| Index | 0 | 1 | 2 | 3 | 4 | 5 |\n|-------|---|---|---|---|---|---|\n| Value | 1 | 2 | 4 | 5 | 7 | 9 |\n\nInsertion: O(n) (find position and shift elements)\n\nDeletion: O(n) (shift elements after deletion)\n\nSearch: O(\\log n) (binary search)\n\n\n\n\nThe last element is conceptually followed by the first element, forming a circle.\n    ‚îå‚îÄ‚îÄ‚îÄ‚îê\n‚îå‚îÄ‚îÄ‚îÄ‚î§ 5 ‚îú‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ 4 ‚îÇ   ‚îÇ 1 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚î§   ‚îú‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 3 ‚îÇ   ‚îÇ 2 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò\n\nUseful for queue implementations\n\nEfficient use of fixed-size array\n\nOperations wrap around the end of the array\n\n\n\n\nAn array where each element is an array, possibly of different lengths.\n| Index | Array                |\n|-------|----------------------|\n|   0   | [1, 2, 3]            |\n|   1   | [4, 5]               |\n|   2   | [6, 7, 8, 9]         |\n|   3   | [10]                 |\n\nFlexible structure for 2D data with varying row lengths\nEfficient for sparse matrices\nAccess: O(1) for both dimensions\n\n\n\n\nAn array where most elements have the same value (usually zero).\nLogical view:\n| 0 | 0 | 0 | 3 | 0 | 0 | 1 | 0 | 0 | 2 | 0 |\nActual storage (index-value pairs):\n| 3:3 | 6:1 | 9:2 |\n\nSpace-efficient for sparse data\n\nSlower access time compared to standard arrays\n\n\n\n\n\n\n\nArray based sets are collections of items, stored in contiguous memory, that do not allow duplicate item to be inserted, and may or may not be sorted:\nUnsorted:\n| Index | 0 | 1 | 2 | 3 | 4 |\n|-------|---|---|---|---|---|\n| Value | 5 | 2 | 8 | 1 | 3 |\nSorted:\n| Index | 0 | 1 | 2 | 3 | 4 |\n|-------|---|---|---|---|---|\n| Value | 1 | 2 | 3 | 5 | 8 |\nEvery insert requires a search first to determine the value to be inserted does not already exist.\nAccess: O(1)\nInsertion:\n- Unsorted: O(1) (append to end)\n- Sorted: O(n) (find correct position and shift elements, to maintain order)\nDeletion: O(n) (need to shift elements to fill the gap)\nSearch:\n- Unsorted: O(n)\n- Sorted: O(\\log n) (using binary search)\n\n\n\nHash table-based sets use a hash function (Section¬†1.11) to map elements to indices in an array.\nHash function: h(x) = x \\mod 7\ni.e.¬†every input is divided by 7 and the remainder is the output of the hash function (output between 0 and 6)\n\n\n\nIndex\nBucket\n\n\n\n\n0\n7\n\n\n1\n1, 8\n\n\n2\n2\n\n\n3\n3\n\n\n4\n\n\n\n5\n5\n\n\n6\n\n\n\n\n\nCollision - in this example, there is a collision at index 1, with values 1 and 8 returning the same output and therefore being assigned to the same bucket.\nEmpty bucket - important to monitor (or be aware) of the distribution of data which may indicate the hashing function may need to be altered if there are many empty buckets.\n\nAccess: N/A (sets don‚Äôt typically support direct access)\nInsertion: \\theta(1) O(n)\nDeletion: \\theta(1) O(n)\nSearch: \\theta(1) O(n)\n\n\n\nTree-based sets store elements in a self-balancing binary search tree (BST). A binary search tree is a data structure composed of nodes. Each node has a key, and each key in the left subtree of a node is less than the node‚Äôs key, while each key in the right subtree is greater. Standard operations (search, insert, delete) on a BST generally have action time complexity proportional to the height of the tree.\nThe height of a tree is the length of the longest path from the root node to a leaf node. In a balanced tree, the height is O(\\log ‚Å°n). If the tree becomes unbalanced (e.g., all nodes added to one side), the height can become O(n).\nA self-balancing BST is a BST that automatically maintains its height after insertions or deletions, which keeps the height as small as possible, ensuring search/insertion/deletion can be performed in O(\\log n) time complexity, where n is the number of nodes in the tree. After every insertion or deletion, the tree checks if it has become unbalanced. If it has, the tree undergoes a series of rotations or restructuring operations to restore balance. The two most common implementations are Red-Black Trees and AVL Trees.\n    4\n   / \\\n  2   6\n / \\ / \\\n1  3 5  7\nGeneral characteristics: - Access: N/A (sets don‚Äôt typically support direct access) - Insertion: O(\\log n) - Deletion: O(\\log n) - Search: O(\\log n)\n\n\nRed-Black trees are BSTs with one extra bit of storage per node: its color, which can be either red or black. By constraining the way nodes can be colored, Red-Black trees ensure that no path from the root to a leaf is more than twice as long as any other path.\nProperties: 1. Every node is either red or black. 2. The root is black. 3. Every leaf (NIL) is black. 4. If a node is red, then both its children are black i.e.¬†no two reds in a row 5. For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.\nVisualization:\n    4B\n   /  \\\n  2R   6R\n / \\   / \\\n1B 3B 5B 7B\n(B = Black, R = Red)\nRed-Black trees provide faster insertion and removal operations than AVL trees as they require fewer rotations on average. However, AVL trees provide faster lookups as they are more strictly balanced.\n\n\n\nAVL trees (named after Adelson-Velsky and Landis) are self-balancing binary search trees. In an AVL tree, the heights of the two child subtrees of any node differ by at most one. If, after an insertion or deletion, the tree becomes unbalanced (i.e., the height difference becomes greater than one), rotations are performed to restore balance.\nProperties: 1. The heights of the left and right subtrees of every node differ by at most one. 2. Every subtree is an AVL tree.\nBalance Factor = Height(Left Subtree) - Height(Right Subtree)\nFor any node in an AVL tree, the balance factor must be -1, 0, or 1.\nVisualization:\n    4 (0)\n   /   \\\n2 (-1)  6 (0)\n / \\    / \\\n1   3  5   7\n(Numbers in parentheses represent balance factors)\nAVL trees maintain a stricter balance than Red-Black trees, leading to faster lookups but slower insertion and deletion due to more frequent rotations.\n\n\n\n\nB-Trees are generalisations of self-balancing trees that can have more than two children. B-Trees can be used in databases and file systems because they are work well with systems that read and write large blocks of data.\n\n\n\nA splay tree is a self-balancing BST that performs splaying, a process that moves a given node to the root of the tree by performing a series of tree rotations. Splay trees keep recently accessed elements near the top, making them efficient for specific workload styles.\n\n\n\nBalance:\n\nAVL trees are more rigidly balanced\nRed-Black trees may have longer paths (up to 2 times the shortest path)\n\nOperations:\n\nLookups tend to be faster in AVL trees\nInsertions and deletions tend to be faster in Red-Black trees\n\nUsage:\n\nAVL trees are preferred for lookup-intensive applications\nRed-Black trees are preferred for insertion/deletion-intensive applications\n\n\nBoth tree types guarantee O(\\log n) time complexity for basic operations, making them efficient choices for set and map implementations.\n\n\n\n\n\nBit vector sets use a bit array to represent the presence or absence of elements.\n\n\nUniverse: {0, 1, 2, 3, 4, 5, 6, 7}\nSet: {1, 2, 4, 7}\n\nBit Vector: \n| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n| 0 | 1 | 1 | 0 | 1 | 0 | 0 | 1 |\n\nAccess: O(1)\n\nInsertion: O(1)\n\nDeletion: O(1)\n\nSearch: O(1)\n\n\n\n\n\nArray-based sets are simple but inefficient for large, dynamic sets.\nHash table-based sets offer great average-case performance.\nTree-based sets provide balanced performance and maintain order.\nBit vector sets are highly efficient but limited in applicability.\n\nChoosing between them depends on the expected set size, frequency of operations, memory constraints, and need for maintaining order.\n\n\n\n\nA linked list consists of nodes where each node contains a data field and a reference to the next node in the sequence.\n[3] -&gt; [7] -&gt; [1] -&gt; [9] -&gt; [4] -&gt; null\nAccess: O(n) Search: O(n) Insertion: O(1) Deletion: O(1)\n\n\n\nA stack follows the Last In First Out (LIFO) principle.\n| 4 | &lt;- Top\n| 9 |\n| 1 |\n| 7 |\n| 3 |\nPush: O(1) Pop: O(1) Peek: O(1)\n\n\n\nA queue follows the First In First Out (FIFO) principle.\nFront -&gt; [3] -&gt; [7] -&gt; [1] -&gt; [9] -&gt; [4] &lt;- Rear\nEnqueue: O(1) Dequeue: O(1) Front: O(1)\n\n\n\nA tree is a hierarchical structure with a root node and child nodes.\n    1\n   / \\\n  2   3\n / \\   \\\n4   5   6\nAccess: O(\\log n) (for balanced trees) Search: O(\\log n) (for balanced trees) Insertion: O(\\log n) (for balanced trees) Deletion: O(\\log n) (for balanced trees)\n\n\n\nA graph consists of vertices connected by edges.\n    A --- B\n    |   / |\n    |  /  |\n    | /   |\n    C --- D\nTraversal: O(V + E) where V is the number of vertices and E is the number of edges\n\n\n\nA hash table stores key-value pairs and uses a hash function to compute an index into an array of buckets.\n\n\n\nKey\nHash\nIndex\nValue\n\n\n\n\n‚Äúapple‚Äù\nhash(‚Äúapple‚Äù)\n2\n5\n\n\n‚Äúbanana‚Äù\nhash(‚Äúbanana‚Äù)\n4\n8\n\n\n‚Äúcherry‚Äù\nhash(‚Äúcherry‚Äù)\n1\n3\n\n\n\nAccess: \\theta(1), O(n)\nSearch: \\theta(1), O(n)\nInsertion: \\theta(1), O(n)\nDeletion: \\theta(1), O(n)\n\n\n\n\n\nh(x) = x \\mod n\nTake the modulus of some input value by some number, n, where n is usually a prime number (Link).\n\n\n\nThe multiplication method involves multiplying the input value x by a constant A (where 0 &lt; A &lt; 1), taking the fractional part of the result, and then multiplying by the size of the hash table m:\nh(x) = \\left\\lfloor m \\times (x \\times A \\mod 1) \\right\\rfloor\nLet‚Äôs choose m = 10 and A = \\frac{\\sqrt{5} - 1}{2} \\approx 0.6180339887\nFor x = 123:\n\nMultiply: 123 \\times 0.6180339887 = 76.01818\n\nTake fractional part: 0.01818\n\nMultiply by m: 10 \\times 0.01818 = 0.1818\n\nFloor the result: \\lfloor 0.1818 \\rfloor = 0\n\nSo, h(123) = 0\n\n\n\nThe mid-square method involves squaring the key and taking the middle digits:\n\nSquare the key\n\nExtract a fixed number of digits from the middle\n\nUse these digits as the hash value\n\nFor key x = 3791 and a 4-digit hash:\n\nSquare: 3791^2 = 14371681\n\nExtract middle 4 digits: 3716\n\nSo, h(3791) = 3716\n\n\n\nThe folding method involves dividing the key into equal-sized parts (except possibly the last part) and combining these parts using addition or XOR:\n\nDivide the key into equal-sized parts\nSum or XOR these parts\nTake the result modulo the table size\n\nFor key x = 12345678 and 2-digit parts:\n\nDivide: 12|34|56|78\nSum: 12 + 34 + 56 + 78 = 180\nIf table size is 100: 180 \\mod 100 = 80\n\nSo, h(12345678) = 80"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#introduction",
    "href": "posts/2024-08-24-DSA_1.html#introduction",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "Data structures provide an organised format for data storage, management and access. Most of the following data structures are used in four basic ways:\n\nAccess - provided a location, return the item at that location, or alternatively, return all the items\nSearch - look for a specific item or value, and it‚Äôs location\nInsert - add an item into the collection i.e.¬†at the start, end or in between\nDelete - remove an item from the collection\n\nTransformations like sorting and filtering can also be performed. The following are some common and basic data structures:"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#arrays",
    "href": "posts/2024-08-24-DSA_1.html#arrays",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "Arrays are collections of items, typically stored in contiguous memory:\n| Index | 0 | 1 | 2 | 3 | 4 | 5 |\n|-------|---|---|---|---|---|---|\n| Value | 1 | 2 | 3 | 4 | 5 | 6 |\nEfficiencies:\n\nAccess: O(1)\nSearch: O(n)\nInsertion: O(n)\nDeletion: O(n)\n\n\n\n\nElements are stored in no particular order.\n| Index | 0 | 1 | 2 | 3 | 4 | 5 |\n|-------|---|---|---|---|---|---|\n| Value | 7 | 2 | 9 | 4 | 5 | 1 |\n\nInsertion: O(1) (add to the end)\n\nDeletion: O(n) (need to shift elements)\n\nSearch: O(n) (linear search)\n\n\n\n\nElements are stored in a specific order, typically ascending or descending.\n| Index | 0 | 1 | 2 | 3 | 4 | 5 |\n|-------|---|---|---|---|---|---|\n| Value | 1 | 2 | 4 | 5 | 7 | 9 |\n\nInsertion: O(n) (find position and shift elements)\n\nDeletion: O(n) (shift elements after deletion)\n\nSearch: O(\\log n) (binary search)\n\n\n\n\nThe last element is conceptually followed by the first element, forming a circle.\n    ‚îå‚îÄ‚îÄ‚îÄ‚îê\n‚îå‚îÄ‚îÄ‚îÄ‚î§ 5 ‚îú‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ 4 ‚îÇ   ‚îÇ 1 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚î§   ‚îú‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 3 ‚îÇ   ‚îÇ 2 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò\n\nUseful for queue implementations\n\nEfficient use of fixed-size array\n\nOperations wrap around the end of the array\n\n\n\n\nAn array where each element is an array, possibly of different lengths.\n| Index | Array                |\n|-------|----------------------|\n|   0   | [1, 2, 3]            |\n|   1   | [4, 5]               |\n|   2   | [6, 7, 8, 9]         |\n|   3   | [10]                 |\n\nFlexible structure for 2D data with varying row lengths\nEfficient for sparse matrices\nAccess: O(1) for both dimensions\n\n\n\n\nAn array where most elements have the same value (usually zero).\nLogical view:\n| 0 | 0 | 0 | 3 | 0 | 0 | 1 | 0 | 0 | 2 | 0 |\nActual storage (index-value pairs):\n| 3:3 | 6:1 | 9:2 |\n\nSpace-efficient for sparse data\n\nSlower access time compared to standard arrays"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#sets",
    "href": "posts/2024-08-24-DSA_1.html#sets",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "Array based sets are collections of items, stored in contiguous memory, that do not allow duplicate item to be inserted, and may or may not be sorted:\nUnsorted:\n| Index | 0 | 1 | 2 | 3 | 4 |\n|-------|---|---|---|---|---|\n| Value | 5 | 2 | 8 | 1 | 3 |\nSorted:\n| Index | 0 | 1 | 2 | 3 | 4 |\n|-------|---|---|---|---|---|\n| Value | 1 | 2 | 3 | 5 | 8 |\nEvery insert requires a search first to determine the value to be inserted does not already exist.\nAccess: O(1)\nInsertion:\n- Unsorted: O(1) (append to end)\n- Sorted: O(n) (find correct position and shift elements, to maintain order)\nDeletion: O(n) (need to shift elements to fill the gap)\nSearch:\n- Unsorted: O(n)\n- Sorted: O(\\log n) (using binary search)\n\n\n\nHash table-based sets use a hash function (Section¬†1.11) to map elements to indices in an array.\nHash function: h(x) = x \\mod 7\ni.e.¬†every input is divided by 7 and the remainder is the output of the hash function (output between 0 and 6)\n\n\n\nIndex\nBucket\n\n\n\n\n0\n7\n\n\n1\n1, 8\n\n\n2\n2\n\n\n3\n3\n\n\n4\n\n\n\n5\n5\n\n\n6\n\n\n\n\n\nCollision - in this example, there is a collision at index 1, with values 1 and 8 returning the same output and therefore being assigned to the same bucket.\nEmpty bucket - important to monitor (or be aware) of the distribution of data which may indicate the hashing function may need to be altered if there are many empty buckets.\n\nAccess: N/A (sets don‚Äôt typically support direct access)\nInsertion: \\theta(1) O(n)\nDeletion: \\theta(1) O(n)\nSearch: \\theta(1) O(n)\n\n\n\nTree-based sets store elements in a self-balancing binary search tree (BST). A binary search tree is a data structure composed of nodes. Each node has a key, and each key in the left subtree of a node is less than the node‚Äôs key, while each key in the right subtree is greater. Standard operations (search, insert, delete) on a BST generally have action time complexity proportional to the height of the tree.\nThe height of a tree is the length of the longest path from the root node to a leaf node. In a balanced tree, the height is O(\\log ‚Å°n). If the tree becomes unbalanced (e.g., all nodes added to one side), the height can become O(n).\nA self-balancing BST is a BST that automatically maintains its height after insertions or deletions, which keeps the height as small as possible, ensuring search/insertion/deletion can be performed in O(\\log n) time complexity, where n is the number of nodes in the tree. After every insertion or deletion, the tree checks if it has become unbalanced. If it has, the tree undergoes a series of rotations or restructuring operations to restore balance. The two most common implementations are Red-Black Trees and AVL Trees.\n    4\n   / \\\n  2   6\n / \\ / \\\n1  3 5  7\nGeneral characteristics: - Access: N/A (sets don‚Äôt typically support direct access) - Insertion: O(\\log n) - Deletion: O(\\log n) - Search: O(\\log n)\n\n\nRed-Black trees are BSTs with one extra bit of storage per node: its color, which can be either red or black. By constraining the way nodes can be colored, Red-Black trees ensure that no path from the root to a leaf is more than twice as long as any other path.\nProperties: 1. Every node is either red or black. 2. The root is black. 3. Every leaf (NIL) is black. 4. If a node is red, then both its children are black i.e.¬†no two reds in a row 5. For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.\nVisualization:\n    4B\n   /  \\\n  2R   6R\n / \\   / \\\n1B 3B 5B 7B\n(B = Black, R = Red)\nRed-Black trees provide faster insertion and removal operations than AVL trees as they require fewer rotations on average. However, AVL trees provide faster lookups as they are more strictly balanced.\n\n\n\nAVL trees (named after Adelson-Velsky and Landis) are self-balancing binary search trees. In an AVL tree, the heights of the two child subtrees of any node differ by at most one. If, after an insertion or deletion, the tree becomes unbalanced (i.e., the height difference becomes greater than one), rotations are performed to restore balance.\nProperties: 1. The heights of the left and right subtrees of every node differ by at most one. 2. Every subtree is an AVL tree.\nBalance Factor = Height(Left Subtree) - Height(Right Subtree)\nFor any node in an AVL tree, the balance factor must be -1, 0, or 1.\nVisualization:\n    4 (0)\n   /   \\\n2 (-1)  6 (0)\n / \\    / \\\n1   3  5   7\n(Numbers in parentheses represent balance factors)\nAVL trees maintain a stricter balance than Red-Black trees, leading to faster lookups but slower insertion and deletion due to more frequent rotations.\n\n\n\n\nB-Trees are generalisations of self-balancing trees that can have more than two children. B-Trees can be used in databases and file systems because they are work well with systems that read and write large blocks of data.\n\n\n\nA splay tree is a self-balancing BST that performs splaying, a process that moves a given node to the root of the tree by performing a series of tree rotations. Splay trees keep recently accessed elements near the top, making them efficient for specific workload styles.\n\n\n\nBalance:\n\nAVL trees are more rigidly balanced\nRed-Black trees may have longer paths (up to 2 times the shortest path)\n\nOperations:\n\nLookups tend to be faster in AVL trees\nInsertions and deletions tend to be faster in Red-Black trees\n\nUsage:\n\nAVL trees are preferred for lookup-intensive applications\nRed-Black trees are preferred for insertion/deletion-intensive applications\n\n\nBoth tree types guarantee O(\\log n) time complexity for basic operations, making them efficient choices for set and map implementations."
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#bit-vector-sets",
    "href": "posts/2024-08-24-DSA_1.html#bit-vector-sets",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "Bit vector sets use a bit array to represent the presence or absence of elements.\n\n\nUniverse: {0, 1, 2, 3, 4, 5, 6, 7}\nSet: {1, 2, 4, 7}\n\nBit Vector: \n| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n| 0 | 1 | 1 | 0 | 1 | 0 | 0 | 1 |\n\nAccess: O(1)\n\nInsertion: O(1)\n\nDeletion: O(1)\n\nSearch: O(1)\n\n\n\n\n\nArray-based sets are simple but inefficient for large, dynamic sets.\nHash table-based sets offer great average-case performance.\nTree-based sets provide balanced performance and maintain order.\nBit vector sets are highly efficient but limited in applicability.\n\nChoosing between them depends on the expected set size, frequency of operations, memory constraints, and need for maintaining order."
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#linked-list",
    "href": "posts/2024-08-24-DSA_1.html#linked-list",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "A linked list consists of nodes where each node contains a data field and a reference to the next node in the sequence.\n[3] -&gt; [7] -&gt; [1] -&gt; [9] -&gt; [4] -&gt; null\nAccess: O(n) Search: O(n) Insertion: O(1) Deletion: O(1)"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#stack",
    "href": "posts/2024-08-24-DSA_1.html#stack",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "A stack follows the Last In First Out (LIFO) principle.\n| 4 | &lt;- Top\n| 9 |\n| 1 |\n| 7 |\n| 3 |\nPush: O(1) Pop: O(1) Peek: O(1)"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#queue",
    "href": "posts/2024-08-24-DSA_1.html#queue",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "A queue follows the First In First Out (FIFO) principle.\nFront -&gt; [3] -&gt; [7] -&gt; [1] -&gt; [9] -&gt; [4] &lt;- Rear\nEnqueue: O(1) Dequeue: O(1) Front: O(1)"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#tree",
    "href": "posts/2024-08-24-DSA_1.html#tree",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "A tree is a hierarchical structure with a root node and child nodes.\n    1\n   / \\\n  2   3\n / \\   \\\n4   5   6\nAccess: O(\\log n) (for balanced trees) Search: O(\\log n) (for balanced trees) Insertion: O(\\log n) (for balanced trees) Deletion: O(\\log n) (for balanced trees)"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#graph",
    "href": "posts/2024-08-24-DSA_1.html#graph",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "A graph consists of vertices connected by edges.\n    A --- B\n    |   / |\n    |  /  |\n    | /   |\n    C --- D\nTraversal: O(V + E) where V is the number of vertices and E is the number of edges"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#hash-table",
    "href": "posts/2024-08-24-DSA_1.html#hash-table",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "A hash table stores key-value pairs and uses a hash function to compute an index into an array of buckets.\n\n\n\nKey\nHash\nIndex\nValue\n\n\n\n\n‚Äúapple‚Äù\nhash(‚Äúapple‚Äù)\n2\n5\n\n\n‚Äúbanana‚Äù\nhash(‚Äúbanana‚Äù)\n4\n8\n\n\n‚Äúcherry‚Äù\nhash(‚Äúcherry‚Äù)\n1\n3\n\n\n\nAccess: \\theta(1), O(n)\nSearch: \\theta(1), O(n)\nInsertion: \\theta(1), O(n)\nDeletion: \\theta(1), O(n)"
  },
  {
    "objectID": "posts/2024-08-24-DSA_1.html#sec-hash-functions",
    "href": "posts/2024-08-24-DSA_1.html#sec-hash-functions",
    "title": "Introduction to Basic Data Structures",
    "section": "",
    "text": "h(x) = x \\mod n\nTake the modulus of some input value by some number, n, where n is usually a prime number (Link).\n\n\n\nThe multiplication method involves multiplying the input value x by a constant A (where 0 &lt; A &lt; 1), taking the fractional part of the result, and then multiplying by the size of the hash table m:\nh(x) = \\left\\lfloor m \\times (x \\times A \\mod 1) \\right\\rfloor\nLet‚Äôs choose m = 10 and A = \\frac{\\sqrt{5} - 1}{2} \\approx 0.6180339887\nFor x = 123:\n\nMultiply: 123 \\times 0.6180339887 = 76.01818\n\nTake fractional part: 0.01818\n\nMultiply by m: 10 \\times 0.01818 = 0.1818\n\nFloor the result: \\lfloor 0.1818 \\rfloor = 0\n\nSo, h(123) = 0\n\n\n\nThe mid-square method involves squaring the key and taking the middle digits:\n\nSquare the key\n\nExtract a fixed number of digits from the middle\n\nUse these digits as the hash value\n\nFor key x = 3791 and a 4-digit hash:\n\nSquare: 3791^2 = 14371681\n\nExtract middle 4 digits: 3716\n\nSo, h(3791) = 3716\n\n\n\nThe folding method involves dividing the key into equal-sized parts (except possibly the last part) and combining these parts using addition or XOR:\n\nDivide the key into equal-sized parts\nSum or XOR these parts\nTake the result modulo the table size\n\nFor key x = 12345678 and 2-digit parts:\n\nDivide: 12|34|56|78\nSum: 12 + 34 + 56 + 78 = 180\nIf table size is 100: 180 \\mod 100 = 80\n\nSo, h(12345678) = 80"
  },
  {
    "objectID": "posts/2023-04-03-inductive-program-synthesis.html",
    "href": "posts/2023-04-03-inductive-program-synthesis.html",
    "title": "Inductive Program Synthesis",
    "section": "",
    "text": "A simple implementation would be to construct all possible programs from the terminals of a grammar. This can be inefficient as the space of all expressions can grow quickly. Observationally equivalent programs are discarded (i.e.¬†if the same output is generated based on the same input, the primitive can be discarded), which reduces the solution space exponentially.\nfunction synthesise (inputs, outputs):\n  terms := set_of_all_terminals\n  while(true):\n    terms := grow(terms)\n    terms := eliminate_observational_equivalents(terms, inputs);\n    foreach(t in terms):\n      if(is_correct(t, inputs, outputs)):\n        return t;\nOne characteristic is that bottom-up search explores small programs before larger ones, so it can potentially find the smallest program that satisfies the specification. Further heuristics can be applied in the grow or eliminate function level to direct the search, and it copes well with symmetry reduction. However, even with aggressive pruning, it is hard to scale beyond small lists of terms, and whilst it can connect discrete components, it is not effective at determining constants. It also may fail at pruning if the language has context dependant semantics e.g.¬†if the same expression had dufferent values in different contexts e.g.¬†via mutation.\n\n\n\nBy modularising the search, i.e.¬†search for multiple programs that work for different inputs and find a way to combine them that will work for all inputs, the scalability can be improved. The first step is a best-effort synthesis (if it works on all then it is just returned), then it can try to either improve the current program by taking a selection of the currently failing inputs alongside the correctly mapped inputs, or STUN can be called recursively on the inputs where the synthesised program failed. Then the programs can be combined. #### Anti/Unification If there are no top-level branches to allow unification, antiunification can be used. Unification is finding a common structure for two different expressions by replacing variables with expressions. Antiunification is to process of finding the common structure by replacing expressions with variables. For example, if we find 2 expressions a \\times c and b \\times c cover all inputs, antiunification can produce a common expression v \\times c where v stands for a code fragment that can be solved, being a much smaller synthesis problem. When a recursive call to STUN is made, additional constraints can also be passed that the expression must satisfy to avoid situations where, for example, anntiunification cannot be used e.g.¬†b \\times c would be chosen over -b as b \\times c can be antiunified with a \\times c. \\bigoplus represents unification.\n\n\n\nModularisation may also be performed if the program can be split into different levels of abstraction, and the search performed independently at each level. A potential optimisation here is if we can eliminate branches where the output is not a superset of the input set.\n\n\n\nTop down search can utilise types to assist in pruning invalid programs efficiently. A basic algorithm will start from the grammar rules and test with the inputs, pruning out programs that are fully expanded which don‚Äôt return the correct output or which return incompatible types. Some languages will have infinite types, e.g.¬†for supporting nested lists and functions. Typing rules have the following form:  \\frac{premises}{context ‚ä¢ expr : \\tau }  ‚ä¢ = proves or satisfies or is derived or assuming expr will have type \\tau in a given context as long as all the premises are satisfied. Alternatively:  \\frac{conditions}{Examples, ‚ä¢ expression : \\tau } \nAn example typing rule might be:  \\frac{\\bold{\\textit C},x : \\tau_1 ‚ä¢ expr : \\tau_2}{\\bold{\\textit C} ‚ä¢ \\bold{\\lambda}x.expr : \\tau_1 \\to \\tau_2 }  states the following: \\bold{\\lambda}x.expr will have type \\tau_1 \\to \\tau_2 if it can be shown that expr has type \\tau_2 in a context that is like the context \\bold{\\textit C} but that also has x as having type \\tau_1.\nThe search space can also be further restricted in further iterations/generations for functions like map; if the expected output is an array of integer arrays, the first expression must correspond to a function with a type of integer array, allowing filtering of programs that won‚Äôt have the desired type before evaluating any concrete input values.\n\n\nType rules are a form of deductive rule; information about inputs/outputs are propogated to potential sub-expressions. Additional rules for different constructs in the language can prune the solution space. Given a candidate expression with an unknown subexpression \\bold{\\textit f} and a set of input-outputs, the input-outputs can be propogated to the subexpression or establish that this line is not viable. Rules can inform the search if a candidate is not going to work e.g.¬†map will always return a list of the same length so if the input length is different, map alone will not be viable. They can also provide information on how to propogate input-outputs to new expressions. It may not work when expressions involve functions e.g.¬†if the same input value is mapped to multiple output values.\n\n\n\n\n\n\nA probabilistic process where there is a finite set of states $ $, and the probability of transitioning from a given state x to a different state y at each step of the process, is given by matrix $ (x,y): $ where $ (x,y) $ and $ _y (x,y) = 1 $. Becuase the values of \\bold{\\textit K} are probabilities, \\forall x,y. 0 \\le \\bold{\\textit K}(x,y) &lt; 1.0 and at every state there will be a transition (potentially to the same state) so \\forall x.\\sum_{y\\in{\\chi}} \\bold{\\textit K}(x,y) = 1.0.\nA markov chain is a sequence of states \\textit X_0, \\textit X_1, \\textit X_2,... in a markov process. The probability of the whole chain is the product of the probability of each transition:  (\\textit X_1 = y|\\textit X_0 = x) = \\bold{\\textit K}   \\bold{\\textit P}(\\textit X_1 = y, \\textit X_2 = z|\\textit X_0 = x) = \\bold{\\textit K}   \\bold{\\textit P}(\\textit X_2 = z|\\textit X_0 = x) = \\sum_y \\bold{\\textit K(x,y)} \\times \\bold{\\textit K(y,z)}  To obtain the probability that X_2 = z given a starting point of X_0 = x, then we need to consider all the possible states of X_1 to get from x to z. This is $ _y $. \\bold{\\textit K} is a matrix which gives the probability of transitioning from x to y in one step. \\bold{\\textit K}^2 is the probability of transitioning in 2 steps, and \\bold{\\textit K}^n (x,y) is the probability of transitioning from x to y in exactly n steps."
  },
  {
    "objectID": "posts/2023-04-03-inductive-program-synthesis.html#explicit",
    "href": "posts/2023-04-03-inductive-program-synthesis.html#explicit",
    "title": "Inductive Program Synthesis",
    "section": "",
    "text": "A simple implementation would be to construct all possible programs from the terminals of a grammar. This can be inefficient as the space of all expressions can grow quickly. Observationally equivalent programs are discarded (i.e.¬†if the same output is generated based on the same input, the primitive can be discarded), which reduces the solution space exponentially.\nfunction synthesise (inputs, outputs):\n  terms := set_of_all_terminals\n  while(true):\n    terms := grow(terms)\n    terms := eliminate_observational_equivalents(terms, inputs);\n    foreach(t in terms):\n      if(is_correct(t, inputs, outputs)):\n        return t;\nOne characteristic is that bottom-up search explores small programs before larger ones, so it can potentially find the smallest program that satisfies the specification. Further heuristics can be applied in the grow or eliminate function level to direct the search, and it copes well with symmetry reduction. However, even with aggressive pruning, it is hard to scale beyond small lists of terms, and whilst it can connect discrete components, it is not effective at determining constants. It also may fail at pruning if the language has context dependant semantics e.g.¬†if the same expression had dufferent values in different contexts e.g.¬†via mutation.\n\n\n\nBy modularising the search, i.e.¬†search for multiple programs that work for different inputs and find a way to combine them that will work for all inputs, the scalability can be improved. The first step is a best-effort synthesis (if it works on all then it is just returned), then it can try to either improve the current program by taking a selection of the currently failing inputs alongside the correctly mapped inputs, or STUN can be called recursively on the inputs where the synthesised program failed. Then the programs can be combined. #### Anti/Unification If there are no top-level branches to allow unification, antiunification can be used. Unification is finding a common structure for two different expressions by replacing variables with expressions. Antiunification is to process of finding the common structure by replacing expressions with variables. For example, if we find 2 expressions a \\times c and b \\times c cover all inputs, antiunification can produce a common expression v \\times c where v stands for a code fragment that can be solved, being a much smaller synthesis problem. When a recursive call to STUN is made, additional constraints can also be passed that the expression must satisfy to avoid situations where, for example, anntiunification cannot be used e.g.¬†b \\times c would be chosen over -b as b \\times c can be antiunified with a \\times c. \\bigoplus represents unification.\n\n\n\nModularisation may also be performed if the program can be split into different levels of abstraction, and the search performed independently at each level. A potential optimisation here is if we can eliminate branches where the output is not a superset of the input set.\n\n\n\nTop down search can utilise types to assist in pruning invalid programs efficiently. A basic algorithm will start from the grammar rules and test with the inputs, pruning out programs that are fully expanded which don‚Äôt return the correct output or which return incompatible types. Some languages will have infinite types, e.g.¬†for supporting nested lists and functions. Typing rules have the following form:  \\frac{premises}{context ‚ä¢ expr : \\tau }  ‚ä¢ = proves or satisfies or is derived or assuming expr will have type \\tau in a given context as long as all the premises are satisfied. Alternatively:  \\frac{conditions}{Examples, ‚ä¢ expression : \\tau } \nAn example typing rule might be:  \\frac{\\bold{\\textit C},x : \\tau_1 ‚ä¢ expr : \\tau_2}{\\bold{\\textit C} ‚ä¢ \\bold{\\lambda}x.expr : \\tau_1 \\to \\tau_2 }  states the following: \\bold{\\lambda}x.expr will have type \\tau_1 \\to \\tau_2 if it can be shown that expr has type \\tau_2 in a context that is like the context \\bold{\\textit C} but that also has x as having type \\tau_1.\nThe search space can also be further restricted in further iterations/generations for functions like map; if the expected output is an array of integer arrays, the first expression must correspond to a function with a type of integer array, allowing filtering of programs that won‚Äôt have the desired type before evaluating any concrete input values.\n\n\nType rules are a form of deductive rule; information about inputs/outputs are propogated to potential sub-expressions. Additional rules for different constructs in the language can prune the solution space. Given a candidate expression with an unknown subexpression \\bold{\\textit f} and a set of input-outputs, the input-outputs can be propogated to the subexpression or establish that this line is not viable. Rules can inform the search if a candidate is not going to work e.g.¬†map will always return a list of the same length so if the input length is different, map alone will not be viable. They can also provide information on how to propogate input-outputs to new expressions. It may not work when expressions involve functions e.g.¬†if the same input value is mapped to multiple output values.\n\n\n\n\n\n\nA probabilistic process where there is a finite set of states $ $, and the probability of transitioning from a given state x to a different state y at each step of the process, is given by matrix $ (x,y): $ where $ (x,y) $ and $ _y (x,y) = 1 $. Becuase the values of \\bold{\\textit K} are probabilities, \\forall x,y. 0 \\le \\bold{\\textit K}(x,y) &lt; 1.0 and at every state there will be a transition (potentially to the same state) so \\forall x.\\sum_{y\\in{\\chi}} \\bold{\\textit K}(x,y) = 1.0.\nA markov chain is a sequence of states \\textit X_0, \\textit X_1, \\textit X_2,... in a markov process. The probability of the whole chain is the product of the probability of each transition:  (\\textit X_1 = y|\\textit X_0 = x) = \\bold{\\textit K}   \\bold{\\textit P}(\\textit X_1 = y, \\textit X_2 = z|\\textit X_0 = x) = \\bold{\\textit K}   \\bold{\\textit P}(\\textit X_2 = z|\\textit X_0 = x) = \\sum_y \\bold{\\textit K(x,y)} \\times \\bold{\\textit K(y,z)}  To obtain the probability that X_2 = z given a starting point of X_0 = x, then we need to consider all the possible states of X_1 to get from x to z. This is $ _y $. \\bold{\\textit K} is a matrix which gives the probability of transitioning from x to y in one step. \\bold{\\textit K}^2 is the probability of transitioning in 2 steps, and \\bold{\\textit K}^n (x,y) is the probability of transitioning from x to y in exactly n steps."
  },
  {
    "objectID": "posts/2022-04-30-tensors.html",
    "href": "posts/2022-04-30-tensors.html",
    "title": "Tensors and Tensor Operations",
    "section": "",
    "text": "Transpose of a scalar is the scalar itself i.e.¬†x{^T} = x\n\n\n\nTransposing a vector transofrms columns to rows, and rows to columns i.e.\n\n\\begin{bmatrix} x{_{1,1}} & x{_{1,2}} \\\\ x{_{2,1}} & x{_{2,2}} \\\\ x{_{3,1}} & x{_{3,2}}\n\\end{bmatrix}{^T} =\n\\begin{bmatrix}\nx{_{1,1}} & x{_{2,1}} & x{_{3,1}} \\\\\nx{_{1,2}} & x{_{2,2}} & x{_{3,2}}\n\\end{bmatrix}\n\nScalar and vector transpoition are special cases of matrix transposition, where the axes are ‚Äúflipped‚Äù across the main diagonal.\n(\\boldsymbol X^T)_{i,j} = (\\boldsymbol X)_{j,i} \n\n\n\n\n\n\nCalculate the sum across all elements of the vector or matrix.\nVector ${i=1}^n x_i $\nMatrix ${i=1}^m {j=1}^n X{j,i} $\n\n\n\n\nDifferent to the Hadamard product as there is summation in the dot product (or scalar product) to produce a single scalar value.\nCan calculate the dot product if two vectors tensors x and y have the same length n and can be denoted as: \\boldsymbol X\\cdot \\boldsymbol Y ,\\boldsymbol X^T \\boldsymbol Y or \\lang \\boldsymbol X,\\boldsymbol Y \\rang. The product is calculated in an elemnent-wise pattern, then summed across the products to produce a scalar value:\n\\boldsymbol X \\cdot \\boldsymbol Y = \\sum_{i=1}^n x_iy_i"
  },
  {
    "objectID": "posts/2022-04-30-tensors.html#tensor-transposition",
    "href": "posts/2022-04-30-tensors.html#tensor-transposition",
    "title": "Tensors and Tensor Operations",
    "section": "",
    "text": "Transpose of a scalar is the scalar itself i.e.¬†x{^T} = x\n\n\n\nTransposing a vector transofrms columns to rows, and rows to columns i.e.\n\n\\begin{bmatrix} x{_{1,1}} & x{_{1,2}} \\\\ x{_{2,1}} & x{_{2,2}} \\\\ x{_{3,1}} & x{_{3,2}}\n\\end{bmatrix}{^T} =\n\\begin{bmatrix}\nx{_{1,1}} & x{_{2,1}} & x{_{3,1}} \\\\\nx{_{1,2}} & x{_{2,2}} & x{_{3,2}}\n\\end{bmatrix}\n\nScalar and vector transpoition are special cases of matrix transposition, where the axes are ‚Äúflipped‚Äù across the main diagonal.\n(\\boldsymbol X^T)_{i,j} = (\\boldsymbol X)_{j,i}"
  },
  {
    "objectID": "posts/2022-04-30-tensors.html#tensor-reduction",
    "href": "posts/2022-04-30-tensors.html#tensor-reduction",
    "title": "Tensors and Tensor Operations",
    "section": "",
    "text": "Calculate the sum across all elements of the vector or matrix.\nVector ${i=1}^n x_i $\nMatrix ${i=1}^m {j=1}^n X{j,i} $"
  },
  {
    "objectID": "posts/2022-04-30-tensors.html#dot-product",
    "href": "posts/2022-04-30-tensors.html#dot-product",
    "title": "Tensors and Tensor Operations",
    "section": "",
    "text": "Different to the Hadamard product as there is summation in the dot product (or scalar product) to produce a single scalar value.\nCan calculate the dot product if two vectors tensors x and y have the same length n and can be denoted as: \\boldsymbol X\\cdot \\boldsymbol Y ,\\boldsymbol X^T \\boldsymbol Y or \\lang \\boldsymbol X,\\boldsymbol Y \\rang. The product is calculated in an elemnent-wise pattern, then summed across the products to produce a scalar value:\n\\boldsymbol X \\cdot \\boldsymbol Y = \\sum_{i=1}^n x_iy_i"
  },
  {
    "objectID": "posts/2022-04-30-eigenvectors.html",
    "href": "posts/2022-04-30-eigenvectors.html",
    "title": "Eigenvectors and Eigenvalues",
    "section": "",
    "text": "Coming soon!\n\n\nCh 1-7 Strang 1 Ch 2 Goodfellow 2"
  },
  {
    "objectID": "posts/2022-04-30-eigenvectors.html#footnotes",
    "href": "posts/2022-04-30-eigenvectors.html#footnotes",
    "title": "Eigenvectors and Eigenvalues",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilbert Strang. Introduction to linear algebra. Cambridge Press, Wellesley, MA, 2016.‚Ü©Ô∏é\nIan Goodfellow et Al. Deep learning. MIT press, 2016.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html",
    "href": "posts/2022-04-19-ml-forms.html",
    "title": "Forms of ML",
    "section": "",
    "text": "Here are a few ways commonly used to classify ML systems:\n\nsupervision based\nincremental learning based\ngeneralisability\n\n\n\nIn this classification, there are four main classes based on the amount or type of supervision\n\n\n\nData\n\nLabelled examples \\large{({\\bold x_i},y_i)_{i=1}^N}\n\\large x_i is a feature vector(\\large n-dimensional vector of numerical features \\large x^d)\n\nrepresent objects numerically e.g.¬†for an image, \\large x^{(1)} could be the hue, and \\large x^{(2)} could be the intensity\nuseful for comparing objects e.g.¬†euclidean distance\ngranularity depends on the purpose\n\n\\large y_i can take the form of a member of a set, real number, matrix, vector etc.\n\nTasks\n\nClassification e.g.¬†spam filter\nPredict numeric values based on predictors (features) e.g.¬†house price given room numbers and areas\n\nGoal\n\nTrain a model on a dataset to predict labels based on input feature vectors\n\nMethods\n\nLinear regression\nLogistic regression\nkNN\nSVM\nDT (& random forests)\nNeural networks\n\n\n\n\n\n\nData\n\nUnlabelled examples \\large{({\\bold x_i})_{i=1}^N}\n\\large x_i is a feature vector\n\nTasks\n\nClustering - group data points with shared attributes to extrapolate a relationship e.g.¬†molecular structure similarity\nAnomaly/Outlier detection - find outliers e.g.¬†fraud-detection\nRule learning\n\nGoal\n\nClustering - transform feature vector \\large \\bold x into a useful value e.g.¬†an id\nDimensionality reduction - output feature vector should have fewer features than \\large \\bold x\nAnomaly/Outlier detection - output value quantifies the difference of \\large \\bold x from the data\n\nMethods\n\nClustering\n\nExclusive\n\nK-means\nHierarchical\n\nSoft - more probabilistic\n\nGMM\nExpectation Maximisation\n\n\nAssociation\n\nApriori\nEclat\n\nDimensionality reduction\n\nPCA\nSVD\nLLE\nt-SNE\n\n\nChallenges\n\nComputation and time complexity of training\nCan be unclear as to basis for clustering\nAccuracy\n\n\n\n\n\n\nData usually partially labelled\nCombination of supervised and unsupervised\nMethods\n\nDeep Belief Networks (DBN) - based on stacked Restricted Boltzmann machines\n\n\n\n\n\n\nAgent - learning system\n\nobserves the environment - state\nmakes decisions\nperforms actions\nfeedback loop - penalties or rewards - aims to maximise rewards\n\nLearns a policy - a function that takes the state as an input feature vector and outputs an action that leads to the highest expected average reward\n\n\n\n\n\nSplit into batch (non-incremental) and online\n\n\n\nOffline - unable to learn incrementally from a data stream (usually due to complexity)\nSystem trained first then applied, without learning further unless it is taken offline and retrained\nCan be automated e.g.¬†weekly\nHigh computational requirements\n\n\n\n\n\nTraining using small sequential chunks of data - streamed\nDoes not require storage of previous data\nLearning steps are of low complexity therefore can be performed on mobile systems\nCan also be used to process extremely large datasets as a stream\nLearning rate\n\nif high, will forget old data faster, sensitive to noise\nif low, inertia will be high, slower to learn, less sensitive to noise or outliers\n\nCorrupted or errors in the stream can affect the performance in real-time\n\npair with anomaly detection\n\n\n\n\n\n\nCan also categorise based on models of generalisation\n\n\n\nLearn from prior examples, then generalise to new data using a measure of similarity\n\n\n\n\n\nBuild or select a model from prior examples\nDefine a fitness function or cost function\nMinimise cost or maximise fitness, depending on the chosen model\nTrain the model on the training data to optimise parameters for a reasonable fit\nMake predictions\n\n\n\n\n\n\nBurkov, A. (2019) The Hundred-Page Machine Learning Book.\nGeron, A. (2017) Hands-On Machine Learning with Scikit-Learn & TensorFlow : concepts, tools, and techniques to build intelligent systems."
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html#supervision",
    "href": "posts/2022-04-19-ml-forms.html#supervision",
    "title": "Forms of ML",
    "section": "",
    "text": "In this classification, there are four main classes based on the amount or type of supervision\n\n\n\nData\n\nLabelled examples \\large{({\\bold x_i},y_i)_{i=1}^N}\n\\large x_i is a feature vector(\\large n-dimensional vector of numerical features \\large x^d)\n\nrepresent objects numerically e.g.¬†for an image, \\large x^{(1)} could be the hue, and \\large x^{(2)} could be the intensity\nuseful for comparing objects e.g.¬†euclidean distance\ngranularity depends on the purpose\n\n\\large y_i can take the form of a member of a set, real number, matrix, vector etc.\n\nTasks\n\nClassification e.g.¬†spam filter\nPredict numeric values based on predictors (features) e.g.¬†house price given room numbers and areas\n\nGoal\n\nTrain a model on a dataset to predict labels based on input feature vectors\n\nMethods\n\nLinear regression\nLogistic regression\nkNN\nSVM\nDT (& random forests)\nNeural networks\n\n\n\n\n\n\nData\n\nUnlabelled examples \\large{({\\bold x_i})_{i=1}^N}\n\\large x_i is a feature vector\n\nTasks\n\nClustering - group data points with shared attributes to extrapolate a relationship e.g.¬†molecular structure similarity\nAnomaly/Outlier detection - find outliers e.g.¬†fraud-detection\nRule learning\n\nGoal\n\nClustering - transform feature vector \\large \\bold x into a useful value e.g.¬†an id\nDimensionality reduction - output feature vector should have fewer features than \\large \\bold x\nAnomaly/Outlier detection - output value quantifies the difference of \\large \\bold x from the data\n\nMethods\n\nClustering\n\nExclusive\n\nK-means\nHierarchical\n\nSoft - more probabilistic\n\nGMM\nExpectation Maximisation\n\n\nAssociation\n\nApriori\nEclat\n\nDimensionality reduction\n\nPCA\nSVD\nLLE\nt-SNE\n\n\nChallenges\n\nComputation and time complexity of training\nCan be unclear as to basis for clustering\nAccuracy\n\n\n\n\n\n\nData usually partially labelled\nCombination of supervised and unsupervised\nMethods\n\nDeep Belief Networks (DBN) - based on stacked Restricted Boltzmann machines\n\n\n\n\n\n\nAgent - learning system\n\nobserves the environment - state\nmakes decisions\nperforms actions\nfeedback loop - penalties or rewards - aims to maximise rewards\n\nLearns a policy - a function that takes the state as an input feature vector and outputs an action that leads to the highest expected average reward"
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html#incremental",
    "href": "posts/2022-04-19-ml-forms.html#incremental",
    "title": "Forms of ML",
    "section": "",
    "text": "Split into batch (non-incremental) and online\n\n\n\nOffline - unable to learn incrementally from a data stream (usually due to complexity)\nSystem trained first then applied, without learning further unless it is taken offline and retrained\nCan be automated e.g.¬†weekly\nHigh computational requirements\n\n\n\n\n\nTraining using small sequential chunks of data - streamed\nDoes not require storage of previous data\nLearning steps are of low complexity therefore can be performed on mobile systems\nCan also be used to process extremely large datasets as a stream\nLearning rate\n\nif high, will forget old data faster, sensitive to noise\nif low, inertia will be high, slower to learn, less sensitive to noise or outliers\n\nCorrupted or errors in the stream can affect the performance in real-time\n\npair with anomaly detection"
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html#generalisability",
    "href": "posts/2022-04-19-ml-forms.html#generalisability",
    "title": "Forms of ML",
    "section": "",
    "text": "Can also categorise based on models of generalisation\n\n\n\nLearn from prior examples, then generalise to new data using a measure of similarity\n\n\n\n\n\nBuild or select a model from prior examples\nDefine a fitness function or cost function\nMinimise cost or maximise fitness, depending on the chosen model\nTrain the model on the training data to optimise parameters for a reasonable fit\nMake predictions"
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html#references",
    "href": "posts/2022-04-19-ml-forms.html#references",
    "title": "Forms of ML",
    "section": "",
    "text": "Burkov, A. (2019) The Hundred-Page Machine Learning Book.\nGeron, A. (2017) Hands-On Machine Learning with Scikit-Learn & TensorFlow : concepts, tools, and techniques to build intelligent systems."
  },
  {
    "objectID": "posts/2022-04-19-data-types.html",
    "href": "posts/2022-04-19-data-types.html",
    "title": "Data in ML",
    "section": "",
    "text": "Common taxonomy of data types when considering machine learning\n\nCategorical: qualitative\n\nOrdinal: innate ordered values with unknown distances between them that cannot be measured\n\ne.g.¬†first/second/third, good/bad\n\nNominal: values (text or numbers) with no order\n\ne.g.¬†cat/dog, genre, ethnicity\n\n\nNumerical: quantitative\n\nDiscrete: quantitative whole number values\n\ne.g.¬†step count\n\nContinuous: quantitative decimal values\n\ne.g.¬†width, height"
  },
  {
    "objectID": "posts/2022-04-19-data-types.html#footnotes",
    "href": "posts/2022-04-19-data-types.html#footnotes",
    "title": "Data in ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://economistjourney.blogspot.com/2018/06/what-is-sampling-noise.html.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-02-05-matrices.html",
    "href": "posts/2022-02-05-matrices.html",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "||\\boldsymbol{X}||{_F} = \\sqrt{\\displaystyle\\sum_{i,j} X_{i,j}{^2} }\nFor i rows, and j columns, sum the square of each element, then take the square root. This is analogous to the L^2 norm of vectors, and measures the Euclidean size of the matrix. Can also be thought of as the sum of the magnitude of all the vectors in \\boldsymbol{X}.\n\n\n\nNumber of columns in the first matrix must match the number of rows in the second, i.e.¬†in the following example, m is the number of rows in the first matrix, n is the number of columns in the first matrix and the number of rows in the second, and p is the number of columns in the second matrix.\n m\\underset{n}{\\begin{bmatrix} \\, \\\\ A \\\\ \\,\\end{bmatrix}} \\quad\nn\\underset{p}{\\begin{bmatrix}& B &\\end{bmatrix}} = m\\underset{p}{\\begin{bmatrix} \\, \\\\ C \\\\ \\,\\end{bmatrix}}\n\n C_{i,k} = \\displaystyle\\sum_{j}A_{i,j}B_{j,k} \nMultiplying a matrix with a vector:\n\n\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3*1 + 4*2\\\\ 5*1+6*2 \\\\ 7*1+8*2 \\end{bmatrix} = \\begin{bmatrix} 11 \\\\ 17 \\\\ 23\\end{bmatrix}\n\nMultiplying a matrix with a matrix:\n\n\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n\\begin{bmatrix} 1 & 9\\\\ 2 & 0 \\end{bmatrix} = \\begin{bmatrix} 3*1 + 4*2 & 3*9+4*0\\\\ 5*1+6*2 & 5*9+6*0\\\\ 7*1+8*2 &7*9+8*0 \\end{bmatrix} = \\begin{bmatrix} 11 & 27\\\\ 17 & 45\\\\ 23&63\\end{bmatrix}\n\nMatrix multiplication is typically not commutative: \\boldsymbol{X}\\boldsymbol{Y} \\not= \\boldsymbol{Y}\\boldsymbol{X}\nIf both matrices are square, multiplying either way will work, but the results will usually be different.\nCan represent the following regression as a matrix multiplication:\n\n\\begin{bmatrix}\ny{_1} |\\alpha + \\beta x{_{1,1}} + \\gamma x{_{1,2}} + ... + m_{-1}x{_{1,m_{-1}}} \\\\\ny{_2} | \\alpha + \\beta x{_{2,1}} + \\gamma x{_{2,2}} + ... + m_{-1}x{_{2,m_{-1}}} \\\\\n\\vdots \\\\\ny{_n} |  \\alpha + \\beta x{_{n,1}} + \\gamma x{_{n,2}} + ... + m_{-1}x{_{n,m_{-1}}}\n\\end{bmatrix} \\\\ \\begin{bmatrix} y{_1} \\\\ y{_2} \\\\ \\vdots \\\\ y{_n}\\end{bmatrix} =\\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & \\dots & X_{1,m_{-1}}\\\\ 1 & X_{2,1} & X_{2,2} & \\dots & X_{2,m_{-1}} \\\\ \\vdots \\\\ 1 & X_{n,1} & X_{n,2} & \\dots & X_{n,m_{-1}} \\end{bmatrix}\n\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\vdots \\\\ m_{-1}\\end{bmatrix}\n\nn cases tall, m features wide.\n\n\n\n\n\nNumber of rows = number of columns i.e.¬†square Transpose is equal to itself i.e.¬†any value on the diagonal but the values either side must be equal.\n\n\n\nAn identity matrix is a symmetric matrix where every element along the main diagonal is 1, all others are 0. This satisfies the condition of being an identity as, if an identity matrix is multiplied by a n length vector, it will remain unchanged i.e.¬†we get the same vector returned.\nNotation \\boldsymbol{I}_n where n = height or width.\n1 is the identity element for multiplication - the number which when operated on with any other number results in the other number. 0 is the identity element for addition, and 1 is the identity element for multiplication. Whenever the identity element for an operation is the solution, the two items operated on are inverses of each other e.g.¬†5x = 15, both sides can be multiplied by the multiplicative inverse i.e.¬†\\frac{1}{5} or 5^{-1} to get x=3. A number multiplied by its reciprocal or inverse always returns 1 e.g.¬†\\frac{1}{5} * 5 = 1. Similarly, a matrix multiplied by its inverse, will return the identity matrix.\n\n\n\n\nUsed to computationally solve linear equations. Matrix inverse is denoted \\boldsymbol{X}^{-1}. It satisfies the following property:\n\\boldsymbol{X}^{-1} \\boldsymbol{X} = \\boldsymbol{X}\\boldsymbol{X}^{-1}  = \\boldsymbol{I}_n\n2x2 Matrix inversion: \n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nwhere {ad-bc} is the determinant (which cannot be 0).\nCan‚Äôt divide by a matrix, but can multiply by an inverse to get the same result e.g. we know matrix \\boldsymbol{A} and \\boldsymbol{B}, and want to get the values for matrix \\boldsymbol{X}:\n\\boldsymbol{X} \\boldsymbol{A} = \\boldsymbol{B}\nWe can‚Äôt divide by \\boldsymbol{A}, but can multiply both sides by the inverse of \\boldsymbol{A}:\n$ ^{-1} = ^{-1}$ and we know that \\boldsymbol{A}\\boldsymbol{A}^{-1} = \\boldsymbol{I}\nwhich gives us\n$ = ^{-1}$\nand \\boldsymbol{I} can be ignored, just like 1 would be during multiplication:\n$ = ^{-1}$\nIf a matrix has no inverse i.e.¬†the determinant is zero, then it is called singular.\nMatrix inversion can also only be calculated if the matrix is square i.e.¬†the vector span (number of rows) is equal to the matrix range (number of cols). This avoids overdetermination (rows &gt; cols) i.e.¬†number of equations &gt; number of dimensions (multiple points of overlap), and underdetermination (rows &lt; cols) i.e.¬†number of equations are less than dimensions (no overlap as single line) - may still be able to solve these, but can‚Äôt invert a matrix as a solution.\n\n\n\n\\begin{bmatrix} y{_1} \\\\ y{_2} \\\\ \\vdots \\\\ y{_n}\\end{bmatrix} =\\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & \\dots & X_{1,m_{-1}}\\\\ 1 & X_{2,1} & X_{2,2} & \\dots & X_{2,m_{-1}} \\\\ \\vdots \\\\ 1 & X_{n,1} & X_{n,2} & \\dots & X_{n,m_{-1}} \\end{bmatrix}\n\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\vdots \\\\ m_{-1}\\end{bmatrix}\n Would like to solve for the vector of variables on the right, in this regression model. This can be represented as: \\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{w} where \\boldsymbol{w} is the vector of weights, \\alpha to m_{-1}, corresponding to the number of columns in our feature matrix.\nIn this equation, \\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{w}, we know the outcomes \\boldsymbol{y}, and we know the features \\boldsymbol{X}, and we wish to work out the weights, \\boldsymbol{w}, which we can do using matrix inversion, assuming the inverse matrix exists i.e.¬†it isn‚Äôt singular - all columns must be linearly independent e.g.¬†one column should not be a multiple of another one like [1,2] - another can‚Äôt be [2,4] or [1,2] again as we would have paralell lines or overlapping lines respectively.\nWe can multiply both sides by the inverse of \\boldsymbol{X}, \\boldsymbol{X}^{-1} to obtain:\n$^{-1} = ^{-1} $ and given \\boldsymbol{X}^{-1}\\boldsymbol{X} = \\boldsymbol{I}, we can simplify this to:\n$ = ^{-1} $ and given the property of the identity matrix, we can remove it to leave:\n$ = ^{-1} $\nThe order of operations is key to preserve, as matrix multiplication is not always commutative i.e.¬†$ = $\n\n\n\nNon-zero elements along the main diagonal, zero elsewhere e.g.¬†identity matrix. If it is square, denoted as diag(\\boldsymbol{x}) where \\boldsymbol{x} is a vector of main diagonal elements e.g.¬†[1,1,1] to represent \\boldsymbol{I}_3. Much more computationally efficient: - Multiplication: diag(\\boldsymbol{x})\\boldsymbol{y} = \\boldsymbol{x}\\odot{}\\boldsymbol{y} - Inversion: $diag()^{-1} = diag(,,)^T $ - Can‚Äôt divide by zero, so if any elemenet along the diagonal contains zero, the diagonal can‚Äôt be inverted - Can be non-square and computation can still be efficient e.g.¬†by adding zeros to the product if height &gt; width, or by removing elements from the product if the width &gt; height\n\n\n\nIn orthogonal matrices, orthonormal vectors make up all rows and columns. This means that \\boldsymbol{A}^{T} \\boldsymbol{A} = \\boldsymbol{A}\\boldsymbol{A}^{T}  = \\boldsymbol{I} and also that \\boldsymbol{A}^{T} = \\boldsymbol{A}^{-1}\\boldsymbol{I} = \\boldsymbol{A}^{-1}. Calculating \\boldsymbol{A}^{T} is cheap, and therefore so is \\boldsymbol{A}^{-1} for orthogonal matrices."
  },
  {
    "objectID": "posts/2022-02-05-matrices.html#norms---frobenius-norm",
    "href": "posts/2022-02-05-matrices.html#norms---frobenius-norm",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "||\\boldsymbol{X}||{_F} = \\sqrt{\\displaystyle\\sum_{i,j} X_{i,j}{^2} }\nFor i rows, and j columns, sum the square of each element, then take the square root. This is analogous to the L^2 norm of vectors, and measures the Euclidean size of the matrix. Can also be thought of as the sum of the magnitude of all the vectors in \\boldsymbol{X}."
  },
  {
    "objectID": "posts/2022-02-05-matrices.html#multiplication",
    "href": "posts/2022-02-05-matrices.html#multiplication",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "Number of columns in the first matrix must match the number of rows in the second, i.e.¬†in the following example, m is the number of rows in the first matrix, n is the number of columns in the first matrix and the number of rows in the second, and p is the number of columns in the second matrix.\n m\\underset{n}{\\begin{bmatrix} \\, \\\\ A \\\\ \\,\\end{bmatrix}} \\quad\nn\\underset{p}{\\begin{bmatrix}& B &\\end{bmatrix}} = m\\underset{p}{\\begin{bmatrix} \\, \\\\ C \\\\ \\,\\end{bmatrix}}\n\n C_{i,k} = \\displaystyle\\sum_{j}A_{i,j}B_{j,k} \nMultiplying a matrix with a vector:\n\n\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3*1 + 4*2\\\\ 5*1+6*2 \\\\ 7*1+8*2 \\end{bmatrix} = \\begin{bmatrix} 11 \\\\ 17 \\\\ 23\\end{bmatrix}\n\nMultiplying a matrix with a matrix:\n\n\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n\\begin{bmatrix} 1 & 9\\\\ 2 & 0 \\end{bmatrix} = \\begin{bmatrix} 3*1 + 4*2 & 3*9+4*0\\\\ 5*1+6*2 & 5*9+6*0\\\\ 7*1+8*2 &7*9+8*0 \\end{bmatrix} = \\begin{bmatrix} 11 & 27\\\\ 17 & 45\\\\ 23&63\\end{bmatrix}\n\nMatrix multiplication is typically not commutative: \\boldsymbol{X}\\boldsymbol{Y} \\not= \\boldsymbol{Y}\\boldsymbol{X}\nIf both matrices are square, multiplying either way will work, but the results will usually be different.\nCan represent the following regression as a matrix multiplication:\n\n\\begin{bmatrix}\ny{_1} |\\alpha + \\beta x{_{1,1}} + \\gamma x{_{1,2}} + ... + m_{-1}x{_{1,m_{-1}}} \\\\\ny{_2} | \\alpha + \\beta x{_{2,1}} + \\gamma x{_{2,2}} + ... + m_{-1}x{_{2,m_{-1}}} \\\\\n\\vdots \\\\\ny{_n} |  \\alpha + \\beta x{_{n,1}} + \\gamma x{_{n,2}} + ... + m_{-1}x{_{n,m_{-1}}}\n\\end{bmatrix} \\\\ \\begin{bmatrix} y{_1} \\\\ y{_2} \\\\ \\vdots \\\\ y{_n}\\end{bmatrix} =\\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & \\dots & X_{1,m_{-1}}\\\\ 1 & X_{2,1} & X_{2,2} & \\dots & X_{2,m_{-1}} \\\\ \\vdots \\\\ 1 & X_{n,1} & X_{n,2} & \\dots & X_{n,m_{-1}} \\end{bmatrix}\n\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\vdots \\\\ m_{-1}\\end{bmatrix}\n\nn cases tall, m features wide."
  },
  {
    "objectID": "posts/2022-02-05-matrices.html#symmetric-identity-matrices",
    "href": "posts/2022-02-05-matrices.html#symmetric-identity-matrices",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "Number of rows = number of columns i.e.¬†square Transpose is equal to itself i.e.¬†any value on the diagonal but the values either side must be equal.\n\n\n\nAn identity matrix is a symmetric matrix where every element along the main diagonal is 1, all others are 0. This satisfies the condition of being an identity as, if an identity matrix is multiplied by a n length vector, it will remain unchanged i.e.¬†we get the same vector returned.\nNotation \\boldsymbol{I}_n where n = height or width.\n1 is the identity element for multiplication - the number which when operated on with any other number results in the other number. 0 is the identity element for addition, and 1 is the identity element for multiplication. Whenever the identity element for an operation is the solution, the two items operated on are inverses of each other e.g.¬†5x = 15, both sides can be multiplied by the multiplicative inverse i.e.¬†\\frac{1}{5} or 5^{-1} to get x=3. A number multiplied by its reciprocal or inverse always returns 1 e.g.¬†\\frac{1}{5} * 5 = 1. Similarly, a matrix multiplied by its inverse, will return the identity matrix."
  },
  {
    "objectID": "posts/2022-02-05-matrices.html#matrix-inversion",
    "href": "posts/2022-02-05-matrices.html#matrix-inversion",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "Used to computationally solve linear equations. Matrix inverse is denoted \\boldsymbol{X}^{-1}. It satisfies the following property:\n\\boldsymbol{X}^{-1} \\boldsymbol{X} = \\boldsymbol{X}\\boldsymbol{X}^{-1}  = \\boldsymbol{I}_n\n2x2 Matrix inversion: \n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nwhere {ad-bc} is the determinant (which cannot be 0).\nCan‚Äôt divide by a matrix, but can multiply by an inverse to get the same result e.g. we know matrix \\boldsymbol{A} and \\boldsymbol{B}, and want to get the values for matrix \\boldsymbol{X}:\n\\boldsymbol{X} \\boldsymbol{A} = \\boldsymbol{B}\nWe can‚Äôt divide by \\boldsymbol{A}, but can multiply both sides by the inverse of \\boldsymbol{A}:\n$ ^{-1} = ^{-1}$ and we know that \\boldsymbol{A}\\boldsymbol{A}^{-1} = \\boldsymbol{I}\nwhich gives us\n$ = ^{-1}$\nand \\boldsymbol{I} can be ignored, just like 1 would be during multiplication:\n$ = ^{-1}$\nIf a matrix has no inverse i.e.¬†the determinant is zero, then it is called singular.\nMatrix inversion can also only be calculated if the matrix is square i.e.¬†the vector span (number of rows) is equal to the matrix range (number of cols). This avoids overdetermination (rows &gt; cols) i.e.¬†number of equations &gt; number of dimensions (multiple points of overlap), and underdetermination (rows &lt; cols) i.e.¬†number of equations are less than dimensions (no overlap as single line) - may still be able to solve these, but can‚Äôt invert a matrix as a solution.\n\n\n\n\\begin{bmatrix} y{_1} \\\\ y{_2} \\\\ \\vdots \\\\ y{_n}\\end{bmatrix} =\\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & \\dots & X_{1,m_{-1}}\\\\ 1 & X_{2,1} & X_{2,2} & \\dots & X_{2,m_{-1}} \\\\ \\vdots \\\\ 1 & X_{n,1} & X_{n,2} & \\dots & X_{n,m_{-1}} \\end{bmatrix}\n\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\vdots \\\\ m_{-1}\\end{bmatrix}\n Would like to solve for the vector of variables on the right, in this regression model. This can be represented as: \\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{w} where \\boldsymbol{w} is the vector of weights, \\alpha to m_{-1}, corresponding to the number of columns in our feature matrix.\nIn this equation, \\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{w}, we know the outcomes \\boldsymbol{y}, and we know the features \\boldsymbol{X}, and we wish to work out the weights, \\boldsymbol{w}, which we can do using matrix inversion, assuming the inverse matrix exists i.e.¬†it isn‚Äôt singular - all columns must be linearly independent e.g.¬†one column should not be a multiple of another one like [1,2] - another can‚Äôt be [2,4] or [1,2] again as we would have paralell lines or overlapping lines respectively.\nWe can multiply both sides by the inverse of \\boldsymbol{X}, \\boldsymbol{X}^{-1} to obtain:\n$^{-1} = ^{-1} $ and given \\boldsymbol{X}^{-1}\\boldsymbol{X} = \\boldsymbol{I}, we can simplify this to:\n$ = ^{-1} $ and given the property of the identity matrix, we can remove it to leave:\n$ = ^{-1} $\nThe order of operations is key to preserve, as matrix multiplication is not always commutative i.e.¬†$ = $\n\n\n\nNon-zero elements along the main diagonal, zero elsewhere e.g.¬†identity matrix. If it is square, denoted as diag(\\boldsymbol{x}) where \\boldsymbol{x} is a vector of main diagonal elements e.g.¬†[1,1,1] to represent \\boldsymbol{I}_3. Much more computationally efficient: - Multiplication: diag(\\boldsymbol{x})\\boldsymbol{y} = \\boldsymbol{x}\\odot{}\\boldsymbol{y} - Inversion: $diag()^{-1} = diag(,,)^T $ - Can‚Äôt divide by zero, so if any elemenet along the diagonal contains zero, the diagonal can‚Äôt be inverted - Can be non-square and computation can still be efficient e.g.¬†by adding zeros to the product if height &gt; width, or by removing elements from the product if the width &gt; height\n\n\n\nIn orthogonal matrices, orthonormal vectors make up all rows and columns. This means that \\boldsymbol{A}^{T} \\boldsymbol{A} = \\boldsymbol{A}\\boldsymbol{A}^{T}  = \\boldsymbol{I} and also that \\boldsymbol{A}^{T} = \\boldsymbol{A}^{-1}\\boldsymbol{I} = \\boldsymbol{A}^{-1}. Calculating \\boldsymbol{A}^{T} is cheap, and therefore so is \\boldsymbol{A}^{-1} for orthogonal matrices."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Notes from my studies and interests. This is a work in progress and is constantly under refinement."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "notes",
    "section": "",
    "text": "Duplicate Items\n\n\n\n\n\n\ndsa\n\n\nalgorithms\n\n\ndata-structures\n\n\nbasics\n\n\n\nAnalysis of different ways to solve finding duplicate items in an unsorted list\n\n\n\n\n\nDec 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAnagram\n\n\n\n\n\n\ndsa\n\n\nalgorithms\n\n\ndata-structures\n\n\nbasics\n\n\n\nAnalysis of different ways to solve finding out if two input strings are anagrams\n\n\n\n\n\nDec 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Distributed Systems\n\n\nArchitectural Principles and Challenges\n\n\n\narchitecture\n\n\ncomputing\n\n\nnetworking\n\n\ninfrastructure\n\n\n\nOverview of distributed systems architecture, challenges, and implementation strategies\n\n\n\n\n\nNov 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Quantum Programming\n\n\n\n\n\n\nprogramming\n\n\nquantum\n\n\nintroduction\n\n\nmaths\n\n\n\nQuantum Programming Concepts Introduction\n\n\n\n\n\nSep 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to PostgreSQL\n\n\n\n\n\n\nprogramming\n\n\nsql\n\n\nintroduction\n\n\npostgres\n\n\n\nPostgreSQL Introduction\n\n\n\n\n\nSep 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInverting a Binary Tree\n\n\n\n\n\n\nDSA\n\n\ndata-structures\n\n\nintroduction\n\n\nalgorithms\n\n\npython\n\n\nbinary-tree\n\n\n\nBinary Tree Inversion\n\n\n\n\n\nSep 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Binary Search\n\n\n\n\n\n\nDSA\n\n\ndata-structures\n\n\nintroduction\n\n\nalgorithms\n\n\npython\n\n\n\nBinary Search Introduction\n\n\n\n\n\nAug 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Linked Lists\n\n\n\n\n\n\nDSA\n\n\ndata-structures\n\n\nintroduction\n\n\nlinked-lists\n\n\npython\n\n\n\nLinked Lists Introduction\n\n\n\n\n\nAug 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDivision Based Hash Functions\n\n\n\n\n\n\nDSA\n\n\ndata-structures\n\n\nintroduction\n\n\nprime-numbers\n\n\nhash-functions\n\n\n\nData Structures and Algorithms - Division Based Hash Functions\n\n\n\n\n\nAug 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Basic Data Structures\n\n\n\n\n\n\nDSA\n\n\ndata structures\n\n\nintroduction\n\n\n\nData Structures and Algorithms - Basic Data Structures\n\n\n\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to CEGIS\n\n\n\n\n\n\nprogram-synthesis\n\n\nCEGIS\n\n\nintroduction\n\n\n\nNotes about counter example guided inductive programsynthesis\n\n\n\n\n\nApr 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Functional Programming In Python\n\n\n\n\n\n\nprogramming\n\n\nfunctional-programming\n\n\nintroduction\n\n\npython\n\n\n\nNotes about functional programming in python\n\n\n\n\n\nApr 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nInductive Program Synthesis\n\n\n\n\n\n\nprogramming\n\n\ninductive-synthesis\n\n\nalgorithm\n\n\n\nNotes about inductive program synthesis\n\n\n\n\n\nApr 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Program Synthesis\n\n\n\n\n\n\nprogramming\n\n\nintroduction\n\n\nprogram-synthesis\n\n\n\nNotes about basic terms and concepts\n\n\n\n\n\nApr 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEigenvectors and Eigenvalues\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\neigenvectors\n\n\neigenvalues\n\n\nintroduction\n\n\n\nEigenvectors and Eigenvalues\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Linear Algebra Concepts\n\n\n\n\n\n\njupyter\n\n\nmaths\n\n\nlinear-algebra\n\n\ntensors\n\n\nmatrices\n\n\n\nExamining some basic concepts of liner algebra using python libraries\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nTensors and Tensor Operations\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\ntensors\n\n\n\nTensors\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Linear Algebra\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\nintroduction\n\n\n\nIntroduction and basic concepts of linear algebra\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nTraining, Testing, Validation Datasets and Cross-Validation\n\n\n\n\n\n\ndata\n\n\nML\n\n\ntraining\n\n\ntesting\n\n\nvalidation\n\n\ncross-validation\n\n\n\nDefining the concepts of testing, training and validation sets\n\n\n\n\n\nApr 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nData in ML\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\nstatistics\n\n\nML\n\n\n\nData Types, Quality & Quantity\n\n\n\n\n\nApr 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Scaling in ML\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\nstatistics\n\n\nML\n\n\nfeature-scaling\n\n\n\nRescaling, normalisation, standardisation\n\n\n\n\n\nApr 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nForms of ML\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\nstatistics\n\n\nML\n\n\n\nML types\n\n\n\n\n\nApr 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMaths for ML\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\nstatistics\n\n\ncalculus\n\n\n\nLinks to notes for maths for ML\n\n\n\n\n\nApr 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Properties and Operations\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\nmatrix\n\n\nintroduction\n\n\n\nMatrix properties and Operations\n\n\n\n\n\nFeb 5, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-04-18-maths-for-ml.html",
    "href": "posts/2022-04-18-maths-for-ml.html",
    "title": "Maths for ML",
    "section": "",
    "text": "Links to my notes on learning maths for ML.\n\n\n\n\n\nIntroduction & Notation\nMatrices\nTensors\nEigenvectors\n\nCh 1-7 Strang 1 Ch 2 Goodfellow 2\n\n\n\nCh 3 Goodfellow 3 Ch 1 Barber 4\n\n\n\nAppendix 2 Barber 5"
  },
  {
    "objectID": "posts/2022-04-18-maths-for-ml.html#topics",
    "href": "posts/2022-04-18-maths-for-ml.html#topics",
    "title": "Maths for ML",
    "section": "",
    "text": "Introduction & Notation\nMatrices\nTensors\nEigenvectors\n\nCh 1-7 Strang 1 Ch 2 Goodfellow 2\n\n\n\nCh 3 Goodfellow 3 Ch 1 Barber 4\n\n\n\nAppendix 2 Barber 5"
  },
  {
    "objectID": "posts/2022-04-18-maths-for-ml.html#footnotes",
    "href": "posts/2022-04-18-maths-for-ml.html#footnotes",
    "title": "Maths for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilbert Strang. Introduction to linear algebra. Cambridge Press, Wellesley, MA, 2016.‚Ü©Ô∏é\nIan Goodfellow et Al. Deep learning. MIT press, 2016.‚Ü©Ô∏é\nIan Goodfellow et Al. Deep learning. MIT press, 2016.‚Ü©Ô∏é\nDavid Barber. Bayesian reasoning and machine learning. Cambridge University Press, 2012.‚Ü©Ô∏é\nDavid Barber. Bayesian reasoning and machine learning. Cambridge University Press, 2012.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html",
    "href": "posts/2022-04-19-feature-scaling.html",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "For classifiers that calculate distances between points, e.g.¬†Euclidean kNN or SVM, one feature can dominate the others purely due to scale when proportional contributions of all the features are wanted. Feature scaling (particularly standardisation) can also increase the convergence speed of gradient descent algorithms by altering the step size for each feature. Tree based algorithms and others (e.g.¬†NB, LDA) are insensitive to the scale of the features as the decision to split at that node is not influenced by other features, rather the decision to split is usually based on what will decrease the variance or increase the homogeneity in the sub-nodes.\nGenerally, normalisation is applied when the distribution of the data is known to not be gaussian, e.g.¬†for algorithms that do not presume a distribution e.g.¬†kNN. It is important to consider outliers as normalising data with outliers will scale potentially important features into a small interval, so they must be handled first.\n\n\n\nPrevents the scale of the variable from over-influencing the model due to unit difference\nScale features or variables to a common scale without distortion of differences in the range\n\ne.g.¬†[0,1]\n\nExamples\n\nRescaling (min-max normalisation) to a range of [0,1] or [-1,-1] or [a,b]\n\nx'= \\frac {x-min(x)} {max(x)-min(x)}\nx'= \\frac {(x-min(x))(b-a)} {max(x)-min(x)}\n\nMean normalisation\n\nx'= \\frac {x-average(x)} {max(x)-min(x)}\n\n\n\n\n\n\n\nStandardise to find out how many standard deviations the values have from the mean\nStandardisation will scale the data to have a mean of 0 and standard deviation of 1\nNo specific upper or lower bound of values\nMethods:\n\nZ-Score: for population mean and std.dev.\n\n\\LARGE z = \\frac{x - \\mu}{\\sigma}\n\nZ-Score (estimated): for sample mean and std.dev.(\\large S)\n\n\\LARGE z = \\frac{x - \\bar x}{S}\n\n\nUsages:\n\nStandardising regression coefficients\nStandardising variables prior to PCA\n\n\n\n\n\n\nScale the components of a feature vector such that the overall vector has length 1\ne.g.¬†dividing by the Euclidean length of the vector\n\\large x'= \\frac {x} {||x||}\n\n\n\n\n\nWikipedia - Feature Scaling\nKDNuggets - Feature Scaling"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html#normalisation",
    "href": "posts/2022-04-19-feature-scaling.html#normalisation",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "Prevents the scale of the variable from over-influencing the model due to unit difference\nScale features or variables to a common scale without distortion of differences in the range\n\ne.g.¬†[0,1]\n\nExamples\n\nRescaling (min-max normalisation) to a range of [0,1] or [-1,-1] or [a,b]\n\nx'= \\frac {x-min(x)} {max(x)-min(x)}\nx'= \\frac {(x-min(x))(b-a)} {max(x)-min(x)}\n\nMean normalisation\n\nx'= \\frac {x-average(x)} {max(x)-min(x)}"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html#standardisation",
    "href": "posts/2022-04-19-feature-scaling.html#standardisation",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "Standardise to find out how many standard deviations the values have from the mean\nStandardisation will scale the data to have a mean of 0 and standard deviation of 1\nNo specific upper or lower bound of values\nMethods:\n\nZ-Score: for population mean and std.dev.\n\n\\LARGE z = \\frac{x - \\mu}{\\sigma}\n\nZ-Score (estimated): for sample mean and std.dev.(\\large S)\n\n\\LARGE z = \\frac{x - \\bar x}{S}\n\n\nUsages:\n\nStandardising regression coefficients\nStandardising variables prior to PCA"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html#scaling-to-unit-length",
    "href": "posts/2022-04-19-feature-scaling.html#scaling-to-unit-length",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "Scale the components of a feature vector such that the overall vector has length 1\ne.g.¬†dividing by the Euclidean length of the vector\n\\large x'= \\frac {x} {||x||}"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html#references",
    "href": "posts/2022-04-19-feature-scaling.html#references",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "Wikipedia - Feature Scaling\nKDNuggets - Feature Scaling"
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html",
    "href": "posts/2022-04-27-testing-validation.html",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "To build a mathematical model to make predictions, data is needed to train and test the model, for which three data sets are usually used. Typically, the data is split into a training set and testing set (80-20 or 70-30 etc.), or a training, testing and validation set. Cross-Validation can be particularly helpful for smaller datasets, helping to avoid over-fitting and hyperparameter tuning.\n\n\nDataset used to create a model to be used for predictions. The model can then be applied to the training data sets to assess the accuracy of predictions against labelled data. The model can then be tuned if necessary.\n\n\n\nThe model can then be applied to a testing set, data which has not been used to make the model, from which an error rate can be estimated (generalisation error).\nIf the training set error rate is low and the testing set error rate is high, it suggests the model has been over-fitted to the training data set.\n\n\n\nAnother holdout dataset can be used is the validation set. It provides an unbiased estimation of the performance of the model which can be useful when assessing multiple models to minimise over-fitting. Multiple models can be trained with different hyperparameters, and evaluated against the validation set, with the best performing hyperparameters and model being used on the test dataset to estimate the generalisation error. They can also be used to stop training early on a model where the error on the validation set increases beyond a specified limit.\n\n\n\nIf the dataset size is relatively smaller to begin with, to avoid issues with trying to train a model on less data, cross-validation can be used. The training dataset can be split up into subsets or subsamples. Each model can be trained against combinations of these and validated against the remaining ones. Then the final model can be trained, using the best performing hyperparameters, on the complete training set, so all observations have been used.\n\n\n\nK-folding\n\nSplit the data int k-subsets e.g.¬†3,5, 10 etc.\nBuild k models, each trained on k-1 and tested on the datasets they haven‚Äôt been trained on\n\nLeave p out\n\nUse p observations as the validation set, and the remaining as the training set\nThis is repeated for as many variations of sampling p elements out of the total observations\nif p&gt;1, this will mean training and validating C{n \\choose p} times, where n is the number of observations in the original sample\nif p=1, as in leave-one-out, this will mean training and validating C{n \\choose 1} times. If n is large, k-fold may be less computationally expensive.\n\nStratified k-fold\n\nEnsure that each fold is a representative sample of the whole population of observations e.g.¬†mean is similar in all the subsets or proportions for binary classifiers\n\nRepeated random sub-sampling\n\nMonte Carlo cross validation - create multiple random training and validation subsets, then for each split, fit the model to the training subset and assess the accuracy on the validation subset. Then average the results over the subsets.\nValidation subsets may overlap, or some data points may never be used due to the random sampling.\nCan also be stratified by the mean of the subsets\n\n\n\n\n\n\n\nAny two optimisation algorithms are equivalent when their performance is averaged across all possible problems, if no assumptions are made about the data."
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#training-set",
    "href": "posts/2022-04-27-testing-validation.html#training-set",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "Dataset used to create a model to be used for predictions. The model can then be applied to the training data sets to assess the accuracy of predictions against labelled data. The model can then be tuned if necessary."
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#testing-set",
    "href": "posts/2022-04-27-testing-validation.html#testing-set",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "The model can then be applied to a testing set, data which has not been used to make the model, from which an error rate can be estimated (generalisation error).\nIf the training set error rate is low and the testing set error rate is high, it suggests the model has been over-fitted to the training data set."
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#validation-set",
    "href": "posts/2022-04-27-testing-validation.html#validation-set",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "Another holdout dataset can be used is the validation set. It provides an unbiased estimation of the performance of the model which can be useful when assessing multiple models to minimise over-fitting. Multiple models can be trained with different hyperparameters, and evaluated against the validation set, with the best performing hyperparameters and model being used on the test dataset to estimate the generalisation error. They can also be used to stop training early on a model where the error on the validation set increases beyond a specified limit."
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#cross-validation",
    "href": "posts/2022-04-27-testing-validation.html#cross-validation",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "If the dataset size is relatively smaller to begin with, to avoid issues with trying to train a model on less data, cross-validation can be used. The training dataset can be split up into subsets or subsamples. Each model can be trained against combinations of these and validated against the remaining ones. Then the final model can be trained, using the best performing hyperparameters, on the complete training set, so all observations have been used.\n\n\n\nK-folding\n\nSplit the data int k-subsets e.g.¬†3,5, 10 etc.\nBuild k models, each trained on k-1 and tested on the datasets they haven‚Äôt been trained on\n\nLeave p out\n\nUse p observations as the validation set, and the remaining as the training set\nThis is repeated for as many variations of sampling p elements out of the total observations\nif p&gt;1, this will mean training and validating C{n \\choose p} times, where n is the number of observations in the original sample\nif p=1, as in leave-one-out, this will mean training and validating C{n \\choose 1} times. If n is large, k-fold may be less computationally expensive.\n\nStratified k-fold\n\nEnsure that each fold is a representative sample of the whole population of observations e.g.¬†mean is similar in all the subsets or proportions for binary classifiers\n\nRepeated random sub-sampling\n\nMonte Carlo cross validation - create multiple random training and validation subsets, then for each split, fit the model to the training subset and assess the accuracy on the validation subset. Then average the results over the subsets.\nValidation subsets may overlap, or some data points may never be used due to the random sampling.\nCan also be stratified by the mean of the subsets"
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#no-free-lunch-theorem1",
    "href": "posts/2022-04-27-testing-validation.html#no-free-lunch-theorem1",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "Any two optimisation algorithms are equivalent when their performance is averaged across all possible problems, if no assumptions are made about the data."
  },
  {
    "objectID": "posts/2022-04-30-intro-to-linear-algebra.html",
    "href": "posts/2022-04-30-intro-to-linear-algebra.html",
    "title": "Basic Linear Algebra Concepts",
    "section": "",
    "text": "Intro to Linear Algebra\nReferences: 1. https://github.com/jonkrohn/ML-foundations\n\nimport numpy as np\nimport math\nimport torch\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nModuleNotFoundError: No module named 'torch'\n\n\n\nScalar Tensors\nPytorch tensors can be used for operations on the GPU or CPU, unlike numpy arrays, by casting the tensor to a CUDA data type.\nTensorflow tensors are also immutable, and created using wrappers.\n\n# Example scalar pytorch tensor\nx = torch.tensor(10, dtype=torch.int64)\nprint(f\"Type: {x.dtype}, Shape: {x.shape}\")\n\nType: torch.int64, Shape: torch.Size([])\n\n\n\n# Example scalar tensorflow tensor\nx = tf.constant(10, dtype=tf.int64)\nx\n\nNameError: name 'tf' is not defined\n\n\n\n\nVector Transposition\n\nx = np.array([[5,6,7]]) #Need extra brackets else shape will be (,3) (1D, not 2D)\nprint(f\"x = {x} \\nShape = {x.shape}\")\n\nx = [[5 6 7]] \nShape = (1, 3)\n\n\n\nx_t = x.T\nprint(f\"x transposed:\\n{x_t} \\nShape = {x_t.shape}\")\n\nx transposed:\n[[5]\n [6]\n [7]] \nShape = (3, 1)\n\n\n\nx = torch.tensor([5,6,7])\nprint(f\"x = {x} \\nShape = {x.shape}\")\n\nx = tensor([5, 6, 7]) \nShape = torch.Size([3])\n\n\n\n#L2 Norm - euclidean distance\nx = np.array([[5,6,7]])\nnp.linalg.norm(x)\n\n10.488088481701515\n\n\n\n# L1 Norm\nnp.sum(np.absolute(x))\n\n18\n\n\n\n# Squared L2 Norm\nx = np.array([5,6,7])\nnp.dot(x,x)\n\n110\n\n\n\n# Max norm\nnp.max(np.abs(x))\n\n7\n\n\n\n#Orthogonal\ni = np.asarray([1,0])\nj = np.asarray([0,1])\nnp.dot(i,j)\n\n0\n\n\n\n\nMatrices (2-Tensor)\n\nX = np.array([[5,3],[6,8],[2,10]])\nprint(f\"{X}\\n Shape: {X.shape} Size: {X.size}\")\n\n[[ 5  3]\n [ 6  8]\n [ 2 10]]\n Shape: (3, 2) Size: 6\n\n\n\nX[:,0] # left column\n\narray([5, 6, 2])\n\n\n\nX[1,:] # middle row\n\narray([6, 8])\n\n\n\n#pytorch\nX = torch.tensor([[5,3],[6,8],[2,10]])\nprint(f\"{X}\\n Shape: {X.shape}\")\n\ntensor([[ 5,  3],\n        [ 6,  8],\n        [ 2, 10]])\n Shape: torch.Size([3, 2])\n\n\n\n#Tensorflow\nX = tf.constant([[5,3],[6,8],[2,10]])\nprint(f\"Rank: {tf.rank(X)} Shape: {tf.shape(X)}\")\n\nRank: 2 Shape: [3 2]\n\n\n\n\nn-Tensors\n\nX = torch.zeros([5,2,4,2])\nX.shape\n\ntorch.Size([5, 2, 4, 2])\n\n\n\nX = tf.zeros([5,2,4,2])\nprint(f\"Rank: {tf.rank(X)}\")\n\nRank: 4\n\n\n\n\nTensor Operations\n\nX = np.array([[5,3],[6,8],[2,10]])\nprint(f\"X*2 = \\n{X*2}\\n  X+2 =\\n{X+2}\")\n\nX*2 = \n[[10  6]\n [12 16]\n [ 4 20]]\n  X+2 =\n[[ 7  5]\n [ 8 10]\n [ 4 12]]\n\n\n\nX = torch.tensor([[5,3],[6,8],[2,10]])\nprint(f\"X*2 = \\n{torch.mul(X,2)}\\n  X+2 =\\n{torch.add(X,2)}\")\n\nX*2 = \ntensor([[10,  6],\n        [12, 16],\n        [ 4, 20]])\n  X+2 =\ntensor([[ 7,  5],\n        [ 8, 10],\n        [ 4, 12]])\n\n\n\n\nElementwise Product\nIf tensors have the same size, operations can be aplied elementwise, this is the Hadamard product: \\boldsymbol X \\odot \\boldsymbol Y It produces a tensor that is the same shape as the input, unlike the dot product which will produce a scalar value.\n\nX = np.array([[5,3],[6,8],[2,10]])\nY = X + 5\nprint(X*Y) ##dot product, not matrix multiplication\n\n[[ 50  24]\n [ 66 104]\n [ 14 150]]\n\n\n\n\nTensor Reduction\n\nprint(X.sum(), torch.sum(torch.tensor([[5,3],[6,8],[2,10]])))\nprint(tf.reduce_sum(tf.constant([[5,3],[6,8],[2,10]])))\nprint(X.sum(axis=0), X.sum(axis=1)) #along specific axes\nprint(torch.sum(torch.tensor([[5,3],[6,8],[2,10]]),0))\nprint(torch.sum(torch.tensor([[5,3],[6,8],[2,10]]),1))\nprint(tf.reduce_sum(tf.constant([[5,3],[6,8],[2,10]]),0))\nprint(tf.reduce_sum(tf.constant([[5,3],[6,8],[2,10]]),1))\n\n34 tensor(34)\ntf.Tensor(34, shape=(), dtype=int32)\n[13 21] [ 8 14 12]\ntensor([13, 21])\ntensor([ 8, 14, 12])\ntf.Tensor([13 21], shape=(2,), dtype=int32)\ntf.Tensor([ 8 14 12], shape=(3,), dtype=int32)\n\n\n\n\nDot Product\nThe vectors must be of the same length or shape for the element-wise multiplication to occurr.\n\nX = np.array([5,3,6,8,2,10])\nY = X + 1\nprint(np.dot(X,Y))\n\n272\n\n\n\nprint(torch.dot(torch.tensor([5,3,6,8,2,10]),torch.add(torch.tensor([5,3,6,8,2,10]),1)))\n\ntensor(272)\n\n\n\nprint(tf.reduce_sum(tf.multiply(tf.constant([5,3,6,8,2,10]),tf.add(tf.constant([5,3,6,8,2,10]),1))))\n\ntf.Tensor(272, shape=(), dtype=int32)\n\n\n\n\n\nMatrices\n\nFrobenius Norm\n\nX = np.array([[5,3],[6,8],[2,10]])\nprint(np.linalg.norm(X))\nprint(torch.norm(torch.tensor([[5.,3],[6,8],[2,10]])))\nprint(tf.norm(tf.constant([[5.,3],[6,8],[2,10]])))\n\n15.427248620541512\ntensor(15.4272)\ntf.Tensor(15.427248, shape=(), dtype=float32)\n\n\n\n\nMatrix Multiplication\n\nA = np.array([[3,4],[5,6],[7,8]])\nb = np.array([1,2])\nprint(np.dot(A,b)) #infers dot product vs matrix multiplication based on shape\nC = torch.tensor([[3,4],[5,6],[7,8]])\nd = torch.tensor([1,2])\nprint(torch.matmul(C,d))\nE = tf.constant([[3,4],[5,6],[7,8]])\nf = tf.constant([1,2])\nprint(tf.linalg.matvec(E,f))\n\ntensor([11, 17, 23])\ntf.Tensor([11 17 23], shape=(3,), dtype=int32)\n\n\n\nA = np.array([[3,4],[5,6],[7,8]])\nb = np.array([[1,9],[2,0]])\nprint(np.dot(A,b)) \nC = torch.tensor([[3,4],[5,6],[7,8]])\nd = torch.tensor([[1,9],[2,0]])\nprint(torch.matmul(C,d))\nE = tf.convert_to_tensor(A)\nf = tf.convert_to_tensor(b)\nprint(tf.matmul(E,f))\n\n[[11 27]\n [17 45]\n [23 63]]\ntensor([[11, 27],\n        [17, 45],\n        [23, 63]])\ntf.Tensor(\n[[11 27]\n [17 45]\n [23 63]], shape=(3, 2), dtype=int64)\n\n\n\n\nMatrix Inversion\n\nX = np.array([[4,2],[-5,-3]])\nX_inv = np.linalg.inv(X)\ny = np.array([4,-7])\nprint(w:= np.dot(X_inv,y))\nprint(np.dot(X,w))\n\n[-1.  4.]\n[ 4. -7.]"
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html",
    "href": "posts/2023-04-01-introduction-program-synthesis.html",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Generally, compilers transform source code from the source language to the target language, usually to create an executable program, whereas program synthesisers search for a program that will satisfy some stated requirements. They both take high-level specifications to generate software, but synthesisers can search over a solution space to discover how to generate a program to satisfy the requirements (some compiler optimisations also do this e.g.¬†autotuning). A program in this context, is a description of how to perform a computation. It is useful to constrain the solution to narrower notation than programming languages such as Domain Specific Languages (DSL) to avoid using specific constructs, rather build using a limited set of functions. Functional programming notation is useful for synthesis as we avoid side effects which can simplify the reasoning process, and can be expressed concisely.\n## Synthesis Program synthesis will generate a program that will solve the stated problem, within a specified program solution space i.e.¬†with control over the space of programs, not just the intended behaviour. An example is flash fill (Excel) which derives and applies a small program to the rest of the data. The order can also be reversed e.g.¬†start with a program and search for the specification i.e.¬†reverse engineering e.g.¬†Verified Lifting - discovering a high level representation that is provably equivalent to an implementation which can then be uo generate a more efficient version of the program. There are 3 major challenges; intention, invention and adaptation to consider.\n\n\n\nSemantic and syntactic constraints: form of user input will influence the synthesis system e.g.¬†input-output examples like flash fill may not be suitable. May need a multimodal approach with some examples or abstract examples combined with logical specifications, to provide enough data about intended behaviour.\nUnder specification: if there are multiple solutions that satisfy requirements, which one should be chosen?\n\n\n\n\n\nDiscovering the code to satisfy the requirements\n\n\n\n\n\nThe application of synthesis to broader software development e.g.¬†bug fixing, optimisation and other maintenance or existing codebase tasks.\n\n\n\n\n\nInductive synthesis generates a function that matches provided input-output examples. Deductive synthesis generally requires a complete list of axioms and formal specification for the language and program which may be difficult to obtain. Inductive synthesis allows for less complete specification with the downside of scaling issues due to iteration. ### Programming by example (PBE) and Programming by demonstration (PBD) In PBE, just the input-output is provided, which can leave a large potential solution space e.g.¬†f(1) = 1 which could be addition, division, multiplication etc.\nIn PBD, a trace of the computation performed is also provided to provide additional specification which makes it easier to infer the intended program, e.g.¬†f(1) = 1 \\times 1 = 1\nPBD and PBE can still be under-specificed with a large solution program space; need to address how to find the program that matches the observations, and how to know that the program that the user required is the one that was found. Arbritrary spaces can also be searched e.g.¬†a large but highly constrained space, allowing exclusion of undesirable solutions, but would still likely require ranking of solutions.\n\n\n\nNeed to represent code as a data structure - Abstrac Syntax Trees (AST) are commonly used for this as they usually follow the same structure of the parse tree of the program, whilst ignoring additional characters e.g.¬†parentheses, hence abstract. Context free grammar notation is often used to describe ASTs, to represent the structure of DSLs.\n\n\n\n\n\nExplicitly construct programs until one satisfies the observations, ideally avoiding the generation of programs that are not viable e.g.¬†cannot satisfy the observations or can be proven redundant from existing found solutions. - Bottom-Up: discover low level components then ho to assemble them into programs i.e.¬†leaf nodes up. - Top-Down: discover the high level structure first, then enumerate to the low level components i.e.¬†root down. The synthesiser will maintain a partial or completely constructed program that is being evaluated e.g.¬†would evaluate potential solutions until success. ### Symbolic Search In symbolic searching, the synthesiser will maintain a symbolic representation of the space of all programs that are considered valid e.g.¬†Version Space Algebras and Constraint Systems. A symbolic search may perform some algebraeic manipulation to deduce results which may perform better in some cases, but not all e.g.¬†binary search being effective in finding solutions where algebraeic manipulation may be too costly. ## Program Solution Space ### Small DSL One way to simplify the solution space is to define or use a small DSL, then consider all possible valid programs within this language, using context free grammar to describe the ASTs. This makes it simple to enumerate all (or randomly sample) programs in a small DSL. If the DSL has a type system, this can help eliminate solutions which violate type rules. ### Constraint Based Constraint based approaches utilise parametric representations of the space - different choices of paramters correspond to different forms of the program. Paramtetric representations can be more general than grammars - grammars can be encoded in paramteric representation provided a boundary on the length of the program. Paramteric programs can also be referred to as generative models, particularly when the free parameter choice is associated with probabilities. ### Symmetry If there are many ways to represent the same program, the program space has lots of symmetry. Symmetry can be reduced with reduction of cummtativity e.g.¬†forcing right or left associativity. Constraint and enumeration based strategies can benefit from symmetry reduction, though there are additional techniques that are mostly removed from this issue. For example $expr := var N | expr + expr $ has a greater soltuion space than $expr := var N | (var * N) + expr $ which forces right associativity."
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html#compilation-vs-synthesis",
    "href": "posts/2023-04-01-introduction-program-synthesis.html#compilation-vs-synthesis",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Generally, compilers transform source code from the source language to the target language, usually to create an executable program, whereas program synthesisers search for a program that will satisfy some stated requirements. They both take high-level specifications to generate software, but synthesisers can search over a solution space to discover how to generate a program to satisfy the requirements (some compiler optimisations also do this e.g.¬†autotuning). A program in this context, is a description of how to perform a computation. It is useful to constrain the solution to narrower notation than programming languages such as Domain Specific Languages (DSL) to avoid using specific constructs, rather build using a limited set of functions. Functional programming notation is useful for synthesis as we avoid side effects which can simplify the reasoning process, and can be expressed concisely.\n## Synthesis Program synthesis will generate a program that will solve the stated problem, within a specified program solution space i.e.¬†with control over the space of programs, not just the intended behaviour. An example is flash fill (Excel) which derives and applies a small program to the rest of the data. The order can also be reversed e.g.¬†start with a program and search for the specification i.e.¬†reverse engineering e.g.¬†Verified Lifting - discovering a high level representation that is provably equivalent to an implementation which can then be uo generate a more efficient version of the program. There are 3 major challenges; intention, invention and adaptation to consider.\n\n\n\nSemantic and syntactic constraints: form of user input will influence the synthesis system e.g.¬†input-output examples like flash fill may not be suitable. May need a multimodal approach with some examples or abstract examples combined with logical specifications, to provide enough data about intended behaviour.\nUnder specification: if there are multiple solutions that satisfy requirements, which one should be chosen?\n\n\n\n\n\nDiscovering the code to satisfy the requirements\n\n\n\n\n\nThe application of synthesis to broader software development e.g.¬†bug fixing, optimisation and other maintenance or existing codebase tasks."
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html#inductive-synthesis",
    "href": "posts/2023-04-01-introduction-program-synthesis.html#inductive-synthesis",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Inductive synthesis generates a function that matches provided input-output examples. Deductive synthesis generally requires a complete list of axioms and formal specification for the language and program which may be difficult to obtain. Inductive synthesis allows for less complete specification with the downside of scaling issues due to iteration. ### Programming by example (PBE) and Programming by demonstration (PBD) In PBE, just the input-output is provided, which can leave a large potential solution space e.g.¬†f(1) = 1 which could be addition, division, multiplication etc.\nIn PBD, a trace of the computation performed is also provided to provide additional specification which makes it easier to infer the intended program, e.g.¬†f(1) = 1 \\times 1 = 1\nPBD and PBE can still be under-specificed with a large solution program space; need to address how to find the program that matches the observations, and how to know that the program that the user required is the one that was found. Arbritrary spaces can also be searched e.g.¬†a large but highly constrained space, allowing exclusion of undesirable solutions, but would still likely require ranking of solutions."
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html#programming",
    "href": "posts/2023-04-01-introduction-program-synthesis.html#programming",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Need to represent code as a data structure - Abstrac Syntax Trees (AST) are commonly used for this as they usually follow the same structure of the parse tree of the program, whilst ignoring additional characters e.g.¬†parentheses, hence abstract. Context free grammar notation is often used to describe ASTs, to represent the structure of DSLs."
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html#search-techniques",
    "href": "posts/2023-04-01-introduction-program-synthesis.html#search-techniques",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Explicitly construct programs until one satisfies the observations, ideally avoiding the generation of programs that are not viable e.g.¬†cannot satisfy the observations or can be proven redundant from existing found solutions. - Bottom-Up: discover low level components then ho to assemble them into programs i.e.¬†leaf nodes up. - Top-Down: discover the high level structure first, then enumerate to the low level components i.e.¬†root down. The synthesiser will maintain a partial or completely constructed program that is being evaluated e.g.¬†would evaluate potential solutions until success. ### Symbolic Search In symbolic searching, the synthesiser will maintain a symbolic representation of the space of all programs that are considered valid e.g.¬†Version Space Algebras and Constraint Systems. A symbolic search may perform some algebraeic manipulation to deduce results which may perform better in some cases, but not all e.g.¬†binary search being effective in finding solutions where algebraeic manipulation may be too costly. ## Program Solution Space ### Small DSL One way to simplify the solution space is to define or use a small DSL, then consider all possible valid programs within this language, using context free grammar to describe the ASTs. This makes it simple to enumerate all (or randomly sample) programs in a small DSL. If the DSL has a type system, this can help eliminate solutions which violate type rules. ### Constraint Based Constraint based approaches utilise parametric representations of the space - different choices of paramters correspond to different forms of the program. Paramtetric representations can be more general than grammars - grammars can be encoded in paramteric representation provided a boundary on the length of the program. Paramteric programs can also be referred to as generative models, particularly when the free parameter choice is associated with probabilities. ### Symmetry If there are many ways to represent the same program, the program space has lots of symmetry. Symmetry can be reduced with reduction of cummtativity e.g.¬†forcing right or left associativity. Constraint and enumeration based strategies can benefit from symmetry reduction, though there are additional techniques that are mostly removed from this issue. For example $expr := var N | expr + expr $ has a greater soltuion space than $expr := var N | (var * N) + expr $ which forces right associativity."
  },
  {
    "objectID": "posts/2023-04-06-CEGIS.html",
    "href": "posts/2023-04-06-CEGIS.html",
    "title": "Introduction to CEGIS",
    "section": "",
    "text": "Counter Example Guided Inductive Synthesis (CEGIS)\nIn CEGIS, with a supplied specification, a synthesiser produces a program that may satisfy the specification, and a verifier determines if this is true, otherwise it will provide a defined form of feedback to the synthesiser to further guide its search. The feedback is usually a counterexample - an input upon which the program did not satisfy the specification.\n\n\nReferences\n[^1] A. Solar-Lezama, MIT 6.S981 Introduction to Program Synthesis [^2] J. Bornholt 2015 Program Synthesis Explained"
  },
  {
    "objectID": "posts/2024-08-26-hash-functions-introduction.html",
    "href": "posts/2024-08-26-hash-functions-introduction.html",
    "title": "Division Based Hash Functions",
    "section": "",
    "text": "A simple modulus hash function can be defined as:\nh(x) = x \\mod n\nWhere x is the input value (key) and n is the number of buckets in the hash table. While n can be any positive integer, choosing a prime number for n can significantly reduce collisions, especially for certain types of input data.\n\n\nPrime numbers are particularly useful when the keys are of the form a + k \\times b, where a and b are constants and k varies. This pattern is common in many applications, such as memory addresses or sequential IDs.\nUsing a prime number for n helps to:\n\nReduce collisions\nEnsure a more uniform distribution of hash values\nMinimize the impact of patterns in the input data\n\n\n\n\nIf we have keys of the form 100 + k \\times 50, and we need to hash these into a table.\nCase 1: Non-prime number of buckets Using 100 buckets (non-prime).\nh(x) = x \\mod 100\nFor our keys:\n- h(100) = 0\n- h(150) = 50\n- h(200) = 0\n- h(250) = 50\n- h(300) = 0\nWe see that all keys hash to either 0 or 50, utilizing only 2 out of 100 buckets.\nCase 2: Prime number of buckets Now using 101 buckets (prime).\nh(x) = x \\mod 101\nFor the same keys:\n- h(100) = 100\n- h(150) = 48\n- h(200) = 99\n- h(250) = 47\n- h(300) = 98\nWe get a much better distribution, using 5 different buckets for our 5 keys.\n\n\n\nWhen n is not prime, it‚Äôs more likely to share factors with the step size in the key pattern (in our example, 50). This causes the hash values to repeat in a cycle much shorter than n.\nA prime n is less likely to share factors with the key pattern, ensuring that the hash values cycle through a larger portion of the available buckets before repeating.\n\n\n\nWhile using a prime number doesn‚Äôt guarantee perfect distribution or zero collisions, it significantly improves the hash function‚Äôs performance for many common types of input data (especially those with patterns or structures)."
  },
  {
    "objectID": "posts/2024-08-26-hash-functions-introduction.html#why-prime-numbers-are-used-in-division-based-hash-functions",
    "href": "posts/2024-08-26-hash-functions-introduction.html#why-prime-numbers-are-used-in-division-based-hash-functions",
    "title": "Division Based Hash Functions",
    "section": "",
    "text": "A simple modulus hash function can be defined as:\nh(x) = x \\mod n\nWhere x is the input value (key) and n is the number of buckets in the hash table. While n can be any positive integer, choosing a prime number for n can significantly reduce collisions, especially for certain types of input data.\n\n\nPrime numbers are particularly useful when the keys are of the form a + k \\times b, where a and b are constants and k varies. This pattern is common in many applications, such as memory addresses or sequential IDs.\nUsing a prime number for n helps to:\n\nReduce collisions\nEnsure a more uniform distribution of hash values\nMinimize the impact of patterns in the input data\n\n\n\n\nIf we have keys of the form 100 + k \\times 50, and we need to hash these into a table.\nCase 1: Non-prime number of buckets Using 100 buckets (non-prime).\nh(x) = x \\mod 100\nFor our keys:\n- h(100) = 0\n- h(150) = 50\n- h(200) = 0\n- h(250) = 50\n- h(300) = 0\nWe see that all keys hash to either 0 or 50, utilizing only 2 out of 100 buckets.\nCase 2: Prime number of buckets Now using 101 buckets (prime).\nh(x) = x \\mod 101\nFor the same keys:\n- h(100) = 100\n- h(150) = 48\n- h(200) = 99\n- h(250) = 47\n- h(300) = 98\nWe get a much better distribution, using 5 different buckets for our 5 keys.\n\n\n\nWhen n is not prime, it‚Äôs more likely to share factors with the step size in the key pattern (in our example, 50). This causes the hash values to repeat in a cycle much shorter than n.\nA prime n is less likely to share factors with the key pattern, ensuring that the hash values cycle through a larger portion of the available buckets before repeating.\n\n\n\nWhile using a prime number doesn‚Äôt guarantee perfect distribution or zero collisions, it significantly improves the hash function‚Äôs performance for many common types of input data (especially those with patterns or structures)."
  },
  {
    "objectID": "posts/2024-08-29-binary-search.html",
    "href": "posts/2024-08-29-binary-search.html",
    "title": "Introduction to Binary Search",
    "section": "",
    "text": "Binary search is an efficienct way to find items in a sorted or ordered collection, by repeatedly dividing the range to search in half, as we know, due to the ordering, if the value is to the left or right of the current item.\n\n\n\nPick an item in the middle of the array\n\nIf the item is equal to the item being searched for, the search ends\n\nIf the target is less than the middle elements, repeat the steps in the lower half/left hand side\n\nIf the target is greater, repeat the steps in the upper half/right hand side\n\n// Sorted array\n[1, 3, 4, 6, 8, 9, 11] \n ^           ^      ^\nlow         mid    high\n\nTarget: 6\nStep 1: mid = 8, target &lt; mid, search lower half\n[1, 3, 4, 6]\n ^     ^  ^\nlow   mid high\n\nStep 2: mid = 3, target &gt; mid, search upper half\n      [4, 6]\n       ^  ^\n      low high\n      mid\n\nStep 3: mid = 6, target == mid, found!\n\n\n\n\ndef binary_search_iterative(arr, target):\n    left, right = 0, len(arr) - 1\n    while left &lt;= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef binary_search_recursive(arr, target):\n    # TODO\n    pass\n\nsorted_array = [1, 3, 4, 6, 8, 9, 11]\ntarget = 6\nresult = binary_search_iterative(sorted_array, target)\nprint(f\"Target {target} found at index: {result}\")\n\nTarget 6 found at index: 3\n\n\n\n\n\n\nO(\\log n)\n\n\\Omega(1) (when the middle element is the target)\n\n\n\n\n\nIterative implementation: O(1)\n\nRecursive implementation: O(\\log n) due to the call stack\n\n\n\n\n\nVery efficient for searching in large sorted datasets\n\nLogarithmic time complexity makes it much faster than linear search for large arrays\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport random\n\ndef linear_search(arr, target):\n    for i in range(len(arr)):\n        if arr[i] == target:\n            return i\n    return -1\n\ndef binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left &lt;= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef measure_search_time(search_func, arr, target):\n    start_time = time.time()\n    search_func(arr, target)\n    end_time = time.time()\n    return end_time - start_time\n\ndef create_plot(sizes, linear_times, binary_times):\n    plt.figure(figsize=(12, 6))\n    plt.plot(sizes, linear_times, label='Linear Search', color='red', marker='o')\n    plt.plot(sizes, binary_times, label='Binary Search', color='green', marker='o')\n    plt.title('Binary Search vs Linear Search Performance')\n    plt.xlabel('Input Size (n)')\n    plt.ylabel('Execution Time (seconds)')\n    plt.legend()\n    plt.xscale('log')\n    plt.yscale('log')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    return plt\n\nsizes = np.logspace(1, 6, num=20, dtype=int)\nlinear_times = []\nbinary_times = []\n\nfor size in sizes:\n    arr = sorted(random.sample(range(size * 10), size))\n    target = random.choice(arr)\n    linear_time = measure_search_time(linear_search, arr, target)\n    binary_time = measure_search_time(binary_search, arr, target)\n    linear_times.append(linear_time)\n    binary_times.append(binary_time)\n\nplot = create_plot(sizes, linear_times, binary_times)\nplot.savefig('search_performance.png')\nplt.close()\n\nprint(f\"Largest input size: {sizes[-1]}\")\nprint(f\"Linear search time for largest input: {linear_times[-1]:.6f} seconds\")\nprint(f\"Binary search time for largest input: {binary_times[-1]:.6f} seconds\")\nprint(f\"Difference for the largest input: {linear_times[-1] / binary_times[-1]:.2f}x\")\n\nLargest input size: 1000000\nLinear search time for largest input: 0.029848 seconds\nBinary search time for largest input: 0.000006 seconds\nDifference for the largest input: 5007.60x\n\n\n As you can see, the binary search (green line) remains much faster than the linear search (red line) as the input size grows, demonstrating its logarithmic time complexity compared to the linear time complexity of the linear search.\n\n\n\n\nRequires a sorted array\n\nNot efficient for small datasets compared to linear search due to overhead\n\n\n\n\n\nUsed in algorithms for database searches and spell checkers\n\nFinding the closest element to a target value in a sorted array\n\n\n\n\n\nUniform Binary Search\n\nExponential Search\n\nInterpolation Search"
  },
  {
    "objectID": "posts/2024-08-29-binary-search.html#binary-search",
    "href": "posts/2024-08-29-binary-search.html#binary-search",
    "title": "Introduction to Binary Search",
    "section": "",
    "text": "Binary search is an efficienct way to find items in a sorted or ordered collection, by repeatedly dividing the range to search in half, as we know, due to the ordering, if the value is to the left or right of the current item.\n\n\n\nPick an item in the middle of the array\n\nIf the item is equal to the item being searched for, the search ends\n\nIf the target is less than the middle elements, repeat the steps in the lower half/left hand side\n\nIf the target is greater, repeat the steps in the upper half/right hand side\n\n// Sorted array\n[1, 3, 4, 6, 8, 9, 11] \n ^           ^      ^\nlow         mid    high\n\nTarget: 6\nStep 1: mid = 8, target &lt; mid, search lower half\n[1, 3, 4, 6]\n ^     ^  ^\nlow   mid high\n\nStep 2: mid = 3, target &gt; mid, search upper half\n      [4, 6]\n       ^  ^\n      low high\n      mid\n\nStep 3: mid = 6, target == mid, found!\n\n\n\n\ndef binary_search_iterative(arr, target):\n    left, right = 0, len(arr) - 1\n    while left &lt;= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef binary_search_recursive(arr, target):\n    # TODO\n    pass\n\nsorted_array = [1, 3, 4, 6, 8, 9, 11]\ntarget = 6\nresult = binary_search_iterative(sorted_array, target)\nprint(f\"Target {target} found at index: {result}\")\n\nTarget 6 found at index: 3\n\n\n\n\n\n\nO(\\log n)\n\n\\Omega(1) (when the middle element is the target)\n\n\n\n\n\nIterative implementation: O(1)\n\nRecursive implementation: O(\\log n) due to the call stack\n\n\n\n\n\nVery efficient for searching in large sorted datasets\n\nLogarithmic time complexity makes it much faster than linear search for large arrays\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport random\n\ndef linear_search(arr, target):\n    for i in range(len(arr)):\n        if arr[i] == target:\n            return i\n    return -1\n\ndef binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left &lt;= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] &lt; target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1\n\ndef measure_search_time(search_func, arr, target):\n    start_time = time.time()\n    search_func(arr, target)\n    end_time = time.time()\n    return end_time - start_time\n\ndef create_plot(sizes, linear_times, binary_times):\n    plt.figure(figsize=(12, 6))\n    plt.plot(sizes, linear_times, label='Linear Search', color='red', marker='o')\n    plt.plot(sizes, binary_times, label='Binary Search', color='green', marker='o')\n    plt.title('Binary Search vs Linear Search Performance')\n    plt.xlabel('Input Size (n)')\n    plt.ylabel('Execution Time (seconds)')\n    plt.legend()\n    plt.xscale('log')\n    plt.yscale('log')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    return plt\n\nsizes = np.logspace(1, 6, num=20, dtype=int)\nlinear_times = []\nbinary_times = []\n\nfor size in sizes:\n    arr = sorted(random.sample(range(size * 10), size))\n    target = random.choice(arr)\n    linear_time = measure_search_time(linear_search, arr, target)\n    binary_time = measure_search_time(binary_search, arr, target)\n    linear_times.append(linear_time)\n    binary_times.append(binary_time)\n\nplot = create_plot(sizes, linear_times, binary_times)\nplot.savefig('search_performance.png')\nplt.close()\n\nprint(f\"Largest input size: {sizes[-1]}\")\nprint(f\"Linear search time for largest input: {linear_times[-1]:.6f} seconds\")\nprint(f\"Binary search time for largest input: {binary_times[-1]:.6f} seconds\")\nprint(f\"Difference for the largest input: {linear_times[-1] / binary_times[-1]:.2f}x\")\n\nLargest input size: 1000000\nLinear search time for largest input: 0.029848 seconds\nBinary search time for largest input: 0.000006 seconds\nDifference for the largest input: 5007.60x\n\n\n As you can see, the binary search (green line) remains much faster than the linear search (red line) as the input size grows, demonstrating its logarithmic time complexity compared to the linear time complexity of the linear search.\n\n\n\n\nRequires a sorted array\n\nNot efficient for small datasets compared to linear search due to overhead\n\n\n\n\n\nUsed in algorithms for database searches and spell checkers\n\nFinding the closest element to a target value in a sorted array\n\n\n\n\n\nUniform Binary Search\n\nExponential Search\n\nInterpolation Search"
  },
  {
    "objectID": "posts/2024-08-29-binary-search.html#references",
    "href": "posts/2024-08-29-binary-search.html#references",
    "title": "Introduction to Binary Search",
    "section": "References",
    "text": "References\n[^1] Khan Academy - Binary Search\n[^2] LeetCode - Binary Search Problems"
  },
  {
    "objectID": "posts/2024-09-22-quantum-introduction.html",
    "href": "posts/2024-09-22-quantum-introduction.html",
    "title": "Introduction to Quantum Programming",
    "section": "",
    "text": "In classical computing, we typically deal with systems that can exist in two states, i.e., bits are either 1 or 0. In quantum computing, we deal with systems that can exist in multiple states simultaneously (using the Copenhagen interpretation).\n\n\n\n\nWhile a classical bit is always in a definite state of either 0 or 1, a qubit can exist in a superposition of states:\n\nSuperposition: A qubit can be in a superposition of the basis states |0\\rangle and |1\\rangle. This means it‚Äôs not just 0 or 1, but a complex linear combination of both.\nProbability Distribution vs.¬†Specific State: A qubit has a probability distribution over possible measurement outcomes, rather than a probability of being in a specific state. Before measurement, the qubit exists in superposition.\nState Vector: The state of a qubit is represented by a two-dimensional complex vector called a state vector. This vector contains the amplitudes for each basis state.\nMeasurement: When we measure a qubit, we force it to collapse into one of the basis states (typically |0\\rangle or |1\\rangle). The probability of each outcome is determined by the squared magnitude of the corresponding amplitude in the state vector.\n\n\n\n\nA qubit‚Äôs state is typically represented as:\n\n|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle\n\nWhere:\n- |\\psi\\rangle represents the qubit‚Äôs state\n- \\alpha and \\beta are complex numbers called probability amplitudes\n- |\\alpha|^2 gives the probability of measuring the qubit in state |0\\rangle\n- |\\beta|^2 gives the probability of measuring the qubit in state |1\\rangle\n- |\\alpha|^2 + |\\beta|^2 = 1 (normalization condition)\nA state vector is a mathematical representation of a qubit‚Äôs state in quantum mechanics. For a single qubit, it‚Äôs a two-dimensional complex vector that captures all the information about the qubit‚Äôs state.\n\n\nAmplitudes (\\alpha and \\beta) are complex numbers that determine the quantum state‚Äôs properties.\n\nProbability: The squared magnitude of an amplitude (|\\alpha|^2 or |\\beta|^2) gives the probability of measuring the qubit in the corresponding basis state.\nPhase: The complex phase of the amplitude carries information about the qubit‚Äôs behavior in interference situations.\nSuperposition: Non-zero amplitudes for both |0\\rangle and |1\\rangle indicate the qubit is in a superposition of these basis states.\n\n\n\n\n\n|\\psi\\rangle = 1|0\\rangle + 0|1\\rangle\n\nThis represents a qubit in the definite state |0\\rangle.\nMeasuring this qubit will always yield |0\\rangle.\n\n|\\psi\\rangle = 0|0\\rangle + 1|1\\rangle\n\nThis represents a qubit in the definite state |1\\rangle.\nMeasuring this qubit will always yield |1\\rangle.\n\n\n\n\n\n|\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle\n\nThis is often written as |+\\rangle and represents an equal superposition.\nProbability of measuring |0\\rangle: |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}\nProbability of measuring |1\\rangle: |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}\nThe qubit has an equal chance of being measured in either state.\n\n\n\n\n|\\psi\\rangle = \\frac{\\sqrt{3}}{2}|0\\rangle + \\frac{1}{2}|1\\rangle\n\nProbability of measuring |0\\rangle: |\\frac{\\sqrt{3}}{2}|^2 = \\frac{3}{4}\nProbability of measuring |1\\rangle: |\\frac{1}{2}|^2 = \\frac{1}{4}\nThe qubit is more likely to be measured in state |0\\rangle.\n\n\n\n\n|\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{i}{\\sqrt{2}}|1\\rangle\n\nProbability of measuring |0\\rangle: |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}\nProbability of measuring |1\\rangle: |\\frac{i}{\\sqrt{2}}|^2 = \\frac{1}{2}\nEqual probabilities, but the i in the second amplitude introduces a phase difference.\nThis phase difference is important in quantum interference but doesn‚Äôt affect single-qubit measurements.\n\n\n\n\n\nFor systems with multiple qubits, the state vector grows exponentially:\n\n1 qubit: 2-dimensional vector\n2 qubits: 4-dimensional vector\n3 qubits: 8-dimensional vector\nn qubits: 2^n dimensional vector\n\nThis exponential growth in the state space is what gives quantum computers their potential power relative to classical computers.\n\n\n\nQubits are often visualized using the Bloch sphere, a unit sphere in three-dimensional space. Any single qubit state can be represented as a point on the surface of this sphere.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom qutip import Bloch, basis, sigmax, sigmay, sigmaz\n\ndef plot_bloch_sphere(states, labels):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    b = Bloch(axes=ax)\n    b.point_color = ['r', 'g', 'b', 'y']\n    \n    for state, label in zip(states, labels):\n        b.add_states(state)\n        b.add_annotation(state, label)\n    \n    b.render()\n    return fig\n\n# some example states\npsi1 = basis(2, 0)  # |0‚ü© i.e. classical 0\npsi2 = basis(2, 1)  # |1‚ü© i.e. classical 1\npsi3 = (basis(2, 0) + basis(2, 1)).unit()  # (|0‚ü© + |1‚ü©)/‚àö2 i.e. equal superposition\npsi4 = (basis(2, 0) + 1j*basis(2, 1)).unit()  # (|0‚ü© + i|1‚ü©)/‚àö2 i.e. superposition with a phase difference\n\nstates = [psi1, psi2, psi3, psi4]\nlabels = [\"|0‚ü©\", \"|1‚ü©\", \"|+‚ü©\", \"|+i‚ü©\"]\n\nfig = plot_bloch_sphere(states, labels)\n\nplt.tight_layout()\nplt.savefig('bloch_sphere_example.png', dpi=150, bbox_inches='tight')\nplt.close(fig)\n\n\n\n\nExample Bloch Sphere representing quantum states\n\n\n\nThe |0\\rangle state is represented by a point at the north pole of the sphere.\n\nThe |1\\rangle state is represented by a point at the south pole of the sphere.\n\nThe |+\\rangle state (equal superposition) is represented by a point on the equator along the x-axis.\n\nThe |+i\\rangle state is represented by a point on the equator along the y-axis.\n\n\n\n\nWe can also use this to visualize how quantum states evolve under certain operations. Here‚Äôs an example that shows the state evolution under the action of a Hadamard-like gate:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom qutip import Bloch, basis, qeye, sigmax, sigmay, sigmaz\nfrom matplotlib.animation import FuncAnimation\nimport imageio\n\ndef plot_bloch_sphere(states, filename='bloch_sphere_evolution.gif', duration=0.1):\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    b = Bloch(axes=ax)\n\n    def update(frame):\n        ax.cla()\n        b = Bloch(axes=ax)\n        b.add_states(states[frame])\n        b.add_states([states[0], states[-1]], 'point')\n        b.render()\n        ax.set_title(f'Frame {frame + 1}/{len(states)}')\n\n    anim = FuncAnimation(fig, update, frames=len(states), repeat=True)\n\n    frames = []\n    for i in range(len(states)):\n        update(i)\n        fig.canvas.draw()\n        image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n        image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n        frames.append(image)\n\n    imageio.mimsave(filename, frames, duration=duration)\n    plt.close(fig)\n\ndef state_evolution(initial_state, operation, steps=50):\n    states = [initial_state]\n    for i in range(1, steps+1):\n        states.append(operation(initial_state, i/steps))\n    return states\n\ndef hadamard_like_transform(state, t):\n    H = np.sqrt(1 - t) * sigmaz() + np.sqrt(t) * sigmax()\n    return (1 - t) * state + t * H * state\n\npsi0 = basis(2, 0) # initial state |0‚ü©\n\nstates = state_evolution(psi0, hadamard_like_transform, steps=50)\n\nplot_bloch_sphere(states, 'bloch_sphere_evolution.gif', duration=0.1)\n\n/tmp/ipykernel_3956532/1348161396.py:26: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n  image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n\n\n\n\n\nExample Bloch Sphere representing quantum state evolution\n\n\nThis will show how the |0\\rangle state evolves into the |+\\rangle state under the action of the Hadamard-like gate.\n\n\n\n\nThe basis states are typically labeled |0\\rangle and |1\\rangle, which might suggest values between 0 and 1.\n\nHowever, the actual state of a qubit is described by complex amplitudes \\alpha and \\beta. These amplitudes can take any complex values as long as |\\alpha|^2 + |\\beta|^2 = 1.\n\nThe probabilities of measuring |0\\rangle or |1\\rangle (given by |\\alpha|^2 and |\\beta|^2 respectively) are between 0 and 1 (as they must be for any probability).\n\n\n\n\nWith a qubit in the state:\n\n|\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle\n\nThis state has an equal probability of being measured as |0\\rangle or |1\\rangle. The probabilities are:\n\nP(|0\\rangle) = |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}\nP(|1\\rangle) = |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}\n\n\n\n\n\nProbability vectors are fundamental in describing quantum states. They represent the probability distribution of possible outcomes when measuring a quantum system.\n\n\n\nAll entries are non-negative real numbers.\nThe sum of all entries equals 1.\n\n\n\n\nFor a coin flip:\np = [0.5, 0.5]  # Represents a fair coin (50% chance of heads, 50% chance of tails)\n\n\n\n\nIn quantum mechanics, states can be combined linearly, but probability vectors specifically use convex combinations.\n\n\nA linear combination where: 1. All coefficients are non-negative. 2. The sum of coefficients equals 1.\nThis ensures that the result is still a valid probability vector.\n\n\n\np1 = [0.3, 0.7]\np2 = [0.6, 0.4]\nŒ± = 0.4\n\nconvex_combination = Œ± * np.array(p1) + (1 - Œ±) * np.array(p2)\n# Result: [0.48, 0.52]\n\n\n\n\nClassical transformations in quantum computing are represented by matrices that transform one probability vector into another.\n\n\n\nAll entries are non-negative.\nThe sum of each column is 1 (stochastic property).\n\n\n\n\nThe NOT operation flips a bit. In matrix form for a single qubit:\n\n\\text{NOT} = \\begin{bmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\n\n\n\n\n\nThe tensor product is used to combine quantum systems, creating a joint state space.\n\n\n\nCombines probability vectors of individual systems.\nResults in a higher-dimensional probability vector.\n\n\n\n\nFor two coins:\ncoin1 = [0.5, 0.5]  # Fair coin\ncoin2 = [0.7, 0.3]  # Biased coin\n\njoint_state = np.kron(coin1, coin2)\n# Result: [0.35, 0.15, 0.35, 0.15]\nNot all higher-dimensional probability vectors can be decomposed into tensor products of lower-dimensional vectors.\n\n\n\nIn vector notation, the same state can be written as:\n|\\psi\\rangle = \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}\n\n\n\n\nKet |\\psi\\rangle: Represents a column vector.\n\nBra \\langle\\psi|: Represents the conjugate transpose of |\\psi\\rangle, i.e., a row vector.\n\nConjugate Transpose (or Hermitian Conjugate): formed from an m-by-n matrix by taking the transpose of the matrix (forming an n-by-m matrix), then taking the complex conjugate of each entry (negating their imaginary components).\n\n\n\n\n\nSince \\alpha and \\beta are complex numbers, they can be written in polar form:\n\\begin{align*}\n\\alpha &= r_1e^{i\\theta_1} \\\\\n\\beta &= r_2e^{i\\theta_2}\n\\end{align*}\nWhere: - r_1 and r_2 are the magnitudes (r_1^2 + r_2^2 = 1) - \\theta_1 and \\theta_2 are the phases - i is the imaginary unit (i^2 = -1)\n\n\n\nFor mixed states or when dealing with subsystems, we often use the density matrix representation:\n\\rho = |\\psi\\rangle\\langle\\psi| = \\begin{bmatrix}\n|\\alpha|^2 & \\alpha\\beta^* \\\\\n\\alpha^*\\beta & |\\beta|^2\n\\end{bmatrix}\nWhere * denotes complex conjugation.\n\n\n\n\n|0\\rangle state: |\\psi\\rangle = 1|0\\rangle + 0|1\\rangle = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n|1\\rangle state: |\\psi\\rangle = 0|0\\rangle + 1|1\\rangle = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n|+\\rangle state (equal superposition): |\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n|-\\rangle state: |\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle - \\frac{1}{\\sqrt{2}}|1\\rangle = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n|+i\\rangle state: |\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{i}{\\sqrt{2}}|1\\rangle = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ i \\end{bmatrix}\n\n\n\n\nThe Bloch sphere provides a geometrical representation of a qubit‚Äôs state. Any pure state of a qubit can be represented as:\n|\\psi\\rangle = \\cos\\left(\\frac{\\theta}{2}\\right)|0\\rangle + e^{i\\phi}\\sin\\left(\\frac{\\theta}{2}\\right)|1\\rangle\nWhere: - \\theta (theta) is the polar angle (0 \\leq \\theta \\leq \\pi) - \\phi (phi) is the azimuthal angle (0 \\leq \\phi &lt; 2\\pi)\nThis representation directly maps to the surface of the Bloch sphere.\n\n\n\nFor a system of n qubits, the state is represented by a 2^n dimensional complex vector. For example, a two-qubit system has the general form:\n|\\psi\\rangle = \\alpha|00\\rangle + \\beta|01\\rangle + \\gamma|10\\rangle + \\delta|11\\rangle\nWhere \\alpha, \\beta, \\gamma, and \\delta are complex amplitudes satisfying |\\alpha|^2 + |\\beta|^2 + |\\gamma|^2 + |\\delta|^2 = 1.\n\n\n\n\nQuantum circuits are sequences of operations (gates) applied to qubits. They are the quantum analogue of classical logic circuits.\n\n\n      ‚îå‚îÄ‚îÄ‚îÄ‚îê\nq_0: ‚îÄ‚î§ H ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n      ‚îî‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\nq_1: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                ‚îÇ\nq_2: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nThis circuit applies a Hadamard gate (H) to the first qubit and then a controlled-NOT (CNOT) with the first qubit as control and the third as target.\n\n\n\nThe CNOT (Controlled-NOT) gate is a two-qubit gate that flips the second qubit (target) if and only if the first qubit (control) is in the state |1\\rangle.\n\n\n\n\nThe four possible basis states for a two-qubit system:\n\n|00\\rangle\n|01\\rangle\n|10\\rangle\n|11\\rangle\n\n\n\n\nThe X gate (NOT gate) for a single qubit is represented by the matrix:\nX = \\begin{bmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\nand the 2x2 identity matrix is also used:\nI = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n\n\nThe CNOT gate performs the following operations:\n\nIf the control qubit is |0\\rangle, do nothing to the target qubit.\nIf the control qubit is |1\\rangle, apply the X gate to the target qubit.\n\nThis can be expressed as:\nCNOT = |0\\rangle\\langle0| \\otimes I + |1\\rangle\\langle1| \\otimes X\nWhere \\otimes denotes the tensor product.\n\n|0\\rangle\\langle0| \\otimes I:\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix} \\otimes\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n|1\\rangle\\langle1| \\otimes X:\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix} \\otimes\n\\begin{bmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{bmatrix}\n\n\n\n\nAdding these two matrices gives us the final CNOT matrix:\nCNOT = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{bmatrix}\n\n\n\n\nWe can verify this matrix by applying it to each basis state:\n\nCNOT|00\\rangle = |00\\rangle (No change)\n\nCNOT|01\\rangle = |01\\rangle (No change)\n\nCNOT|10\\rangle = |11\\rangle (Flips second qubit)\n\nCNOT|11\\rangle = |10\\rangle (Flips second qubit)\n\nThe CNOT operation only flips the second qubit when the first qubit is in state |1\\rangle.\n\n\n\n\nThe Hadamard gate is a single-qubit gate that creates superpositions. It‚Äôs often used to initialize qubits in quantum algorithms.\n\n\nThe Hadamard gate is defined by the following matrix:\nH = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}\n1 & 1 \\\\\n1 & -1\n\\end{bmatrix}\n\n\n\nWe can derive the Hadamard gate by considering its action on the computational basis states:\n\nFor |0\\rangle:\nH|0\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)\nFor |1\\rangle:\nH|1\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle - |1\\rangle)\n\nThese actions can be represented in matrix form:\nH\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}\nH\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}\n1 \\\\\n-1\n\\end{bmatrix}\nFrom these, we can construct the Hadamard matrix:\nH = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}\n1 & 1 \\\\\n1 & -1\n\\end{bmatrix}\n\n\n\n\nThe Hadamard gate is its own inverse: H^2 = I\n\nIt creates equal superpositions of computational basis states\n\nIt can be expressed as a combination of Pauli gates: H = \\frac{1}{\\sqrt{2}}(X + Z)\n\n\n\n\nOn the Bloch sphere, the Hadamard gate performs a 180¬∞ rotation around the axis that is 45¬∞ between the X and Z axes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom qutip import Bloch, basis, sigmax, sigmaz\n\ndef hadamard_transform(state, t):\n    H = np.sqrt(1 - t) * sigmaz() + np.sqrt(t) * sigmax()\n    return (1 - t) * state + t * H * state\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nb = Bloch(axes=ax)\n\npsi0 = basis(2, 0)\nstates = [hadamard_transform(psi0, t) for t in np.linspace(0, 1, 50)]\n\nb.add_states(states)\nb.add_states([states[0], states[-1]], 'point')\nb.render()\n\nplt.title(\"Hadamard Transform on Bloch Sphere\")\nplt.tight_layout()\nplt.savefig('hadamard_transform.png', dpi=150, bbox_inches='tight')\nplt.close(fig)\n\n\n\n\nHadamard Transform on Bloch Sphere\n\n\nThe Hadamard gate is fundamental in quantum algorithms, including quantum Fourier transform and quantum phase estimation.\n\n\n\n\n\n\nQWorld\n\nDirac Bra-Ket Notation\nMermin, N. D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press.\nSutor, R. S. (2019). Dancing with Qubits: How quantum computing works and how it can change the world. Packt Publishing.\nGriffiths, D. J., & Schroeter, D. F. (2018). Introduction to Quantum Mechanics (3rd ed.). Cambridge University Press.\n\nPreskill, J. (2018). Quantum Computing in the NISQ era and beyond. Quantum, 2, 79."
  },
  {
    "objectID": "posts/2024-09-22-quantum-introduction.html#quantum-states",
    "href": "posts/2024-09-22-quantum-introduction.html#quantum-states",
    "title": "Introduction to Quantum Programming",
    "section": "",
    "text": "While a classical bit is always in a definite state of either 0 or 1, a qubit can exist in a superposition of states:\n\nSuperposition: A qubit can be in a superposition of the basis states |0\\rangle and |1\\rangle. This means it‚Äôs not just 0 or 1, but a complex linear combination of both.\nProbability Distribution vs.¬†Specific State: A qubit has a probability distribution over possible measurement outcomes, rather than a probability of being in a specific state. Before measurement, the qubit exists in superposition.\nState Vector: The state of a qubit is represented by a two-dimensional complex vector called a state vector. This vector contains the amplitudes for each basis state.\nMeasurement: When we measure a qubit, we force it to collapse into one of the basis states (typically |0\\rangle or |1\\rangle). The probability of each outcome is determined by the squared magnitude of the corresponding amplitude in the state vector.\n\n\n\n\nA qubit‚Äôs state is typically represented as:\n\n|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle\n\nWhere:\n- |\\psi\\rangle represents the qubit‚Äôs state\n- \\alpha and \\beta are complex numbers called probability amplitudes\n- |\\alpha|^2 gives the probability of measuring the qubit in state |0\\rangle\n- |\\beta|^2 gives the probability of measuring the qubit in state |1\\rangle\n- |\\alpha|^2 + |\\beta|^2 = 1 (normalization condition)\nA state vector is a mathematical representation of a qubit‚Äôs state in quantum mechanics. For a single qubit, it‚Äôs a two-dimensional complex vector that captures all the information about the qubit‚Äôs state.\n\n\nAmplitudes (\\alpha and \\beta) are complex numbers that determine the quantum state‚Äôs properties.\n\nProbability: The squared magnitude of an amplitude (|\\alpha|^2 or |\\beta|^2) gives the probability of measuring the qubit in the corresponding basis state.\nPhase: The complex phase of the amplitude carries information about the qubit‚Äôs behavior in interference situations.\nSuperposition: Non-zero amplitudes for both |0\\rangle and |1\\rangle indicate the qubit is in a superposition of these basis states.\n\n\n\n\n\n|\\psi\\rangle = 1|0\\rangle + 0|1\\rangle\n\nThis represents a qubit in the definite state |0\\rangle.\nMeasuring this qubit will always yield |0\\rangle.\n\n|\\psi\\rangle = 0|0\\rangle + 1|1\\rangle\n\nThis represents a qubit in the definite state |1\\rangle.\nMeasuring this qubit will always yield |1\\rangle.\n\n\n\n\n\n|\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle\n\nThis is often written as |+\\rangle and represents an equal superposition.\nProbability of measuring |0\\rangle: |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}\nProbability of measuring |1\\rangle: |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}\nThe qubit has an equal chance of being measured in either state.\n\n\n\n\n|\\psi\\rangle = \\frac{\\sqrt{3}}{2}|0\\rangle + \\frac{1}{2}|1\\rangle\n\nProbability of measuring |0\\rangle: |\\frac{\\sqrt{3}}{2}|^2 = \\frac{3}{4}\nProbability of measuring |1\\rangle: |\\frac{1}{2}|^2 = \\frac{1}{4}\nThe qubit is more likely to be measured in state |0\\rangle.\n\n\n\n\n|\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{i}{\\sqrt{2}}|1\\rangle\n\nProbability of measuring |0\\rangle: |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}\nProbability of measuring |1\\rangle: |\\frac{i}{\\sqrt{2}}|^2 = \\frac{1}{2}\nEqual probabilities, but the i in the second amplitude introduces a phase difference.\nThis phase difference is important in quantum interference but doesn‚Äôt affect single-qubit measurements.\n\n\n\n\n\nFor systems with multiple qubits, the state vector grows exponentially:\n\n1 qubit: 2-dimensional vector\n2 qubits: 4-dimensional vector\n3 qubits: 8-dimensional vector\nn qubits: 2^n dimensional vector\n\nThis exponential growth in the state space is what gives quantum computers their potential power relative to classical computers.\n\n\n\nQubits are often visualized using the Bloch sphere, a unit sphere in three-dimensional space. Any single qubit state can be represented as a point on the surface of this sphere.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom qutip import Bloch, basis, sigmax, sigmay, sigmaz\n\ndef plot_bloch_sphere(states, labels):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    b = Bloch(axes=ax)\n    b.point_color = ['r', 'g', 'b', 'y']\n    \n    for state, label in zip(states, labels):\n        b.add_states(state)\n        b.add_annotation(state, label)\n    \n    b.render()\n    return fig\n\n# some example states\npsi1 = basis(2, 0)  # |0‚ü© i.e. classical 0\npsi2 = basis(2, 1)  # |1‚ü© i.e. classical 1\npsi3 = (basis(2, 0) + basis(2, 1)).unit()  # (|0‚ü© + |1‚ü©)/‚àö2 i.e. equal superposition\npsi4 = (basis(2, 0) + 1j*basis(2, 1)).unit()  # (|0‚ü© + i|1‚ü©)/‚àö2 i.e. superposition with a phase difference\n\nstates = [psi1, psi2, psi3, psi4]\nlabels = [\"|0‚ü©\", \"|1‚ü©\", \"|+‚ü©\", \"|+i‚ü©\"]\n\nfig = plot_bloch_sphere(states, labels)\n\nplt.tight_layout()\nplt.savefig('bloch_sphere_example.png', dpi=150, bbox_inches='tight')\nplt.close(fig)\n\n\n\n\nExample Bloch Sphere representing quantum states\n\n\n\nThe |0\\rangle state is represented by a point at the north pole of the sphere.\n\nThe |1\\rangle state is represented by a point at the south pole of the sphere.\n\nThe |+\\rangle state (equal superposition) is represented by a point on the equator along the x-axis.\n\nThe |+i\\rangle state is represented by a point on the equator along the y-axis.\n\n\n\n\nWe can also use this to visualize how quantum states evolve under certain operations. Here‚Äôs an example that shows the state evolution under the action of a Hadamard-like gate:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom qutip import Bloch, basis, qeye, sigmax, sigmay, sigmaz\nfrom matplotlib.animation import FuncAnimation\nimport imageio\n\ndef plot_bloch_sphere(states, filename='bloch_sphere_evolution.gif', duration=0.1):\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    b = Bloch(axes=ax)\n\n    def update(frame):\n        ax.cla()\n        b = Bloch(axes=ax)\n        b.add_states(states[frame])\n        b.add_states([states[0], states[-1]], 'point')\n        b.render()\n        ax.set_title(f'Frame {frame + 1}/{len(states)}')\n\n    anim = FuncAnimation(fig, update, frames=len(states), repeat=True)\n\n    frames = []\n    for i in range(len(states)):\n        update(i)\n        fig.canvas.draw()\n        image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n        image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n        frames.append(image)\n\n    imageio.mimsave(filename, frames, duration=duration)\n    plt.close(fig)\n\ndef state_evolution(initial_state, operation, steps=50):\n    states = [initial_state]\n    for i in range(1, steps+1):\n        states.append(operation(initial_state, i/steps))\n    return states\n\ndef hadamard_like_transform(state, t):\n    H = np.sqrt(1 - t) * sigmaz() + np.sqrt(t) * sigmax()\n    return (1 - t) * state + t * H * state\n\npsi0 = basis(2, 0) # initial state |0‚ü©\n\nstates = state_evolution(psi0, hadamard_like_transform, steps=50)\n\nplot_bloch_sphere(states, 'bloch_sphere_evolution.gif', duration=0.1)\n\n/tmp/ipykernel_3956532/1348161396.py:26: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n  image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n\n\n\n\n\nExample Bloch Sphere representing quantum state evolution\n\n\nThis will show how the |0\\rangle state evolves into the |+\\rangle state under the action of the Hadamard-like gate.\n\n\n\n\nThe basis states are typically labeled |0\\rangle and |1\\rangle, which might suggest values between 0 and 1.\n\nHowever, the actual state of a qubit is described by complex amplitudes \\alpha and \\beta. These amplitudes can take any complex values as long as |\\alpha|^2 + |\\beta|^2 = 1.\n\nThe probabilities of measuring |0\\rangle or |1\\rangle (given by |\\alpha|^2 and |\\beta|^2 respectively) are between 0 and 1 (as they must be for any probability).\n\n\n\n\nWith a qubit in the state:\n\n|\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle\n\nThis state has an equal probability of being measured as |0\\rangle or |1\\rangle. The probabilities are:\n\nP(|0\\rangle) = |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}\nP(|1\\rangle) = |\\frac{1}{\\sqrt{2}}|^2 = \\frac{1}{2}"
  },
  {
    "objectID": "posts/2024-09-22-quantum-introduction.html#probability-vectors",
    "href": "posts/2024-09-22-quantum-introduction.html#probability-vectors",
    "title": "Introduction to Quantum Programming",
    "section": "",
    "text": "Probability vectors are fundamental in describing quantum states. They represent the probability distribution of possible outcomes when measuring a quantum system.\n\n\n\nAll entries are non-negative real numbers.\nThe sum of all entries equals 1.\n\n\n\n\nFor a coin flip:\np = [0.5, 0.5]  # Represents a fair coin (50% chance of heads, 50% chance of tails)"
  },
  {
    "objectID": "posts/2024-09-22-quantum-introduction.html#linear-combinations-and-convex-combinations",
    "href": "posts/2024-09-22-quantum-introduction.html#linear-combinations-and-convex-combinations",
    "title": "Introduction to Quantum Programming",
    "section": "",
    "text": "In quantum mechanics, states can be combined linearly, but probability vectors specifically use convex combinations.\n\n\nA linear combination where: 1. All coefficients are non-negative. 2. The sum of coefficients equals 1.\nThis ensures that the result is still a valid probability vector.\n\n\n\np1 = [0.3, 0.7]\np2 = [0.6, 0.4]\nŒ± = 0.4\n\nconvex_combination = Œ± * np.array(p1) + (1 - Œ±) * np.array(p2)\n# Result: [0.48, 0.52]"
  },
  {
    "objectID": "posts/2024-09-22-quantum-introduction.html#classical-transformations",
    "href": "posts/2024-09-22-quantum-introduction.html#classical-transformations",
    "title": "Introduction to Quantum Programming",
    "section": "",
    "text": "Classical transformations in quantum computing are represented by matrices that transform one probability vector into another.\n\n\n\nAll entries are non-negative.\nThe sum of each column is 1 (stochastic property).\n\n\n\n\nThe NOT operation flips a bit. In matrix form for a single qubit:\n\n\\text{NOT} = \\begin{bmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}"
  },
  {
    "objectID": "posts/2024-09-22-quantum-introduction.html#tensor-product",
    "href": "posts/2024-09-22-quantum-introduction.html#tensor-product",
    "title": "Introduction to Quantum Programming",
    "section": "",
    "text": "The tensor product is used to combine quantum systems, creating a joint state space.\n\n\n\nCombines probability vectors of individual systems.\nResults in a higher-dimensional probability vector.\n\n\n\n\nFor two coins:\ncoin1 = [0.5, 0.5]  # Fair coin\ncoin2 = [0.7, 0.3]  # Biased coin\n\njoint_state = np.kron(coin1, coin2)\n# Result: [0.35, 0.15, 0.35, 0.15]\nNot all higher-dimensional probability vectors can be decomposed into tensor products of lower-dimensional vectors.\n\n\n\nIn vector notation, the same state can be written as:\n|\\psi\\rangle = \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}\n\n\n\n\nKet |\\psi\\rangle: Represents a column vector.\n\nBra \\langle\\psi|: Represents the conjugate transpose of |\\psi\\rangle, i.e., a row vector.\n\nConjugate Transpose (or Hermitian Conjugate): formed from an m-by-n matrix by taking the transpose of the matrix (forming an n-by-m matrix), then taking the complex conjugate of each entry (negating their imaginary components).\n\n\n\n\n\nSince \\alpha and \\beta are complex numbers, they can be written in polar form:\n\\begin{align*}\n\\alpha &= r_1e^{i\\theta_1} \\\\\n\\beta &= r_2e^{i\\theta_2}\n\\end{align*}\nWhere: - r_1 and r_2 are the magnitudes (r_1^2 + r_2^2 = 1) - \\theta_1 and \\theta_2 are the phases - i is the imaginary unit (i^2 = -1)\n\n\n\nFor mixed states or when dealing with subsystems, we often use the density matrix representation:\n\\rho = |\\psi\\rangle\\langle\\psi| = \\begin{bmatrix}\n|\\alpha|^2 & \\alpha\\beta^* \\\\\n\\alpha^*\\beta & |\\beta|^2\n\\end{bmatrix}\nWhere * denotes complex conjugation.\n\n\n\n\n|0\\rangle state: |\\psi\\rangle = 1|0\\rangle + 0|1\\rangle = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n|1\\rangle state: |\\psi\\rangle = 0|0\\rangle + 1|1\\rangle = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n|+\\rangle state (equal superposition): |\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n|-\\rangle state: |\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle - \\frac{1}{\\sqrt{2}}|1\\rangle = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n|+i\\rangle state: |\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{i}{\\sqrt{2}}|1\\rangle = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ i \\end{bmatrix}\n\n\n\n\nThe Bloch sphere provides a geometrical representation of a qubit‚Äôs state. Any pure state of a qubit can be represented as:\n|\\psi\\rangle = \\cos\\left(\\frac{\\theta}{2}\\right)|0\\rangle + e^{i\\phi}\\sin\\left(\\frac{\\theta}{2}\\right)|1\\rangle\nWhere: - \\theta (theta) is the polar angle (0 \\leq \\theta \\leq \\pi) - \\phi (phi) is the azimuthal angle (0 \\leq \\phi &lt; 2\\pi)\nThis representation directly maps to the surface of the Bloch sphere.\n\n\n\nFor a system of n qubits, the state is represented by a 2^n dimensional complex vector. For example, a two-qubit system has the general form:\n|\\psi\\rangle = \\alpha|00\\rangle + \\beta|01\\rangle + \\gamma|10\\rangle + \\delta|11\\rangle\nWhere \\alpha, \\beta, \\gamma, and \\delta are complex amplitudes satisfying |\\alpha|^2 + |\\beta|^2 + |\\gamma|^2 + |\\delta|^2 = 1."
  },
  {
    "objectID": "posts/2024-09-22-quantum-introduction.html#quantum-circuits",
    "href": "posts/2024-09-22-quantum-introduction.html#quantum-circuits",
    "title": "Introduction to Quantum Programming",
    "section": "",
    "text": "Quantum circuits are sequences of operations (gates) applied to qubits. They are the quantum analogue of classical logic circuits.\n\n\n      ‚îå‚îÄ‚îÄ‚îÄ‚îê\nq_0: ‚îÄ‚î§ H ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n      ‚îî‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\nq_1: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                ‚îÇ\nq_2: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄX‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nThis circuit applies a Hadamard gate (H) to the first qubit and then a controlled-NOT (CNOT) with the first qubit as control and the third as target.\n\n\n\nThe CNOT (Controlled-NOT) gate is a two-qubit gate that flips the second qubit (target) if and only if the first qubit (control) is in the state |1\\rangle.\n\n\n\n\nThe four possible basis states for a two-qubit system:\n\n|00\\rangle\n|01\\rangle\n|10\\rangle\n|11\\rangle\n\n\n\n\nThe X gate (NOT gate) for a single qubit is represented by the matrix:\nX = \\begin{bmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\nand the 2x2 identity matrix is also used:\nI = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\n\n\nThe CNOT gate performs the following operations:\n\nIf the control qubit is |0\\rangle, do nothing to the target qubit.\nIf the control qubit is |1\\rangle, apply the X gate to the target qubit.\n\nThis can be expressed as:\nCNOT = |0\\rangle\\langle0| \\otimes I + |1\\rangle\\langle1| \\otimes X\nWhere \\otimes denotes the tensor product.\n\n|0\\rangle\\langle0| \\otimes I:\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix} \\otimes\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n|1\\rangle\\langle1| \\otimes X:\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix} \\otimes\n\\begin{bmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix} =\n\\begin{bmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{bmatrix}\n\n\n\n\nAdding these two matrices gives us the final CNOT matrix:\nCNOT = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{bmatrix}\n\n\n\n\nWe can verify this matrix by applying it to each basis state:\n\nCNOT|00\\rangle = |00\\rangle (No change)\n\nCNOT|01\\rangle = |01\\rangle (No change)\n\nCNOT|10\\rangle = |11\\rangle (Flips second qubit)\n\nCNOT|11\\rangle = |10\\rangle (Flips second qubit)\n\nThe CNOT operation only flips the second qubit when the first qubit is in state |1\\rangle.\n\n\n\n\nThe Hadamard gate is a single-qubit gate that creates superpositions. It‚Äôs often used to initialize qubits in quantum algorithms.\n\n\nThe Hadamard gate is defined by the following matrix:\nH = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}\n1 & 1 \\\\\n1 & -1\n\\end{bmatrix}\n\n\n\nWe can derive the Hadamard gate by considering its action on the computational basis states:\n\nFor |0\\rangle:\nH|0\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)\nFor |1\\rangle:\nH|1\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle - |1\\rangle)\n\nThese actions can be represented in matrix form:\nH\\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}\n1 \\\\\n1\n\\end{bmatrix}\nH\\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}\n1 \\\\\n-1\n\\end{bmatrix}\nFrom these, we can construct the Hadamard matrix:\nH = \\frac{1}{\\sqrt{2}} \\begin{bmatrix}\n1 & 1 \\\\\n1 & -1\n\\end{bmatrix}\n\n\n\n\nThe Hadamard gate is its own inverse: H^2 = I\n\nIt creates equal superpositions of computational basis states\n\nIt can be expressed as a combination of Pauli gates: H = \\frac{1}{\\sqrt{2}}(X + Z)\n\n\n\n\nOn the Bloch sphere, the Hadamard gate performs a 180¬∞ rotation around the axis that is 45¬∞ between the X and Z axes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom qutip import Bloch, basis, sigmax, sigmaz\n\ndef hadamard_transform(state, t):\n    H = np.sqrt(1 - t) * sigmaz() + np.sqrt(t) * sigmax()\n    return (1 - t) * state + t * H * state\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nb = Bloch(axes=ax)\n\npsi0 = basis(2, 0)\nstates = [hadamard_transform(psi0, t) for t in np.linspace(0, 1, 50)]\n\nb.add_states(states)\nb.add_states([states[0], states[-1]], 'point')\nb.render()\n\nplt.title(\"Hadamard Transform on Bloch Sphere\")\nplt.tight_layout()\nplt.savefig('hadamard_transform.png', dpi=150, bbox_inches='tight')\nplt.close(fig)\n\n\n\n\nHadamard Transform on Bloch Sphere\n\n\nThe Hadamard gate is fundamental in quantum algorithms, including quantum Fourier transform and quantum phase estimation."
  },
  {
    "objectID": "posts/2024-09-22-quantum-introduction.html#references",
    "href": "posts/2024-09-22-quantum-introduction.html#references",
    "title": "Introduction to Quantum Programming",
    "section": "",
    "text": "QWorld\n\nDirac Bra-Ket Notation\nMermin, N. D. (2007). Quantum Computer Science: An Introduction. Cambridge University Press.\nSutor, R. S. (2019). Dancing with Qubits: How quantum computing works and how it can change the world. Packt Publishing.\nGriffiths, D. J., & Schroeter, D. F. (2018). Introduction to Quantum Mechanics (3rd ed.). Cambridge University Press.\n\nPreskill, J. (2018). Quantum Computing in the NISQ era and beyond. Quantum, 2, 79."
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html",
    "href": "posts/2024-12-17-duplicate-items.html",
    "title": "Duplicate Items",
    "section": "",
    "text": "Given an unsorted array of integers, determine if any value appears more than once. The function should return true if any value appears at least twice in the array, and false if every element is distinct.\nExample 1:\nInput: nums = [1, 2, 3, 3]\n\nOutput: true\nExample 2:\nInput: nums = [1, 2, 3, 4]\n\nOutput: false\nIdeally the solution would have O(n) space and time complexity.\n\n\n\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    items = set(nums)\n    return len(items) != len(nums)\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False\n\n\n\n\n\nCreating a set from a list (items = set(nums)):\n\nPython‚Äôs set creation involves hashing each element i.e.¬†for each element, Python must:\n\nCompute the hash value (O(1) for integers)\n\nHandle potential hash collisions\n\nPotentially resize the hash table\n\n\namortised time complexity is O(n), but individual operations might take longer due to rehashing\n\nLength operations (len(items) and len(nums)):\n\nBoth len() operations are O(1) in Python\n\nPython maintains a length counter for both lists and sets\nNo iteration is required to get the length\n\n\nThe overall complexity is O(n); constant factors involved in set creation can be significant due to the hashing overhead. Space complexity is O(n) as we store each unique element in the set.\n\n\n\n\nThe initial solution could be modified to exit early:\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    seen = set()\n    for num in nums:\n        if num in seen:\n            return True\n        seen.add(num)\n    return False\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False\n\n\nThis approach can be more efficient:\n- It stops immediately upon finding a duplicate and the set grows only until a duplicate is found\n- Each set operation (in/add) is amortised O(1)\n\n\n\n\nfrom collections import Counter\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    counter = Counter(nums)\n    return any(count &gt; 1 for count in counter.values())\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False\n\n\n\n\n\nIf the input array is mutable, we can sort the array with O(n \\log n) time, with the trade off for reduced space of O(1):\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    if not nums:\n        return False\n    nums.sort()\n    return any(nums[i] == nums[i+1] for i in range(len(nums)-1))\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False\n\n\n\n\n\nIf the input array is mutable, all numbers are positive, and the numbers are within the array‚Äôs length i.e.¬†(0 to n-1) inclusive, then we can use the array indices as a hash function (each index corresponds to a unique value or element within the range) with the sign of each element in the array as a marker (i.e.¬†as a visited marker array, if negated, marks that we have previously visited this value). This eliminates the need for additional space i.e.¬†this is an in-place solution.\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    n = len(nums)\n    for i in range(n):\n        index = abs(nums[i])  # get the index of the item - abs() as duplicates may already be negated\n        if index &gt;= n:        # skip if the index is out of range\n            continue\n        if nums[index] &lt; 0:   # duplicate found if already negative\n            return True\n        nums[index] = -nums[index]  # mark as seen i.e. make negative\n    return False\n\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False\n\n\n\n\n\nGiven input nums = [2, 3, 1, 2]\n\n\nIndex:     0   1   2   3\nnums:      2   3   1   2\n\ni = 0, nums[0] = 2:\n- abs(nums[0]) = 2 as our index\n- check nums[2] = 1 (positive)\n- negate nums[2] to mark we've seen 2\n\nIndex:     0   1   2   3\nnums:      2   3  -1   2\ni = 1, nums[1] = 3:\n- abs(nums[1]) = 3 as our index\n- check nums[3] = 2 (positive)\n- negate nums[3] to mark we've seen 3\n\nIndex:     0   1   2   3\nnums:      2   3  -1  -2\ni = 2, nums[2] = -1:\n- abs(nums[2]) = 1 as our index\n- check nums[1] = 3 (positive)\n- negate nums[1] to mark we've seen 1\n\nIndex:     0   1   2   3\nnums:      2  -3  -1  -2\ni = 3, nums[3] = -2:\n- abs(nums[3]) = 2 as our index\n- check nums[2] = -1 (negative)\n- since nums[2] is negative, return True as we have a duplicate\n\nIndex:     0   1   2   3\nnums:      2  -3  -1  -2\n\n\nWhen we encounter a number n, we use its absolute value as an index and negate the value at that index.\n\nIf we later find that value at index = abs(n) is already negative, it means we‚Äôve seen this number before.\n\nThis approach cleverly uses the array itself as its own hash table, with the sign bit serving as our presence marker."
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#problem",
    "href": "posts/2024-12-17-duplicate-items.html#problem",
    "title": "Duplicate Items",
    "section": "",
    "text": "Given an unsorted array of integers, determine if any value appears more than once. The function should return true if any value appears at least twice in the array, and false if every element is distinct.\nExample 1:\nInput: nums = [1, 2, 3, 3]\n\nOutput: true\nExample 2:\nInput: nums = [1, 2, 3, 4]\n\nOutput: false\nIdeally the solution would have O(n) space and time complexity."
  },
  {
    "objectID": "posts/2023-04-05-functional-programming-python.html",
    "href": "posts/2023-04-05-functional-programming-python.html",
    "title": "Introduction to Functional Programming In Python",
    "section": "",
    "text": "Python functions can take functions as arguments and can return functions to their calling site.\n\ndef inside():\n    print(\"This is inside\")\n\ndef outside(function):\n    function()\noutside(inside)\n\nThis is inside\n\n\nInner functions can sometimes be referred to as callbacks as they are not invoked immediately. Forexample, passing a custom function to sorted():\n\nfruits = [\"apple\", \"tomato\", \"pear\", \"blueberry\"]\n\ndef reverse_length(x):\n    return -len(x)\n\nprint(sorted(fruits, key=reverse_length))\n\n['blueberry', 'tomato', 'apple', 'pear']\n\n\nFunctions can also return functions, which can be called without intermediate assignement:\n\ndef inside():\n    print(\"This is inside\")\n\ndef outside(function):\n    return function\noutside(inside)()\n\nThis is inside\n\n\nLambda expressions can be useful and take the following form:\nlambda &lt;parameter_list (optional)&gt;: &lt;expression&gt;\nLambda expressions have their own namespace, and can access but not modify global variables. For example:\n\nreverse = lambda x: x[::-1]\ncallable(reverse)\nprint(reverse(\"Hello World!\"))\nprint(reverse([1,2,3,4]))\n\nprint((lambda x1, x2: x1+x2)(2,2))\n\nmeaning_of_life = lambda: 42\nprint(meaning_of_life())\n\n!dlroW olleH\n[4, 3, 2, 1]\n4\n42\n\n\nThe return value of a lambda can only be a single expression, and not assignemnet or return statements, or other control constructs. It also does not allow implicit tuple unpacking. Tuples, lists and dictionaries can still be returned, but reqiore explicit parentheses:\n\nprint((lambda x: (x, x**2, x**3))(2))\nprint((lambda x: [x, x**2, x**3])(2))\nprint((lambda x: {1:x, 2:x**2, 3:x**3})(2))\n\n(2, 4, 8)\n[2, 4, 8]\n{1: 2, 2: 4, 3: 8}\n\n\n\n\n\nMap can apply a function to each element of an iterable, and can return an iterator which can yield the results. Map takes the following form\nmap(&lt;function&gt;, &lt;iterable&gt;)\n\nsquare = lambda x: x**2\nnumbers = [1,2,3,4,5,6,7]\niterable = map(square, numbers)\nprint(next(iterable))\nprint(next(iterable))\nprint(next(iterable))\nprint(list(iterable))\n\n1\n4\n9\n[16, 25, 36, 49]\n\n\nIterating over the iterable yeilds the items.\nMap can also take the form map(&lt;function&gt;, &lt;iterable$_1$&gt;)"
  },
  {
    "objectID": "posts/2023-04-05-functional-programming-python.html#basic-syntax",
    "href": "posts/2023-04-05-functional-programming-python.html#basic-syntax",
    "title": "Introduction to Functional Programming In Python",
    "section": "",
    "text": "Python functions can take functions as arguments and can return functions to their calling site.\n\ndef inside():\n    print(\"This is inside\")\n\ndef outside(function):\n    function()\noutside(inside)\n\nThis is inside\n\n\nInner functions can sometimes be referred to as callbacks as they are not invoked immediately. Forexample, passing a custom function to sorted():\n\nfruits = [\"apple\", \"tomato\", \"pear\", \"blueberry\"]\n\ndef reverse_length(x):\n    return -len(x)\n\nprint(sorted(fruits, key=reverse_length))\n\n['blueberry', 'tomato', 'apple', 'pear']\n\n\nFunctions can also return functions, which can be called without intermediate assignement:\n\ndef inside():\n    print(\"This is inside\")\n\ndef outside(function):\n    return function\noutside(inside)()\n\nThis is inside\n\n\nLambda expressions can be useful and take the following form:\nlambda &lt;parameter_list (optional)&gt;: &lt;expression&gt;\nLambda expressions have their own namespace, and can access but not modify global variables. For example:\n\nreverse = lambda x: x[::-1]\ncallable(reverse)\nprint(reverse(\"Hello World!\"))\nprint(reverse([1,2,3,4]))\n\nprint((lambda x1, x2: x1+x2)(2,2))\n\nmeaning_of_life = lambda: 42\nprint(meaning_of_life())\n\n!dlroW olleH\n[4, 3, 2, 1]\n4\n42\n\n\nThe return value of a lambda can only be a single expression, and not assignemnet or return statements, or other control constructs. It also does not allow implicit tuple unpacking. Tuples, lists and dictionaries can still be returned, but reqiore explicit parentheses:\n\nprint((lambda x: (x, x**2, x**3))(2))\nprint((lambda x: [x, x**2, x**3])(2))\nprint((lambda x: {1:x, 2:x**2, 3:x**3})(2))\n\n(2, 4, 8)\n[2, 4, 8]\n{1: 2, 2: 4, 3: 8}"
  },
  {
    "objectID": "posts/2023-04-05-functional-programming-python.html#map",
    "href": "posts/2023-04-05-functional-programming-python.html#map",
    "title": "Introduction to Functional Programming In Python",
    "section": "",
    "text": "Map can apply a function to each element of an iterable, and can return an iterator which can yield the results. Map takes the following form\nmap(&lt;function&gt;, &lt;iterable&gt;)\n\nsquare = lambda x: x**2\nnumbers = [1,2,3,4,5,6,7]\niterable = map(square, numbers)\nprint(next(iterable))\nprint(next(iterable))\nprint(next(iterable))\nprint(list(iterable))\n\n1\n4\n9\n[16, 25, 36, 49]\n\n\nIterating over the iterable yeilds the items.\nMap can also take the form map(&lt;function&gt;, &lt;iterable$_1$&gt;)"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#my",
    "href": "posts/2024-12-17-duplicate-items.html#my",
    "title": "Duplicate Items",
    "section": "",
    "text": "class Test:\n    pass"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#initial-solutiuon",
    "href": "posts/2024-12-17-duplicate-items.html#initial-solutiuon",
    "title": "Duplicate Items",
    "section": "",
    "text": "class Test:\n    pass"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#initial-solutiion",
    "href": "posts/2024-12-17-duplicate-items.html#initial-solutiion",
    "title": "Duplicate Items",
    "section": "",
    "text": "class Test:\n    pass"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#initial-solution",
    "href": "posts/2024-12-17-duplicate-items.html#initial-solution",
    "title": "Duplicate Items",
    "section": "",
    "text": "def hasDuplicate(nums: list[int]) -&gt; bool:\n    items = set(nums)\n    return len(items) != len(nums)\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False\n\n\nThis could be modified to exit early:\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    seen = set()\n    for num in nums:\n        if num in seen:\n            return True\n        seen.add(num)\n    return False\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#initial-solution---set-based-approach",
    "href": "posts/2024-12-17-duplicate-items.html#initial-solution---set-based-approach",
    "title": "Duplicate Items",
    "section": "",
    "text": "def hasDuplicate(nums: list[int]) -&gt; bool:\n    items = set(nums)\n    return len(items) != len(nums)\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False\n\n\n\n\n\nCreating a set from a list (items = set(nums)):\n\nPython‚Äôs set creation involves hashing each element i.e.¬†for each element, Python must:\n\nCompute the hash value (O(1) for integers)\n\nHandle potential hash collisions\n\nPotentially resize the hash table\n\n\namortised time complexity is O(n), but individual operations might take longer due to rehashing\n\nLength operations (len(items) and len(nums)):\n\nBoth len() operations are O(1) in Python\n\nPython maintains a length counter for both lists and sets\nNo iteration is required to get the length\n\n\nThe overall complexity is O(n); constant factors involved in set creation can be significant due to the hashing overhead. Space complexity is O(n) as we store each unique element in the set."
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#alternative-solution---counter-approach",
    "href": "posts/2024-12-17-duplicate-items.html#alternative-solution---counter-approach",
    "title": "Duplicate Items",
    "section": "",
    "text": "from collections import Counter\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    counter = Counter(nums)\n    return any(count &gt; 1 for count in counter.values())\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#alternative-solution---optimising-for-space",
    "href": "posts/2024-12-17-duplicate-items.html#alternative-solution---optimising-for-space",
    "title": "Duplicate Items",
    "section": "",
    "text": "If the input array is mutable, we can sort the array with O(n \\log n) time, with the trade off for reduced space of O(1):\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    if not nums:\n        return False\n    nums.sort()\n    return any(nums[i] == nums[i+1] for i in range(len(nums)-1))\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#complexity-analysis",
    "href": "posts/2024-12-17-duplicate-items.html#complexity-analysis",
    "title": "Duplicate Items",
    "section": "",
    "text": "Creating a set from a list (items = set(nums)):\n\nPython‚Äôs set creation involves hashing each element i.e.¬†for each element, Python must:\n\nCompute the hash value (O(1) for integers)\n\nHandle potential hash collisions\n\nPotentially resize the hash table\n\n\nAmortized time complexity is O(n), but individual operations might take longer due to rehashing\n\nLength operations (len(items) and len(nums)):\n\nBoth len() operations are O(1) in Python\n\nPython maintains a length counter for both lists and sets\nNo iteration is required to get the length\n\n\nThe overall complexity is O(n); constant factors involved in set creation can be significant due to the hashing overhead. Space complexity is O(n) as we store each unique element in the set.\nThis could be modified to exit early:\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    seen = set()\n    for num in nums:\n        if num in seen:\n            return True\n        seen.add(num)\n    return False\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#initial-solution---set-based-approach-1",
    "href": "posts/2024-12-17-duplicate-items.html#initial-solution---set-based-approach-1",
    "title": "Duplicate Items",
    "section": "",
    "text": "This could be modified to exit early:\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    seen = set()\n    for num in nums:\n        if num in seen:\n            return True\n        seen.add(num)\n    return False\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#modified-initial-solution---set-based-approach",
    "href": "posts/2024-12-17-duplicate-items.html#modified-initial-solution---set-based-approach",
    "title": "Duplicate Items",
    "section": "",
    "text": "This could be modified to exit early:\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    seen = set()\n    for num in nums:\n        if num in seen:\n            return True\n        seen.add(num)\n    return False\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#modified-initial-solution---exit-early-set-based-approach",
    "href": "posts/2024-12-17-duplicate-items.html#modified-initial-solution---exit-early-set-based-approach",
    "title": "Duplicate Items",
    "section": "",
    "text": "The initial solution could be modified to exit early:\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    seen = set()\n    for num in nums:\n        if num in seen:\n            return True\n        seen.add(num)\n    return False\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False\n\n\nThis approach can be more efficient:\n- It stops immediately upon finding a duplicate and the set grows only until a duplicate is found\n- Each set operation (in/add) is amortised O(1)"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#alternative-solution---optimising-for-space-1",
    "href": "posts/2024-12-17-duplicate-items.html#alternative-solution---optimising-for-space-1",
    "title": "Duplicate Items",
    "section": "",
    "text": "If the input array is mutable, we can sort the array with O(n \\log n) time, with the trade off for reduced space of O(1):\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    if not nums:\n        return False\n    nums.sort()\n    return any(nums[i] == nums[i+1] for i in range(len(nums)-1))\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#alternative-solution---array-as-hash-table-approach",
    "href": "posts/2024-12-17-duplicate-items.html#alternative-solution---array-as-hash-table-approach",
    "title": "Duplicate Items",
    "section": "",
    "text": "If the input array is mutable, all numbers are positive, and the numbers are within the array‚Äôs length i.e.¬†(0 to n-1) inclusive, then we can use the array indices as a hash function (each index corresponds to a unique value or element within the range) with the sign of each element in the array as a marker (i.e.¬†as a visited marker array, if negated, marks that we have previously visited this value). This eliminates the need for additional space i.e.¬†this is an in-place solution.\n\ndef hasDuplicate(nums: list[int]) -&gt; bool:\n    n = len(nums)\n    for i in range(n):\n        index = abs(nums[i])  # get the index of the item - abs() as duplicates may already be negated\n        if index &gt;= n:        # skip if the index is out of range\n            continue\n        if nums[index] &lt; 0:   # duplicate found if already negative\n            return True\n        nums[index] = -nums[index]  # mark as seen i.e. make negative\n    return False\n\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#visualizing-the-array-as-hash-table-approach",
    "href": "posts/2024-12-17-duplicate-items.html#visualizing-the-array-as-hash-table-approach",
    "title": "Duplicate Items",
    "section": "",
    "text": "Index:     0   1   2   3\nnums:      2   3   1   2\n\n\n\n\nFirst Iteration (i = 0, examining nums[0] = 2):\n- Take abs(nums[0]) = 2 as our index\n- Check nums[2] = 1 (positive)\n- Negate nums[2] to mark we've seen 2\n\nIndex:     0   1   2   3\nnums:      2   3  -1   2\nSecond Iteration (i = 1, examining nums[1] = 3):\n- Take abs(nums[1]) = 3 as our index\n- Check nums[3] = 2 (positive)\n- Negate nums[3] to mark we've seen 3\n\nIndex:     0   1   2   3\nnums:      2   3  -1  -2\nThird Iteration (i = 2, examining nums[2] = -1):\n- Take abs(nums[2]) = 1 as our index\n- Check nums[1] = 3 (positive)\n- Negate nums[1] to mark we've seen 1\n\nIndex:     0   1   2   3\nnums:      2  -3  -1  -2\nFinal Iteration (i = 3, examining nums[3] = -2):\n- Take abs(nums[3]) = 2 as our index\n- Check nums[2] = -1 (negative)\n- Since nums[2] is negative, we've found a duplicate!\n\nIndex:     0   1   2   3\nnums:      2  -3  -1  -2\n\n\n\n\n\nWhen we encounter a number n, we use its absolute value as an index and negate the value at that index.\nIf we later find that value at index = abs(n) is already negative, it means we‚Äôve seen this number before.\nThis approach cleverly uses the array itself as its own hash table, with the sign bit serving as our presence marker.\n\nThis visualization demonstrates why the approach requires: - All numbers to be within the array‚Äôs valid index range (0 to n-1) - The ability to modify the input array - Special handling for zero (since negating zero doesn‚Äôt change its sign)"
  },
  {
    "objectID": "posts/2024-12-17-duplicate-items.html#visualising-the-array-as-hash-table-approach",
    "href": "posts/2024-12-17-duplicate-items.html#visualising-the-array-as-hash-table-approach",
    "title": "Duplicate Items",
    "section": "",
    "text": "Given input nums = [2, 3, 1, 2]\n\n\nIndex:     0   1   2   3\nnums:      2   3   1   2\n\ni = 0, nums[0] = 2:\n- abs(nums[0]) = 2 as our index\n- check nums[2] = 1 (positive)\n- negate nums[2] to mark we've seen 2\n\nIndex:     0   1   2   3\nnums:      2   3  -1   2\ni = 1, nums[1] = 3:\n- abs(nums[1]) = 3 as our index\n- check nums[3] = 2 (positive)\n- negate nums[3] to mark we've seen 3\n\nIndex:     0   1   2   3\nnums:      2   3  -1  -2\ni = 2, nums[2] = -1:\n- abs(nums[2]) = 1 as our index\n- check nums[1] = 3 (positive)\n- negate nums[1] to mark we've seen 1\n\nIndex:     0   1   2   3\nnums:      2  -3  -1  -2\ni = 3, nums[3] = -2:\n- abs(nums[3]) = 2 as our index\n- check nums[2] = -1 (negative)\n- since nums[2] is negative, return True as we have a duplicate\n\nIndex:     0   1   2   3\nnums:      2  -3  -1  -2\n\n\nWhen we encounter a number n, we use its absolute value as an index and negate the value at that index.\n\nIf we later find that value at index = abs(n) is already negative, it means we‚Äôve seen this number before.\n\nThis approach cleverly uses the array itself as its own hash table, with the sign bit serving as our presence marker."
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html",
    "href": "posts/2024-12-17-is-anagram.html",
    "title": "Anagram",
    "section": "",
    "text": "Given two strings s and t, determine if they are anagrams of each other.\n\n\nExample 1:\nInput: s = \"racecar\", t = \"carrace\"\nOutput: true\nExplanation: Both strings contain exactly the same characters: 'a', 'c', 'c', 'e', 'r', 'r'\n\n\n\n\nBoth strings consist only of lowercase English letters\nThe strings can be empty or have different lengths\n\n\n\n\n\nThe most intuitive approach might be to sort both strings and compare them:\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n\n    if len(s) != len(t):\n        return False\n\n    return sorted(s) == sorted(t)\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\"))\nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))\n\nTest 1: True\nTest 2: False\n\n\n\n\n\nSorting each string:\n\nPython‚Äôs default sorting algorithm (Timsort) has a time complexity of O(n \\log n)\nWhere n is the length of the string\n\nString comparison (sorted(s) == sorted(t)):\n\nRequires comparing each character: O(n)\nBut this is overshadowed by the sorting complexity\n\n\nOverall:\n- Time Complexity: O(n \\log n)\n- Space Complexity: O(n) for creating new sorted strings\n\n\n\n\nWe can achieve better time complexity using a character frequency counter:\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n    if len(s) != len(t):\n        return False\n    \n    char_count = [0] * 26\n    for i in range(len(s)):\n        char_count[ord(s[i]) - ord('a')] += 1\n        char_count[ord(t[i]) - ord('a')] -= 1\n    \n\n    return all(count == 0 for count in char_count)\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\")) \nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))          \n\nTest 1: True\nTest 2: False\n\n\n\n\n\nAn anagram must have exactly the same number of each character\nWe only need to track 26 possible characters (lowercase letters)\nWe can increment counts for one string and decrement for the other\nIf the strings are anagrams, all final counts will be zero\n\n\n\n\n\nLength comparison: O(1)\nCreating counter array: O(1) (fixed size of 26)\nCounting characters: O(n) where n is string length\nChecking final counts: O(1) (always 26 comparisons)\n\nOverall: - Time Complexity: O(n) - Space Complexity: O(1) (fixed size array)\nor alternatively:\n\nfrom collections import Counter\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n    counter_s = Counter(s)\n    counter_t = Counter(t)\n    return counter_s == counter_t\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\"))\nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))              \n\nTest 1: True\nTest 2: False\n\n\n\n\n\nThe Counter approach has some key implementation details worth understanding:\n\nCounter Creation (O(n)):\n\nEach Counter() call iterates through its input string once\nCounter uses a hash table internally for frequency counting\nEach character insertion is amortised O(1)\n\nCounter Comparison (O(n)):\n\nCounter comparison checks if all elements and their frequencies match\nMust examine each unique character at least once\n\n\nOverall:\n- Time Complexity: O(n) where n is the length of the strings\n- Space Complexity: O(k) where k is the size of the character set (26 for lowercase letters)\n\n\n\n\nFor cases where we might have a larger character set (not just lowercase letters), we can use a hash map:\n\nfrom collections import defaultdict\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n    if len(s) != len(t):\n        return False\n    \n    char_count = defaultdict(int)\n    \n    for s_char, t_char in zip(s, t):\n        char_count[s_char] += 1\n        char_count[t_char] -= 1\n    \n    return all(count == 0 for count in char_count.values())\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\")) \nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))\n\nTest 1: True\nTest 2: False\n\n\n\n\n\nDictionary operations (defaultdict):\n\nEach insertion/lookup is amortised O(1)\n\nSpace considerations:\n\nGrows with unique characters in input\nStill O(k) where k is the size of the character set\n\n\nOverall:\n- Time Complexity: O(n)\n- Space Complexity: O(k) where k is the character set size\n\n\n\n\n\n\n\nApproach\nTime Complexity\nSpace Complexity\nBest Used When\n\n\n\n\nSorting\nO(n \\log n)\nO(n)\nSimple solution needed\n\n\nArray Counter\nO(n)\nO(1)\nKnown small character set\n\n\nHash Map\nO(n)\nO(k)\nLarge/unknown character set\n\n\n\n\n\n\n\nThe array counter approach is most efficient for this specific problem due to the constraint of lowercase letters only.\nThe hash map approach is more flexible and can handle any character set."
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#problem",
    "href": "posts/2024-12-17-is-anagram.html#problem",
    "title": "Anagram",
    "section": "",
    "text": "Given an unsorted array of integers, determine if any value appears more than once. The function should return true if any value appears at least twice in the array, and false if every element is distinct.\nExample 1:\nInput: nums = [1, 2, 3, 3]\n\nOutput: true\nExample 2:\nInput: nums = [1, 2, 3, 4]\n\nOutput: false\nIdeally the solution would have O(n) space and time complexity."
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#initial-solution---set-based-approach",
    "href": "posts/2024-12-17-is-anagram.html#initial-solution---set-based-approach",
    "title": "Anagram",
    "section": "",
    "text": "def hasDuplicate(nums: list[int]) -&gt; bool:\n    items = set(nums)\n    return len(items) != len(nums)\n\nprint('[1,2,3,3]',hasDuplicate([1,2,3,3]))\nprint('[1,2,3,4]',hasDuplicate([1,2,3,4]))\n\n[1,2,3,3] True\n[1,2,3,4] False\n\n\n\n\n\nCreating a set from a list (items = set(nums)):\n\nPython‚Äôs set creation involves hashing each element i.e.¬†for each element, Python must:\n\nCompute the hash value (O(1) for integers)\n\nHandle potential hash collisions\n\nPotentially resize the hash table\n\n\namortised time complexity is O(n), but individual operations might take longer due to rehashing\n\nLength operations (len(items) and len(nums)):\n\nBoth len() operations are O(1) in Python\n\nPython maintains a length counter for both lists and sets\nNo iteration is required to get the length\n\n\nThe overall complexity is O(n); constant factors involved in set creation can be significant due to the hashing overhead. Space complexity is O(n) as we store each unique element in the set."
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#problem-statement",
    "href": "posts/2024-12-17-is-anagram.html#problem-statement",
    "title": "Anagram",
    "section": "",
    "text": "Given two strings s and t, determine if they are anagrams of each other.\n\n\nExample 1:\nInput: s = \"racecar\", t = \"carrace\"\nOutput: true\nExplanation: Both strings contain exactly the same characters: 'a', 'c', 'c', 'e', 'r', 'r'\n\n\n\n\nBoth strings consist only of lowercase English letters\nThe strings can be empty or have different lengths"
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#initial-solution---sorting-approach",
    "href": "posts/2024-12-17-is-anagram.html#initial-solution---sorting-approach",
    "title": "Anagram",
    "section": "",
    "text": "The most intuitive approach might be to sort both strings and compare them:\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n\n    if len(s) != len(t):\n        return False\n\n    return sorted(s) == sorted(t)\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\"))\nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))\n\nTest 1: True\nTest 2: False\n\n\n\n\n\nSorting each string:\n\nPython‚Äôs default sorting algorithm (Timsort) has a time complexity of O(n \\log n)\nWhere n is the length of the string\n\nString comparison (sorted(s) == sorted(t)):\n\nRequires comparing each character: O(n)\nBut this is overshadowed by the sorting complexity\n\n\nOverall:\n- Time Complexity: O(n \\log n)\n- Space Complexity: O(n) for creating new sorted strings"
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#optimized-solution---character-count-approach",
    "href": "posts/2024-12-17-is-anagram.html#optimized-solution---character-count-approach",
    "title": "Anagram",
    "section": "",
    "text": "We can achieve better time complexity using a character frequency counter:\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n    if len(s) != len(t):\n        return False\n    \n    char_count = [0] * 26\n    for i in range(len(s)):\n        char_count[ord(s[i]) - ord('a')] += 1\n        char_count[ord(t[i]) - ord('a')] -= 1\n    \n\n    return all(count == 0 for count in char_count)\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\")) \nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))          \n\nTest 1: True\nTest 2: False\n\n\n\n\n\nAn anagram must have exactly the same number of each character\nWe only need to track 26 possible characters (lowercase letters)\nWe can increment counts for one string and decrement for the other\nIf the strings are anagrams, all final counts will be zero\n\n\n\n\n\nLength comparison: O(1)\nCreating counter array: O(1) (fixed size of 26)\nCounting characters: O(n) where n is string length\nChecking final counts: O(1) (always 26 comparisons)\n\nOverall: - Time Complexity: O(n) - Space Complexity: O(1) (fixed size array)\nor alternatively:\n\nfrom collections import Counter\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n    counter_s = Counter(s)\n    counter_t = Counter(t)\n    return counter_s == counter_t\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\"))\nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))              \n\nTest 1: True\nTest 2: False\n\n\n\n\n\nThe Counter approach has some key implementation details worth understanding:\n\nCounter Creation (O(n)):\n\nEach Counter() call iterates through its input string once\nCounter uses a hash table internally for frequency counting\nEach character insertion is amortized O(1)\n\nCounter Comparison (O(n)):\n\nPython‚Äôs Counter comparison checks if all elements and their frequencies match\nMust examine each unique character at least once\n\n\nOverall: - Time Complexity: O(n) where n is the length of the strings - Space Complexity: O(k) where k is the size of the character set (26 for lowercase letters)"
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#alternative-solution---hash-map-approach",
    "href": "posts/2024-12-17-is-anagram.html#alternative-solution---hash-map-approach",
    "title": "Anagram",
    "section": "",
    "text": "For cases where we might have a larger character set (not just lowercase letters), we can use a hash map:\n\nfrom collections import defaultdict\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n    if len(s) != len(t):\n        return False\n    \n    char_count = defaultdict(int)\n    \n    for s_char, t_char in zip(s, t):\n        char_count[s_char] += 1\n        char_count[t_char] -= 1\n    \n    return all(count == 0 for count in char_count.values())\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\")) \nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))\n\nTest 1: True\nTest 2: False\n\n\n\n\n\nDictionary operations (defaultdict):\n\nEach insertion/lookup is amortised O(1)\n\nSpace considerations:\n\nGrows with unique characters in input\nStill O(k) where k is the size of the character set\n\n\nOverall:\n- Time Complexity: O(n)\n- Space Complexity: O(k) where k is the character set size"
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#performance-comparison",
    "href": "posts/2024-12-17-is-anagram.html#performance-comparison",
    "title": "Anagram",
    "section": "",
    "text": "Approach\nTime Complexity\nSpace Complexity\nBest Used When\n\n\n\n\nSorting\nO(n \\log n)\nO(n)\nSimple solution needed\n\n\nArray Counter\nO(n)\nO(1)\nKnown small character set\n\n\nHash Map\nO(n)\nO(k)\nLarge/unknown character set"
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#key-takeaways",
    "href": "posts/2024-12-17-is-anagram.html#key-takeaways",
    "title": "Anagram",
    "section": "",
    "text": "The array counter approach is most efficient for this specific problem due to the constraint of lowercase letters only.\nThe hash map approach is more flexible and can handle any character set.\nThe sorting approach, while intuitive, is less efficient but might be more readable for small inputs."
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#optimised-solution---character-count-approach",
    "href": "posts/2024-12-17-is-anagram.html#optimised-solution---character-count-approach",
    "title": "Anagram",
    "section": "",
    "text": "We can achieve better time complexity using a character frequency counter:\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n    if len(s) != len(t):\n        return False\n    \n    char_count = [0] * 26\n    for i in range(len(s)):\n        char_count[ord(s[i]) - ord('a')] += 1\n        char_count[ord(t[i]) - ord('a')] -= 1\n    \n\n    return all(count == 0 for count in char_count)\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\")) \nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))          \n\nTest 1: True\nTest 2: False\n\n\n\n\n\nAn anagram must have exactly the same number of each character\nWe only need to track 26 possible characters (lowercase letters)\nWe can increment counts for one string and decrement for the other\nIf the strings are anagrams, all final counts will be zero\n\n\n\n\n\nLength comparison: O(1)\nCreating counter array: O(1) (fixed size of 26)\nCounting characters: O(n) where n is string length\nChecking final counts: O(1) (always 26 comparisons)\n\nOverall: - Time Complexity: O(n) - Space Complexity: O(1) (fixed size array)\nor alternatively:\n\nfrom collections import Counter\n\ndef isAnagram(s: str, t: str) -&gt; bool:\n    counter_s = Counter(s)\n    counter_t = Counter(t)\n    return counter_s == counter_t\n\nprint(\"Test 1:\", isAnagram(\"racecar\", \"carrace\"))\nprint(\"Test 2:\", isAnagram(\"jar\", \"jam\"))              \n\nTest 1: True\nTest 2: False\n\n\n\n\n\nThe Counter approach has some key implementation details worth understanding:\n\nCounter Creation (O(n)):\n\nEach Counter() call iterates through its input string once\nCounter uses a hash table internally for frequency counting\nEach character insertion is amortised O(1)\n\nCounter Comparison (O(n)):\n\nCounter comparison checks if all elements and their frequencies match\nMust examine each unique character at least once\n\n\nOverall:\n- Time Complexity: O(n) where n is the length of the strings\n- Space Complexity: O(k) where k is the size of the character set (26 for lowercase letters)"
  },
  {
    "objectID": "posts/2024-12-17-is-anagram.html#takeaways",
    "href": "posts/2024-12-17-is-anagram.html#takeaways",
    "title": "Anagram",
    "section": "",
    "text": "The array counter approach is most efficient for this specific problem due to the constraint of lowercase letters only.\nThe hash map approach is more flexible and can handle any character set."
  }
]