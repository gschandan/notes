[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Notes from my studies and interests, work in progress ðŸ˜„"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "notes",
    "section": "",
    "text": "Inductive Program Synthesis\n\n\n\n\n\n\n\nprogramming\n\n\ninductive-synthesis\n\n\nalgorithm\n\n\n\n\nNotes about inductive program synthesis\n\n\n\n\n\n\nApr 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Program Synthesis\n\n\n\n\n\n\n\nprogramming\n\n\nintroduction\n\n\nprogram-synthesis\n\n\n\n\nNotes about basic terms and concepts\n\n\n\n\n\n\nApr 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nEigenvectors and Eigenvalues\n\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\neigenvectors\n\n\neigenvalues\n\n\nintroduction\n\n\n\n\nEigenvectors and Eigenvalues\n\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBasic Linear Algebra Concepts\n\n\n\n\n\n\n\njupyter\n\n\nmaths\n\n\nlinear-algebra\n\n\ntensors\n\n\nmatrices\n\n\n\n\nExamining some basic concepts of liner algebra using python libraries\n\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Linear Algebra\n\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\nintroduction\n\n\n\n\nIntroduction and basic concepts of linear algebra\n\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTensors and Tensor Operations\n\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\ntensors\n\n\n\n\nTensors\n\n\n\n\n\n\nApr 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTraining, Testing, Validation Datasets and Cross-Validation\n\n\n\n\n\n\n\ndata\n\n\nML\n\n\ntraining\n\n\ntesting\n\n\nvalidation\n\n\ncross-validation\n\n\n\n\nDefining the concepts of testing, training and validation sets\n\n\n\n\n\n\nApr 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nData in ML\n\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\nstatistics\n\n\nML\n\n\n\n\nData Types, Quality & Quantity\n\n\n\n\n\n\nApr 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nFeature Scaling in ML\n\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\nstatistics\n\n\nML\n\n\nfeature-scaling\n\n\n\n\nRescaling, normalisation, standardisation\n\n\n\n\n\n\nApr 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nForms of ML\n\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\nstatistics\n\n\nML\n\n\n\n\nML types\n\n\n\n\n\n\nApr 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMaths for ML\n\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\nstatistics\n\n\ncalculus\n\n\n\n\nLinks to notes for maths for ML\n\n\n\n\n\n\nApr 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Properties and Operations\n\n\n\n\n\n\n\nmath\n\n\nlinear-algebra\n\n\nmatrix\n\n\nintroduction\n\n\n\n\nMatrix properties and Operations\n\n\n\n\n\n\nFeb 5, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-02-05-matrices.html",
    "href": "posts/2022-02-05-matrices.html",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "||\\boldsymbol{X}||{_F} = \\sqrt{\\displaystyle\\sum_{i,j} X_{i,j}{^2} }\nFor i rows, and j columns, sum the square of each element, then take the square root. This is analogous to the L^2 norm of vectors, and measures the Euclidean size of the matrix. Can also be thought of as the sum of the magnitude of all the vectors in \\boldsymbol{X}.\n\n\n\nNumber of columns in the first matrix must match the number of rows in the second, i.e.Â in the following example, m is the number of rows in the first matrix, n is the number of columns in the first matrix and the number of rows in the second, and p is the number of columns in the second matrix.\n m\\underset{n}{\\begin{bmatrix} \\, \\\\ A \\\\ \\,\\end{bmatrix}} \\quad\nn\\underset{p}{\\begin{bmatrix}& B &\\end{bmatrix}} = m\\underset{p}{\\begin{bmatrix} \\, \\\\ C \\\\ \\,\\end{bmatrix}}\n\n C_{i,k} = \\displaystyle\\sum_{j}A_{i,j}B_{j,k} \nMultiplying a matrix with a vector:\n\n\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3*1 + 4*2\\\\ 5*1+6*2 \\\\ 7*1+8*2 \\end{bmatrix} = \\begin{bmatrix} 11 \\\\ 17 \\\\ 23\\end{bmatrix}\n\nMultiplying a matrix with a matrix:\n\n\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n\\begin{bmatrix} 1 & 9\\\\ 2 & 0 \\end{bmatrix} = \\begin{bmatrix} 3*1 + 4*2 & 3*9+4*0\\\\ 5*1+6*2 & 5*9+6*0\\\\ 7*1+8*2 &7*9+8*0 \\end{bmatrix} = \\begin{bmatrix} 11 & 27\\\\ 17 & 45\\\\ 23&63\\end{bmatrix}\n\nMatrix multiplication is typically not commutative: \\boldsymbol{X}\\boldsymbol{Y} \\not= \\boldsymbol{Y}\\boldsymbol{X}\nIf both matrices are square, multiplying either way will work, but the results will usually be different.\nCan represent the following regression as a matrix multiplication:\n\n\\begin{bmatrix}\ny{_1} |\\alpha + \\beta x{_{1,1}} + \\gamma x{_{1,2}} + ... + m_{-1}x{_{1,m_{-1}}} \\\\\ny{_2} | \\alpha + \\beta x{_{2,1}} + \\gamma x{_{2,2}} + ... + m_{-1}x{_{2,m_{-1}}} \\\\\n\\vdots \\\\\ny{_n} |  \\alpha + \\beta x{_{n,1}} + \\gamma x{_{n,2}} + ... + m_{-1}x{_{n,m_{-1}}}\n\\end{bmatrix} \\\\ \\begin{bmatrix} y{_1} \\\\ y{_2} \\\\ \\vdots \\\\ y{_n}\\end{bmatrix} =\\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & \\dots & X_{1,m_{-1}}\\\\ 1 & X_{2,1} & X_{2,2} & \\dots & X_{2,m_{-1}} \\\\ \\vdots \\\\ 1 & X_{n,1} & X_{n,2} & \\dots & X_{n,m_{-1}} \\end{bmatrix}\n\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\vdots \\\\ m_{-1}\\end{bmatrix}\n\nn cases tall, m features wide.\n\n\n\n\n\nNumber of rows = number of columns i.e.Â square Transpose is equal to itself i.e.Â any value on the diagonal but the values either side must be equal.\n\n\n\nAn identity matrix is a symmetric matrix where every element along the main diagonal is 1, all others are 0. This satisfies the condition of being an identity as, if an identity matrix is multiplied by a n length vector, it will remain unchanged i.e.Â we get the same vector returned.\nNotation \\boldsymbol{I}_n where n = height or width.\n1 is the identity element for multiplication - the number which when operated on with any other number results in the other number. 0 is the identity element for addition, and 1 is the identity element for multiplication. Whenever the identity element for an operation is the solution, the two items operated on are inverses of each other e.g.Â 5x = 15, both sides can be multiplied by the multiplicative inverse i.e.Â \\frac{1}{5} or 5^{-1} to get x=3. A number multiplied by its reciprocal or inverse always returns 1 e.g.Â \\frac{1}{5} * 5 = 1. Similarly, a matrix multiplied by its inverse, will return the identity matrix.\n\n\n\n\nUsed to computationally solve linear equations. Matrix inverse is denoted \\boldsymbol{X}^{-1}. It satisfies the following property:\n\\boldsymbol{X}^{-1} \\boldsymbol{X} = \\boldsymbol{X}\\boldsymbol{X}^{-1} = \\boldsymbol{I}_n\n2x2 Matrix inversion: \n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nwhere {ad-bc} is the determinant (which cannot be 0).\nCanâ€™t divide by a matrix, but can multiply by an inverse to get the same result e.g. we know matrix \\boldsymbol{A} and \\boldsymbol{B}, and want to get the values for matrix \\boldsymbol{X}:\n\\boldsymbol{X} \\boldsymbol{A} = \\boldsymbol{B}\nWe canâ€™t divide by \\boldsymbol{A}, but can multiply both sides by the inverse of \\boldsymbol{A}:\n$ ^{-1} = ^{-1}$ and we know that \\boldsymbol{A}\\boldsymbol{A}^{-1} = \\boldsymbol{I}\nwhich gives us\n$ = ^{-1}$\nand \\boldsymbol{I} can be ignored, just like 1 would be during multiplication:\n$ = ^{-1}$\nIf a matrix has no inverse i.e.Â the determinant is zero, then it is called singular.\nMatrix inversion can also only be calculated if the matrix is square i.e.Â the vector span (number of rows) is equal to the matrix range (number of cols). This avoids overdetermination (rows &gt; cols) i.e.Â number of equations &gt; number of dimensions (multiple points of overlap), and underdetermination (rows &lt; cols) i.e.Â number of equations are less than dimensions (no overlap as single line) - may still be able to solve these, but canâ€™t invert a matrix as a solution.\n\n\n\n\\begin{bmatrix} y{_1} \\\\ y{_2} \\\\ \\vdots \\\\ y{_n}\\end{bmatrix} =\\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & \\dots & X_{1,m_{-1}}\\\\ 1 & X_{2,1} & X_{2,2} & \\dots & X_{2,m_{-1}} \\\\ \\vdots \\\\ 1 & X_{n,1} & X_{n,2} & \\dots & X_{n,m_{-1}} \\end{bmatrix}\n\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\vdots \\\\ m_{-1}\\end{bmatrix}\n Would like to solve for the vector of variables on the right, in this regression model. This can be represented as: \\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{w} where \\boldsymbol{w} is the vector of weights, \\alpha to m_{-1}, corresponding to the number of columns in our feature matrix.\nIn this equation, \\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{w}, we know the outcomes \\boldsymbol{y}, and we know the features \\boldsymbol{X}, and we wish to work out the weights, \\boldsymbol{w}, which we can do using matrix inversion, assuming the inverse matrix exists i.e.Â it isnâ€™t singular - all columns must be linearly independent e.g.Â one column should not be a multiple of another one like [1,2] - another canâ€™t be [2,4] or [1,2] again as we would have paralell lines or overlapping lines respectively.\nWe can multiply both sides by the inverse of \\boldsymbol{X}, \\boldsymbol{X}^{-1} to obtain:\n$^{-1} = ^{-1} $ and given \\boldsymbol{X}^{-1}\\boldsymbol{X} = \\boldsymbol{I}, we can simplify this to:\n$ = ^{-1} $ and given the property of the identity matrix, we can remove it to leave:\n$ = ^{-1} $\nThe order of operations is key to preserve, as matrix multiplication is not always commutative i.e.Â $ = $\n\n\n\nNon-zero elements along the main diagonal, zero elsewhere e.g.Â identity matrix. If it is square, denoted as diag(\\boldsymbol{x}) where \\boldsymbol{x} is a vector of main diagonal elements e.g.Â [1,1,1] to represent \\boldsymbol{I}_3. Much more computationally efficient: - Multiplication: diag(\\boldsymbol{x})\\boldsymbol{y} = \\boldsymbol{x}\\odot{}\\boldsymbol{y} - Inversion: $diag()^{-1} = diag(,,)^T $ - Canâ€™t divide by zero, so if any elemenet along the diagonal contains zero, the diagonal canâ€™t be inverted - Can be non-square and computation can still be efficient e.g.Â by adding zeros to the product if height &gt; width, or by removing elements from the product if the width &gt; height\n\n\n\nIn orthogonal matrices, orthonormal vectors make up all rows and columns. This means that \\boldsymbol{A}^{T} \\boldsymbol{A} = \\boldsymbol{A}\\boldsymbol{A}^{T} = \\boldsymbol{I} and also that \\boldsymbol{A}^{T} = \\boldsymbol{A}^{-1}\\boldsymbol{I} = \\boldsymbol{A}^{-1}. Calculating \\boldsymbol{A}^{T} is cheap, and therefore so is \\boldsymbol{A}^{-1} for orthogonal matrices."
  },
  {
    "objectID": "posts/2022-02-05-matrices.html#norms---frobenius-norm",
    "href": "posts/2022-02-05-matrices.html#norms---frobenius-norm",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "||\\boldsymbol{X}||{_F} = \\sqrt{\\displaystyle\\sum_{i,j} X_{i,j}{^2} }\nFor i rows, and j columns, sum the square of each element, then take the square root. This is analogous to the L^2 norm of vectors, and measures the Euclidean size of the matrix. Can also be thought of as the sum of the magnitude of all the vectors in \\boldsymbol{X}."
  },
  {
    "objectID": "posts/2022-02-05-matrices.html#multiplication",
    "href": "posts/2022-02-05-matrices.html#multiplication",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "Number of columns in the first matrix must match the number of rows in the second, i.e.Â in the following example, m is the number of rows in the first matrix, n is the number of columns in the first matrix and the number of rows in the second, and p is the number of columns in the second matrix.\n m\\underset{n}{\\begin{bmatrix} \\, \\\\ A \\\\ \\,\\end{bmatrix}} \\quad\nn\\underset{p}{\\begin{bmatrix}& B &\\end{bmatrix}} = m\\underset{p}{\\begin{bmatrix} \\, \\\\ C \\\\ \\,\\end{bmatrix}}\n\n C_{i,k} = \\displaystyle\\sum_{j}A_{i,j}B_{j,k} \nMultiplying a matrix with a vector:\n\n\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3*1 + 4*2\\\\ 5*1+6*2 \\\\ 7*1+8*2 \\end{bmatrix} = \\begin{bmatrix} 11 \\\\ 17 \\\\ 23\\end{bmatrix}\n\nMultiplying a matrix with a matrix:\n\n\\begin{bmatrix} 3 & 4 \\\\ 5 & 6 \\\\ 7 & 8 \\end{bmatrix}\n\\begin{bmatrix} 1 & 9\\\\ 2 & 0 \\end{bmatrix} = \\begin{bmatrix} 3*1 + 4*2 & 3*9+4*0\\\\ 5*1+6*2 & 5*9+6*0\\\\ 7*1+8*2 &7*9+8*0 \\end{bmatrix} = \\begin{bmatrix} 11 & 27\\\\ 17 & 45\\\\ 23&63\\end{bmatrix}\n\nMatrix multiplication is typically not commutative: \\boldsymbol{X}\\boldsymbol{Y} \\not= \\boldsymbol{Y}\\boldsymbol{X}\nIf both matrices are square, multiplying either way will work, but the results will usually be different.\nCan represent the following regression as a matrix multiplication:\n\n\\begin{bmatrix}\ny{_1} |\\alpha + \\beta x{_{1,1}} + \\gamma x{_{1,2}} + ... + m_{-1}x{_{1,m_{-1}}} \\\\\ny{_2} | \\alpha + \\beta x{_{2,1}} + \\gamma x{_{2,2}} + ... + m_{-1}x{_{2,m_{-1}}} \\\\\n\\vdots \\\\\ny{_n} |  \\alpha + \\beta x{_{n,1}} + \\gamma x{_{n,2}} + ... + m_{-1}x{_{n,m_{-1}}}\n\\end{bmatrix} \\\\ \\begin{bmatrix} y{_1} \\\\ y{_2} \\\\ \\vdots \\\\ y{_n}\\end{bmatrix} =\\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & \\dots & X_{1,m_{-1}}\\\\ 1 & X_{2,1} & X_{2,2} & \\dots & X_{2,m_{-1}} \\\\ \\vdots \\\\ 1 & X_{n,1} & X_{n,2} & \\dots & X_{n,m_{-1}} \\end{bmatrix}\n\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\vdots \\\\ m_{-1}\\end{bmatrix}\n\nn cases tall, m features wide."
  },
  {
    "objectID": "posts/2022-02-05-matrices.html#symmetric-identity-matrices",
    "href": "posts/2022-02-05-matrices.html#symmetric-identity-matrices",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "Number of rows = number of columns i.e.Â square Transpose is equal to itself i.e.Â any value on the diagonal but the values either side must be equal.\n\n\n\nAn identity matrix is a symmetric matrix where every element along the main diagonal is 1, all others are 0. This satisfies the condition of being an identity as, if an identity matrix is multiplied by a n length vector, it will remain unchanged i.e.Â we get the same vector returned.\nNotation \\boldsymbol{I}_n where n = height or width.\n1 is the identity element for multiplication - the number which when operated on with any other number results in the other number. 0 is the identity element for addition, and 1 is the identity element for multiplication. Whenever the identity element for an operation is the solution, the two items operated on are inverses of each other e.g.Â 5x = 15, both sides can be multiplied by the multiplicative inverse i.e.Â \\frac{1}{5} or 5^{-1} to get x=3. A number multiplied by its reciprocal or inverse always returns 1 e.g.Â \\frac{1}{5} * 5 = 1. Similarly, a matrix multiplied by its inverse, will return the identity matrix."
  },
  {
    "objectID": "posts/2022-02-05-matrices.html#matrix-inversion",
    "href": "posts/2022-02-05-matrices.html#matrix-inversion",
    "title": "Matrix Properties and Operations",
    "section": "",
    "text": "Used to computationally solve linear equations. Matrix inverse is denoted \\boldsymbol{X}^{-1}. It satisfies the following property:\n\\boldsymbol{X}^{-1} \\boldsymbol{X} = \\boldsymbol{X}\\boldsymbol{X}^{-1} = \\boldsymbol{I}_n\n2x2 Matrix inversion: \n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n\nwhere {ad-bc} is the determinant (which cannot be 0).\nCanâ€™t divide by a matrix, but can multiply by an inverse to get the same result e.g. we know matrix \\boldsymbol{A} and \\boldsymbol{B}, and want to get the values for matrix \\boldsymbol{X}:\n\\boldsymbol{X} \\boldsymbol{A} = \\boldsymbol{B}\nWe canâ€™t divide by \\boldsymbol{A}, but can multiply both sides by the inverse of \\boldsymbol{A}:\n$ ^{-1} = ^{-1}$ and we know that \\boldsymbol{A}\\boldsymbol{A}^{-1} = \\boldsymbol{I}\nwhich gives us\n$ = ^{-1}$\nand \\boldsymbol{I} can be ignored, just like 1 would be during multiplication:\n$ = ^{-1}$\nIf a matrix has no inverse i.e.Â the determinant is zero, then it is called singular.\nMatrix inversion can also only be calculated if the matrix is square i.e.Â the vector span (number of rows) is equal to the matrix range (number of cols). This avoids overdetermination (rows &gt; cols) i.e.Â number of equations &gt; number of dimensions (multiple points of overlap), and underdetermination (rows &lt; cols) i.e.Â number of equations are less than dimensions (no overlap as single line) - may still be able to solve these, but canâ€™t invert a matrix as a solution.\n\n\n\n\\begin{bmatrix} y{_1} \\\\ y{_2} \\\\ \\vdots \\\\ y{_n}\\end{bmatrix} =\\begin{bmatrix} 1 & X_{1,1} & X_{1,2} & \\dots & X_{1,m_{-1}}\\\\ 1 & X_{2,1} & X_{2,2} & \\dots & X_{2,m_{-1}} \\\\ \\vdots \\\\ 1 & X_{n,1} & X_{n,2} & \\dots & X_{n,m_{-1}} \\end{bmatrix}\n\\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\vdots \\\\ m_{-1}\\end{bmatrix}\n Would like to solve for the vector of variables on the right, in this regression model. This can be represented as: \\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{w} where \\boldsymbol{w} is the vector of weights, \\alpha to m_{-1}, corresponding to the number of columns in our feature matrix.\nIn this equation, \\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{w}, we know the outcomes \\boldsymbol{y}, and we know the features \\boldsymbol{X}, and we wish to work out the weights, \\boldsymbol{w}, which we can do using matrix inversion, assuming the inverse matrix exists i.e.Â it isnâ€™t singular - all columns must be linearly independent e.g.Â one column should not be a multiple of another one like [1,2] - another canâ€™t be [2,4] or [1,2] again as we would have paralell lines or overlapping lines respectively.\nWe can multiply both sides by the inverse of \\boldsymbol{X}, \\boldsymbol{X}^{-1} to obtain:\n$^{-1} = ^{-1} $ and given \\boldsymbol{X}^{-1}\\boldsymbol{X} = \\boldsymbol{I}, we can simplify this to:\n$ = ^{-1} $ and given the property of the identity matrix, we can remove it to leave:\n$ = ^{-1} $\nThe order of operations is key to preserve, as matrix multiplication is not always commutative i.e.Â $ = $\n\n\n\nNon-zero elements along the main diagonal, zero elsewhere e.g.Â identity matrix. If it is square, denoted as diag(\\boldsymbol{x}) where \\boldsymbol{x} is a vector of main diagonal elements e.g.Â [1,1,1] to represent \\boldsymbol{I}_3. Much more computationally efficient: - Multiplication: diag(\\boldsymbol{x})\\boldsymbol{y} = \\boldsymbol{x}\\odot{}\\boldsymbol{y} - Inversion: $diag()^{-1} = diag(,,)^T $ - Canâ€™t divide by zero, so if any elemenet along the diagonal contains zero, the diagonal canâ€™t be inverted - Can be non-square and computation can still be efficient e.g.Â by adding zeros to the product if height &gt; width, or by removing elements from the product if the width &gt; height\n\n\n\nIn orthogonal matrices, orthonormal vectors make up all rows and columns. This means that \\boldsymbol{A}^{T} \\boldsymbol{A} = \\boldsymbol{A}\\boldsymbol{A}^{T} = \\boldsymbol{I} and also that \\boldsymbol{A}^{T} = \\boldsymbol{A}^{-1}\\boldsymbol{I} = \\boldsymbol{A}^{-1}. Calculating \\boldsymbol{A}^{T} is cheap, and therefore so is \\boldsymbol{A}^{-1} for orthogonal matrices."
  },
  {
    "objectID": "posts/2022-04-18-maths-for-ml.html",
    "href": "posts/2022-04-18-maths-for-ml.html",
    "title": "Maths for ML",
    "section": "",
    "text": "Links to my notes on learning maths for ML.\n\n\n\n\n\nIntroduction & Notation\nMatrices\nTensors\nEigenvectors\n\nCh 1-7 Strang 1 Ch 2 Goodfellow 2\n\n\n\nCh 3 Goodfellow 3 Ch 1 Barber 4\n\n\n\nAppendix 2 Barber 5"
  },
  {
    "objectID": "posts/2022-04-18-maths-for-ml.html#topics",
    "href": "posts/2022-04-18-maths-for-ml.html#topics",
    "title": "Maths for ML",
    "section": "",
    "text": "Introduction & Notation\nMatrices\nTensors\nEigenvectors\n\nCh 1-7 Strang 1 Ch 2 Goodfellow 2\n\n\n\nCh 3 Goodfellow 3 Ch 1 Barber 4\n\n\n\nAppendix 2 Barber 5"
  },
  {
    "objectID": "posts/2022-04-18-maths-for-ml.html#footnotes",
    "href": "posts/2022-04-18-maths-for-ml.html#footnotes",
    "title": "Maths for ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilbert Strang. Introduction to linear algebra. Cambridge Press, Wellesley, MA, 2016.â†©ï¸Ž\nIan Goodfellow et Al. Deep learning. MIT press, 2016.â†©ï¸Ž\nIan Goodfellow et Al. Deep learning. MIT press, 2016.â†©ï¸Ž\nDavid Barber. Bayesian reasoning and machine learning. Cambridge University Press, 2012.â†©ï¸Ž\nDavid Barber. Bayesian reasoning and machine learning. Cambridge University Press, 2012.â†©ï¸Ž"
  },
  {
    "objectID": "posts/2022-04-19-data-types.html",
    "href": "posts/2022-04-19-data-types.html",
    "title": "Data in ML",
    "section": "",
    "text": "Common taxonomy of data types when considering machine learning\n\nCategorical: qualitative\n\nOrdinal: innate ordered values with unknown distances between them that cannot be measured\n\ne.g.Â first/second/third, good/bad\n\nNominal: values (text or numbers) with no order\n\ne.g.Â cat/dog, genre, ethnicity\n\n\nNumerical: quantitative\n\nDiscrete: quantitative whole number values\n\ne.g.Â step count\n\nContinuous: quantitative decimal values\n\ne.g.Â width, height"
  },
  {
    "objectID": "posts/2022-04-19-data-types.html#footnotes",
    "href": "posts/2022-04-19-data-types.html#footnotes",
    "title": "Data in ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://economistjourney.blogspot.com/2018/06/what-is-sampling-noise.html.â†©ï¸Ž"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html",
    "href": "posts/2022-04-19-feature-scaling.html",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "For classifiers that calculate distances between points, e.g.Â Euclidean kNN or SVM, one feature can dominate the others purely due to scale when proportional contributions of all the features are wanted. Feature scaling (particularly standardisation) can also increase the convergence speed of gradient descent algorithms by altering the step size for each feature. Tree based algorithms and others (e.g.Â NB, LDA) are insensitive to the scale of the features as the decision to split at that node is not influenced by other features, rather the decision to split is usually based on what will decrease the variance or increase the homogeneity in the sub-nodes.\nGenerally, normalisation is applied when the distribution of the data is known to not be gaussian, e.g.Â for algorithms that do not presume a distribution e.g.Â kNN. It is important to consider outliers as normalising data with outliers will scale potentially important features into a small interval, so they must be handled first.\n\n\n\nPrevents the scale of the variable from over-influencing the model due to unit difference\nScale features or variables to a common scale without distortion of differences in the range\n\ne.g.Â [0,1]\n\nExamples\n\nRescaling (min-max normalisation) to a range of [0,1] or [-1,-1] or [a,b]\n\nx'= \\frac {x-min(x)} {max(x)-min(x)}\nx'= \\frac {(x-min(x))(b-a)} {max(x)-min(x)}\n\nMean normalisation\n\nx'= \\frac {x-average(x)} {max(x)-min(x)}\n\n\n\n\n\n\n\nStandardise to find out how many standard deviations the values have from the mean\nStandardisation will scale the data to have a mean of 0 and standard deviation of 1\nNo specific upper or lower bound of values\nMethods:\n\nZ-Score: for population mean and std.dev.\n\n\\LARGE z = \\frac{x - \\mu}{\\sigma}\n\nZ-Score (estimated): for sample mean and std.dev.(\\large S)\n\n\\LARGE z = \\frac{x - \\bar x}{S}\n\n\nUsages:\n\nStandardising regression coefficients\nStandardising variables prior to PCA\n\n\n\n\n\n\nScale the components of a feature vector such that the overall vector has length 1\ne.g.Â dividing by the Euclidean length of the vector\n\\large x'= \\frac {x} {||x||}\n\n\n\n\n\nWikipedia - Feature Scaling\nKDNuggets - Feature Scaling"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html#normalisation",
    "href": "posts/2022-04-19-feature-scaling.html#normalisation",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "Prevents the scale of the variable from over-influencing the model due to unit difference\nScale features or variables to a common scale without distortion of differences in the range\n\ne.g.Â [0,1]\n\nExamples\n\nRescaling (min-max normalisation) to a range of [0,1] or [-1,-1] or [a,b]\n\nx'= \\frac {x-min(x)} {max(x)-min(x)}\nx'= \\frac {(x-min(x))(b-a)} {max(x)-min(x)}\n\nMean normalisation\n\nx'= \\frac {x-average(x)} {max(x)-min(x)}"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html#standardisation",
    "href": "posts/2022-04-19-feature-scaling.html#standardisation",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "Standardise to find out how many standard deviations the values have from the mean\nStandardisation will scale the data to have a mean of 0 and standard deviation of 1\nNo specific upper or lower bound of values\nMethods:\n\nZ-Score: for population mean and std.dev.\n\n\\LARGE z = \\frac{x - \\mu}{\\sigma}\n\nZ-Score (estimated): for sample mean and std.dev.(\\large S)\n\n\\LARGE z = \\frac{x - \\bar x}{S}\n\n\nUsages:\n\nStandardising regression coefficients\nStandardising variables prior to PCA"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html#scaling-to-unit-length",
    "href": "posts/2022-04-19-feature-scaling.html#scaling-to-unit-length",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "Scale the components of a feature vector such that the overall vector has length 1\ne.g.Â dividing by the Euclidean length of the vector\n\\large x'= \\frac {x} {||x||}"
  },
  {
    "objectID": "posts/2022-04-19-feature-scaling.html#references",
    "href": "posts/2022-04-19-feature-scaling.html#references",
    "title": "Feature Scaling in ML",
    "section": "",
    "text": "Wikipedia - Feature Scaling\nKDNuggets - Feature Scaling"
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html",
    "href": "posts/2022-04-19-ml-forms.html",
    "title": "Forms of ML",
    "section": "",
    "text": "Here are a few ways commonly used to classify ML systems:\n\nsupervision based\nincremental learning based\ngeneralisability\n\n\n\nIn this classification, there are four main classes based on the amount or type of supervision\n\n\n\nData\n\nLabelled examples \\large{({\\bold x_i},y_i)_{i=1}^N}\n\\large x_i is a feature vector(\\large n-dimensional vector of numerical features \\large x^d)\n\nrepresent objects numerically e.g.Â for an image, \\large x^{(1)} could be the hue, and \\large x^{(2)} could be the intensity\nuseful for comparing objects e.g.Â euclidean distance\ngranularity depends on the purpose\n\n\\large y_i can take the form of a member of a set, real number, matrix, vector etc.\n\nTasks\n\nClassification e.g.Â spam filter\nPredict numeric values based on predictors (features) e.g.Â house price given room numbers and areas\n\nGoal\n\nTrain a model on a dataset to predict labels based on input feature vectors\n\nMethods\n\nLinear regression\nLogistic regression\nkNN\nSVM\nDT (& random forests)\nNeural networks\n\n\n\n\n\n\nData\n\nUnlabelled examples \\large{({\\bold x_i})_{i=1}^N}\n\\large x_i is a feature vector\n\nTasks\n\nClustering - group data points with shared attributes to extrapolate a relationship e.g.Â molecular structure similarity\nAnomaly/Outlier detection - find outliers e.g.Â fraud-detection\nRule learning\n\nGoal\n\nClustering - transform feature vector \\large \\bold x into a useful value e.g.Â an id\nDimensionality reduction - output feature vector should have fewer features than \\large \\bold x\nAnomaly/Outlier detection - output value quantifies the difference of \\large \\bold x from the data\n\nMethods\n\nClustering\n\nExclusive\n\nK-means\nHierarchical\n\nSoft - more probabilistic\n\nGMM\nExpectation Maximisation\n\n\nAssociation\n\nApriori\nEclat\n\nDimensionality reduction\n\nPCA\nSVD\nLLE\nt-SNE\n\n\nChallenges\n\nComputation and time complexity of training\nCan be unclear as to basis for clustering\nAccuracy\n\n\n\n\n\n\nData usually partially labelled\nCombination of supervised and unsupervised\nMethods\n\nDeep Belief Networks (DBN) - based on stacked Restricted Boltzmann machines\n\n\n\n\n\n\nAgent - learning system\n\nobserves the environment - state\nmakes decisions\nperforms actions\nfeedback loop - penalties or rewards - aims to maximise rewards\n\nLearns a policy - a function that takes the state as an input feature vector and outputs an action that leads to the highest expected average reward\n\n\n\n\n\nSplit into batch (non-incremental) and online\n\n\n\nOffline - unable to learn incrementally from a data stream (usually due to complexity)\nSystem trained first then applied, without learning further unless it is taken offline and retrained\nCan be automated e.g.Â weekly\nHigh computational requirements\n\n\n\n\n\nTraining using small sequential chunks of data - streamed\nDoes not require storage of previous data\nLearning steps are of low complexity therefore can be performed on mobile systems\nCan also be used to process extremely large datasets as a stream\nLearning rate\n\nif high, will forget old data faster, sensitive to noise\nif low, inertia will be high, slower to learn, less sensitive to noise or outliers\n\nCorrupted or errors in the stream can affect the performance in real-time\n\npair with anomaly detection\n\n\n\n\n\n\nCan also categorise based on models of generalisation\n\n\n\nLearn from prior examples, then generalise to new data using a measure of similarity\n\n\n\n\n\nBuild or select a model from prior examples\nDefine a fitness function or cost function\nMinimise cost or maximise fitness, depending on the chosen model\nTrain the model on the training data to optimise parameters for a reasonable fit\nMake predictions\n\n\n\n\n\n\nBurkov, A. (2019) The Hundred-Page Machine Learning Book.\nGeron, A. (2017) Hands-On Machine Learning with Scikit-Learn & TensorFlow : concepts, tools, and techniques to build intelligent systems."
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html#supervision",
    "href": "posts/2022-04-19-ml-forms.html#supervision",
    "title": "Forms of ML",
    "section": "",
    "text": "In this classification, there are four main classes based on the amount or type of supervision\n\n\n\nData\n\nLabelled examples \\large{({\\bold x_i},y_i)_{i=1}^N}\n\\large x_i is a feature vector(\\large n-dimensional vector of numerical features \\large x^d)\n\nrepresent objects numerically e.g.Â for an image, \\large x^{(1)} could be the hue, and \\large x^{(2)} could be the intensity\nuseful for comparing objects e.g.Â euclidean distance\ngranularity depends on the purpose\n\n\\large y_i can take the form of a member of a set, real number, matrix, vector etc.\n\nTasks\n\nClassification e.g.Â spam filter\nPredict numeric values based on predictors (features) e.g.Â house price given room numbers and areas\n\nGoal\n\nTrain a model on a dataset to predict labels based on input feature vectors\n\nMethods\n\nLinear regression\nLogistic regression\nkNN\nSVM\nDT (& random forests)\nNeural networks\n\n\n\n\n\n\nData\n\nUnlabelled examples \\large{({\\bold x_i})_{i=1}^N}\n\\large x_i is a feature vector\n\nTasks\n\nClustering - group data points with shared attributes to extrapolate a relationship e.g.Â molecular structure similarity\nAnomaly/Outlier detection - find outliers e.g.Â fraud-detection\nRule learning\n\nGoal\n\nClustering - transform feature vector \\large \\bold x into a useful value e.g.Â an id\nDimensionality reduction - output feature vector should have fewer features than \\large \\bold x\nAnomaly/Outlier detection - output value quantifies the difference of \\large \\bold x from the data\n\nMethods\n\nClustering\n\nExclusive\n\nK-means\nHierarchical\n\nSoft - more probabilistic\n\nGMM\nExpectation Maximisation\n\n\nAssociation\n\nApriori\nEclat\n\nDimensionality reduction\n\nPCA\nSVD\nLLE\nt-SNE\n\n\nChallenges\n\nComputation and time complexity of training\nCan be unclear as to basis for clustering\nAccuracy\n\n\n\n\n\n\nData usually partially labelled\nCombination of supervised and unsupervised\nMethods\n\nDeep Belief Networks (DBN) - based on stacked Restricted Boltzmann machines\n\n\n\n\n\n\nAgent - learning system\n\nobserves the environment - state\nmakes decisions\nperforms actions\nfeedback loop - penalties or rewards - aims to maximise rewards\n\nLearns a policy - a function that takes the state as an input feature vector and outputs an action that leads to the highest expected average reward"
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html#incremental",
    "href": "posts/2022-04-19-ml-forms.html#incremental",
    "title": "Forms of ML",
    "section": "",
    "text": "Split into batch (non-incremental) and online\n\n\n\nOffline - unable to learn incrementally from a data stream (usually due to complexity)\nSystem trained first then applied, without learning further unless it is taken offline and retrained\nCan be automated e.g.Â weekly\nHigh computational requirements\n\n\n\n\n\nTraining using small sequential chunks of data - streamed\nDoes not require storage of previous data\nLearning steps are of low complexity therefore can be performed on mobile systems\nCan also be used to process extremely large datasets as a stream\nLearning rate\n\nif high, will forget old data faster, sensitive to noise\nif low, inertia will be high, slower to learn, less sensitive to noise or outliers\n\nCorrupted or errors in the stream can affect the performance in real-time\n\npair with anomaly detection"
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html#generalisability",
    "href": "posts/2022-04-19-ml-forms.html#generalisability",
    "title": "Forms of ML",
    "section": "",
    "text": "Can also categorise based on models of generalisation\n\n\n\nLearn from prior examples, then generalise to new data using a measure of similarity\n\n\n\n\n\nBuild or select a model from prior examples\nDefine a fitness function or cost function\nMinimise cost or maximise fitness, depending on the chosen model\nTrain the model on the training data to optimise parameters for a reasonable fit\nMake predictions"
  },
  {
    "objectID": "posts/2022-04-19-ml-forms.html#references",
    "href": "posts/2022-04-19-ml-forms.html#references",
    "title": "Forms of ML",
    "section": "",
    "text": "Burkov, A. (2019) The Hundred-Page Machine Learning Book.\nGeron, A. (2017) Hands-On Machine Learning with Scikit-Learn & TensorFlow : concepts, tools, and techniques to build intelligent systems."
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html",
    "href": "posts/2022-04-27-testing-validation.html",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "To build a mathematical model to make predictions, data is needed to train and test the model, for which three data sets are usually used. Typically, the data is split into a training set and testing set (80-20 or 70-30 etc.), or a training, testing and validation set. Cross-Validation can be particularly helpful for smaller datasets, helping to avoid over-fitting and hyperparameter tuning.\n\n\nDataset used to create a model to be used for predictions. The model can then be applied to the training data sets to assess the accuracy of predictions against labelled data. The model can then be tuned if necessary.\n\n\n\nThe model can then be applied to a testing set, data which has not been used to make the model, from which an error rate can be estimated (generalisation error).\nIf the training set error rate is low and the testing set error rate is high, it suggests the model has been over-fitted to the training data set.\n\n\n\nAnother holdout dataset can be used is the validation set. It provides an unbiased estimation of the performance of the model which can be useful when assessing multiple models to minimise over-fitting. Multiple models can be trained with different hyperparameters, and evaluated against the validation set, with the best performing hyperparameters and model being used on the test dataset to estimate the generalisation error. They can also be used to stop training early on a model where the error on the validation set increases beyond a specified limit.\n\n\n\nIf the dataset size is relatively smaller to begin with, to avoid issues with trying to train a model on less data, cross-validation can be used. The training dataset can be split up into subsets or subsamples. Each model can be trained against combinations of these and validated against the remaining ones. Then the final model can be trained, using the best performing hyperparameters, on the complete training set, so all observations have been used.\n\n\n\nK-folding\n\nSplit the data int k-subsets e.g.Â 3,5, 10 etc.\nBuild k models, each trained on k-1 and tested on the datasets they havenâ€™t been trained on\n\nLeave p out\n\nUse p observations as the validation set, and the remaining as the training set\nThis is repeated for as many variations of sampling p elements out of the total observations\nif p&gt;1, this will mean training and validating C{n \\choose p} times, where n is the number of observations in the original sample\nif p=1, as in leave-one-out, this will mean training and validating C{n \\choose 1} times. If n is large, k-fold may be less computationally expensive.\n\nStratified k-fold\n\nEnsure that each fold is a representative sample of the whole population of observations e.g.Â mean is similar in all the subsets or proportions for binary classifiers\n\nRepeated random sub-sampling\n\nMonte Carlo cross validation - create multiple random training and validation subsets, then for each split, fit the model to the training subset and assess the accuracy on the validation subset. Then average the results over the subsets.\nValidation subsets may overlap, or some data points may never be used due to the random sampling.\nCan also be stratified by the mean of the subsets\n\n\n\n\n\n\n\nAny two optimisation algorithms are equivalent when their performance is averaged across all possible problems, if no assumptions are made about the data."
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#training-set",
    "href": "posts/2022-04-27-testing-validation.html#training-set",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "Dataset used to create a model to be used for predictions. The model can then be applied to the training data sets to assess the accuracy of predictions against labelled data. The model can then be tuned if necessary."
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#testing-set",
    "href": "posts/2022-04-27-testing-validation.html#testing-set",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "The model can then be applied to a testing set, data which has not been used to make the model, from which an error rate can be estimated (generalisation error).\nIf the training set error rate is low and the testing set error rate is high, it suggests the model has been over-fitted to the training data set."
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#validation-set",
    "href": "posts/2022-04-27-testing-validation.html#validation-set",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "Another holdout dataset can be used is the validation set. It provides an unbiased estimation of the performance of the model which can be useful when assessing multiple models to minimise over-fitting. Multiple models can be trained with different hyperparameters, and evaluated against the validation set, with the best performing hyperparameters and model being used on the test dataset to estimate the generalisation error. They can also be used to stop training early on a model where the error on the validation set increases beyond a specified limit."
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#cross-validation",
    "href": "posts/2022-04-27-testing-validation.html#cross-validation",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "If the dataset size is relatively smaller to begin with, to avoid issues with trying to train a model on less data, cross-validation can be used. The training dataset can be split up into subsets or subsamples. Each model can be trained against combinations of these and validated against the remaining ones. Then the final model can be trained, using the best performing hyperparameters, on the complete training set, so all observations have been used.\n\n\n\nK-folding\n\nSplit the data int k-subsets e.g.Â 3,5, 10 etc.\nBuild k models, each trained on k-1 and tested on the datasets they havenâ€™t been trained on\n\nLeave p out\n\nUse p observations as the validation set, and the remaining as the training set\nThis is repeated for as many variations of sampling p elements out of the total observations\nif p&gt;1, this will mean training and validating C{n \\choose p} times, where n is the number of observations in the original sample\nif p=1, as in leave-one-out, this will mean training and validating C{n \\choose 1} times. If n is large, k-fold may be less computationally expensive.\n\nStratified k-fold\n\nEnsure that each fold is a representative sample of the whole population of observations e.g.Â mean is similar in all the subsets or proportions for binary classifiers\n\nRepeated random sub-sampling\n\nMonte Carlo cross validation - create multiple random training and validation subsets, then for each split, fit the model to the training subset and assess the accuracy on the validation subset. Then average the results over the subsets.\nValidation subsets may overlap, or some data points may never be used due to the random sampling.\nCan also be stratified by the mean of the subsets"
  },
  {
    "objectID": "posts/2022-04-27-testing-validation.html#no-free-lunch-theorem1",
    "href": "posts/2022-04-27-testing-validation.html#no-free-lunch-theorem1",
    "title": "Training, Testing, Validation Datasets and Cross-Validation",
    "section": "",
    "text": "Any two optimisation algorithms are equivalent when their performance is averaged across all possible problems, if no assumptions are made about the data."
  },
  {
    "objectID": "posts/2022-04-30-eigenvectors.html",
    "href": "posts/2022-04-30-eigenvectors.html",
    "title": "Eigenvectors and Eigenvalues",
    "section": "",
    "text": "Ch 1-7 Strang 1 Ch 2 Goodfellow 2"
  },
  {
    "objectID": "posts/2022-04-30-eigenvectors.html#footnotes",
    "href": "posts/2022-04-30-eigenvectors.html#footnotes",
    "title": "Eigenvectors and Eigenvalues",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGilbert Strang. Introduction to linear algebra. Cambridge Press, Wellesley, MA, 2016.â†©ï¸Ž\nIan Goodfellow et Al. Deep learning. MIT press, 2016.â†©ï¸Ž"
  },
  {
    "objectID": "posts/2022-04-30-intro-to-linear-algebra.html",
    "href": "posts/2022-04-30-intro-to-linear-algebra.html",
    "title": "Basic Linear Algebra Concepts",
    "section": "",
    "text": "Intro to Linear Algebra\nReferences: 1. https://github.com/jonkrohn/ML-foundations\n\nimport numpy as np\nimport math\nimport torch\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\nScalar Tensors\nPytorch tensors can be used for operations on the GPU or CPU, unlike numpy arrays, by casting the tensor to a CUDA data type.\nTensorflow tensors are also immutable, and created using wrappers.\n\n# Example scalar pytorch tensor\nx = torch.tensor(10, dtype=torch.int64)\nprint(f\"Type: {x.dtype}, Shape: {x.shape}\")\n\nType: torch.int64, Shape: torch.Size([])\n\n\n\n# Example scalar tensorflow tensor\nx = tf.constant(10, dtype=tf.int64)\nx\n\n2022-05-02 12:45:13.935875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-02 12:45:13.978973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-02 12:45:13.979100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-02 12:45:13.979916: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-05-02 12:45:13.980452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-02 12:45:13.980572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-02 12:45:13.980671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-02 12:45:14.569116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-02 12:45:14.569252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-02 12:45:14.569354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-02 12:45:14.569643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6540 MB memory:  -&gt; device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n\n&lt;tf.Tensor: shape=(), dtype=int64, numpy=10&gt;\n\n\n\n\nVector Transposition\n\nx = np.array([[5,6,7]]) #Need extra brackets else shape will be (,3) (1D, not 2D)\nprint(f\"x = {x} \\nShape = {x.shape}\")\n\nx = [[5 6 7]] \nShape = (1, 3)\n\n\n\nx_t = x.T\nprint(f\"x transposed:\\n{x_t} \\nShape = {x_t.shape}\")\n\nx transposed:\n[[5]\n [6]\n [7]] \nShape = (3, 1)\n\n\n\nx = torch.tensor([5,6,7])\nprint(f\"x = {x} \\nShape = {x.shape}\")\n\nx = tensor([5, 6, 7]) \nShape = torch.Size([3])\n\n\n\n#L2 Norm - euclidean distance\nx = np.array([[5,6,7]])\nnp.linalg.norm(x)\n\n10.488088481701515\n\n\n\n# L1 Norm\nnp.sum(np.absolute(x))\n\n18\n\n\n\n# Squared L2 Norm\nx = np.array([5,6,7])\nnp.dot(x,x)\n\n110\n\n\n\n# Max norm\nnp.max(np.abs(x))\n\n7\n\n\n\n#Orthogonal\ni = np.asarray([1,0])\nj = np.asarray([0,1])\nnp.dot(i,j)\n\n0\n\n\n\n\nMatrices (2-Tensor)\n\nX = np.array([[5,3],[6,8],[2,10]])\nprint(f\"{X}\\n Shape: {X.shape} Size: {X.size}\")\n\n[[ 5  3]\n [ 6  8]\n [ 2 10]]\n Shape: (3, 2) Size: 6\n\n\n\nX[:,0] # left column\n\narray([5, 6, 2])\n\n\n\nX[1,:] # middle row\n\narray([6, 8])\n\n\n\n#pytorch\nX = torch.tensor([[5,3],[6,8],[2,10]])\nprint(f\"{X}\\n Shape: {X.shape}\")\n\ntensor([[ 5,  3],\n        [ 6,  8],\n        [ 2, 10]])\n Shape: torch.Size([3, 2])\n\n\n\n#Tensorflow\nX = tf.constant([[5,3],[6,8],[2,10]])\nprint(f\"Rank: {tf.rank(X)} Shape: {tf.shape(X)}\")\n\nRank: 2 Shape: [3 2]\n\n\n\n\nn-Tensors\n\nX = torch.zeros([5,2,4,2])\nX.shape\n\ntorch.Size([5, 2, 4, 2])\n\n\n\nX = tf.zeros([5,2,4,2])\nprint(f\"Rank: {tf.rank(X)}\")\n\nRank: 4\n\n\n\n\nTensor Operations\n\nX = np.array([[5,3],[6,8],[2,10]])\nprint(f\"X*2 = \\n{X*2}\\n  X+2 =\\n{X+2}\")\n\nX*2 = \n[[10  6]\n [12 16]\n [ 4 20]]\n  X+2 =\n[[ 7  5]\n [ 8 10]\n [ 4 12]]\n\n\n\nX = torch.tensor([[5,3],[6,8],[2,10]])\nprint(f\"X*2 = \\n{torch.mul(X,2)}\\n  X+2 =\\n{torch.add(X,2)}\")\n\nX*2 = \ntensor([[10,  6],\n        [12, 16],\n        [ 4, 20]])\n  X+2 =\ntensor([[ 7,  5],\n        [ 8, 10],\n        [ 4, 12]])\n\n\n\n\nElementwise Product\nIf tensors have the same size, operations can be aplied elementwise, this is the Hadamard product: \\boldsymbol X \\odot \\boldsymbol Y It produces a tensor that is the same shape as the input, unlike the dot product which will produce a scalar value.\n\nX = np.array([[5,3],[6,8],[2,10]])\nY = X + 5\nprint(X*Y) ##dot product, not matrix multiplication\n\n[[ 50  24]\n [ 66 104]\n [ 14 150]]\n\n\n\n\nTensor Reduction\n\nprint(X.sum(), torch.sum(torch.tensor([[5,3],[6,8],[2,10]])))\nprint(tf.reduce_sum(tf.constant([[5,3],[6,8],[2,10]])))\nprint(X.sum(axis=0), X.sum(axis=1)) #along specific axes\nprint(torch.sum(torch.tensor([[5,3],[6,8],[2,10]]),0))\nprint(torch.sum(torch.tensor([[5,3],[6,8],[2,10]]),1))\nprint(tf.reduce_sum(tf.constant([[5,3],[6,8],[2,10]]),0))\nprint(tf.reduce_sum(tf.constant([[5,3],[6,8],[2,10]]),1))\n\n34 tensor(34)\ntf.Tensor(34, shape=(), dtype=int32)\n[13 21] [ 8 14 12]\ntensor([13, 21])\ntensor([ 8, 14, 12])\ntf.Tensor([13 21], shape=(2,), dtype=int32)\ntf.Tensor([ 8 14 12], shape=(3,), dtype=int32)\n\n\n\n\nDot Product\nThe vectors must be of the same length or shape for the element-wise multiplication to occurr.\n\nX = np.array([5,3,6,8,2,10])\nY = X + 1\nprint(np.dot(X,Y))\n\n272\n\n\n\nprint(torch.dot(torch.tensor([5,3,6,8,2,10]),torch.add(torch.tensor([5,3,6,8,2,10]),1)))\n\ntensor(272)\n\n\n\nprint(tf.reduce_sum(tf.multiply(tf.constant([5,3,6,8,2,10]),tf.add(tf.constant([5,3,6,8,2,10]),1))))\n\ntf.Tensor(272, shape=(), dtype=int32)\n\n\n\n\n\nMatrices\n\nFrobenius Norm\n\nX = np.array([[5,3],[6,8],[2,10]])\nprint(np.linalg.norm(X))\nprint(torch.norm(torch.tensor([[5.,3],[6,8],[2,10]])))\nprint(tf.norm(tf.constant([[5.,3],[6,8],[2,10]])))\n\n15.427248620541512\ntensor(15.4272)\ntf.Tensor(15.427248, shape=(), dtype=float32)\n\n\n\n\nMatrix Multiplication\n\nA = np.array([[3,4],[5,6],[7,8]])\nb = np.array([1,2])\nprint(np.dot(A,b)) #infers dot product vs matrix multiplication based on shape\nC = torch.tensor([[3,4],[5,6],[7,8]])\nd = torch.tensor([1,2])\nprint(torch.matmul(C,d))\nE = tf.constant([[3,4],[5,6],[7,8]])\nf = tf.constant([1,2])\nprint(tf.linalg.matvec(E,f))\n\ntensor([11, 17, 23])\ntf.Tensor([11 17 23], shape=(3,), dtype=int32)\n\n\n\nA = np.array([[3,4],[5,6],[7,8]])\nb = np.array([[1,9],[2,0]])\nprint(np.dot(A,b)) \nC = torch.tensor([[3,4],[5,6],[7,8]])\nd = torch.tensor([[1,9],[2,0]])\nprint(torch.matmul(C,d))\nE = tf.convert_to_tensor(A)\nf = tf.convert_to_tensor(b)\nprint(tf.matmul(E,f))\n\n[[11 27]\n [17 45]\n [23 63]]\ntensor([[11, 27],\n        [17, 45],\n        [23, 63]])\ntf.Tensor(\n[[11 27]\n [17 45]\n [23 63]], shape=(3, 2), dtype=int64)\n\n\n\n\nMatrix Inversion\n\nX = np.array([[4,2],[-5,-3]])\nX_inv = np.linalg.inv(X)\ny = np.array([4,-7])\nprint(w:= np.dot(X_inv,y))\nprint(np.dot(X,w))\n\n[-1.  4.]\n[ 4. -7.]"
  },
  {
    "objectID": "posts/2022-04-30-linear-algebra-intro.html",
    "href": "posts/2022-04-30-linear-algebra-intro.html",
    "title": "Introduction to Linear Algebra",
    "section": "",
    "text": "Linear algebra deals with linear equations (no polynomial terms). However, it can still be used for approximations for non-linear systems. Uses in ML include solving for unknowns e.g.Â deep learning, dimensionality reduction e.g.Â PCA, ranking e.g.Â eigenvectors, associated recommendations and NLP e.g.Â SVD or matrix factorisation.\nAn example linear equation with many unknowns in a regression model of house prices:\n\\large y = \\alpha + \\beta x{_1} + \\gamma x{_2} + ... + mx{_m}\ny = price, \\alpha = y-intercept, \\beta = number of rooms, \\gamma = area, x = feature\nWhen dealing with linear systems of equations, there can only be three types of solutions, none, one or infinite. Plotting the equations, if the lines are parallel, they will never intersect i.e.Â no solution, if the lines are overlapping, there are infinite solutions, else the lines will intersect at one point only (it is impossible for straight lines to intersect multiple times).\nUsing the house price regression model, we will have as many rows(n) of these equations as we have house prices, each with its m_{-1} features(x) (m-1 due to alpha), and m+1 unknowns(\\alpha,\\beta, \\gamma) which is what we want to solve for:\n y = \\alpha + \\beta x{_1} + \\gamma x{_2} + ... + m_{-1}x_{m_{-1}} \n\n\\begin{bmatrix}\ny{_1} |\\alpha + \\beta x{_{1,1}} + \\gamma x{_{1,2}} + ... + m_{-1}x{_{1,m_{-1}}} \\\\\ny{_2} | \\alpha + \\beta x{_{2,1}} + \\gamma x{_{2,2}} + ... + m_{-1}x{_{2,m_{-1}}} \\\\\n\\dots \\\\\ny{_n} |  \\alpha + \\beta x{_{n,1}} + \\gamma x{_{n,2}} + ... + m_{-1}x{_{n,m_{-1}}}\n\\end{bmatrix}\n\nThis is a set (denoted by square brackets) of linear equations.\n\n\nTensors are a generalisation of vectors and matrices to any number of dimensions.\n\n\n\nDimension\nName\nDescription\n\n\n\n\n0\nScalar\nmagnitude (value)\n\n\n1\nVector\narray\n\n\n2\nMatrix\n2d array (square)\n\n\n3\n3-Tensor\n3d array (cube)\n\n\nn\nn-Tensor\nmultidimensional\n\n\n\n\n\nSingle value, no dimensions, denoted by lower case italic variable names e.g.Â x\n\n\n\n1-Dimensional ordered array of values, lower case bold italic name e.g.Â \\large \\boldsymbol{x}\nElements are scalars, denoted non-bold lower case x with their index e.g.Â x{_1}\n\n\n\\begin{bmatrix} x{_1} & x{_2} & x{_3} \\end{bmatrix}{^T} = $\n\\begin{bmatrix}\nx{_1} \\\\\nx{_2} \\\\\nx{_3}\n\\end{bmatrix}\n$\nRow vector of shape (1,3) transposed to a column vector shape (3,1).\n\n\n\nNorms are functions that quantify vector magnitude (length).\n\\begin{bmatrix}  x{_1} & x{_2} \\end{bmatrix} = \\begin{bmatrix}  5 & 7 \\end{bmatrix} also represents a magnitude and direction from the origin.\n\\large L{^2} Norm ||\\boldsymbol{x}||{_2} = \\sqrt{\\sum_{\\substack{i}}x{_i}{^2}}\nSquare each element in the vector, sum them, then take the square root. Measures euclidean distance from the origin. Also commonly denoted \\large||\\boldsymbol{x}||.\n\\large L{^1} Norm ||\\boldsymbol{x}||{_1} = \\sum_{\\substack{i}}|x{_i}|\nSum the absolute values in the vector. Varies linearly at all locations (not origin dependent). Used when differences between zero and non-zero are needed.\nSquared \\large L{^2} Norm ||\\boldsymbol{x}||{_2^2} = \\sum_{\\substack{i}}x{_i}{^2}\nSimilar to \\large L{^2} Norm, except we donâ€™t take the root, just return the squared value. Computationally cheaper than \\large L{^2} Norm as sqaured \\large L{^2} Norm = \\boldsymbol{x}{^T}\\boldsymbol{x}. Derivative of element x only requires that element, vs \\large L{^2} Norm which requires the vector. However, squared \\large L{^2} Norm grows slowly near the origin so canâ€™t be used if zero/near-zero distinguishing is required.\n\\large L{^\\infin} Norm (or Max Norm) ||\\boldsymbol{x}||{_\\infin} = max{_i}|x{_i}|\nReturn maximum absolute value of the vector.\nGeneralised \\large L{^p} Norm ||\\boldsymbol{x}||{_p} = (\\sum_{\\substack{i}}|x{_i}|{^p}){^\\frac{1}{p}} for p being a real number \\geqslant 1\nNorms, particularly \\large L{^1} and \\large L{^2} norms, are used to regularise objective functions\n\n\n\nUnit vector is a special case where the L2 norm (length) is equal to one i.e.Â \\boldsymbol{x} is a unit vector with unit norm. \\large||\\boldsymbol{x}|| = 1\n\n\n\nBasis vectors are vectors that can be scaled to represent any vector in a given vector space, typically unit vectors are used along the axes of the vector space. For example, the basis vectors i = (1,0) and j=(0,1), then we can represent a vector \\boldsymbol v as follows: \\boldsymbol v = 0.5 i + 3j\n\n\n\nSets of vectors where the dot product of the vectors are zero: \\boldsymbol{x}{^T}\\boldsymbol{y} = 0 i.e.Â assuming they are of non-zero length, they are at 90\\degree angles i.e.Â perpendicular. For an n-dimensional space, there are a maximum of n mutually orthogonal vectors.\n\n\nOrthonormal vectors are orthogonal and have unit norm e.g.Â basis vectors.\n\n\n\n\n\nDenoted uppercase, italics and bold \\boldsymbol X = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\nNotation is (n_{row},n_{col})\nIndividual scalar elements denoted upper case italics only, e.g. \\boldsymbol X = \\begin{bmatrix} X_{1,1} & X_{1,2} \\\\ X_{2,1} & X_{2,2} \\end{bmatrix}\n\n\n\nUpper-case, bold, italic, sans-serif font e.g.Â ð™“ For a 4-tensor ð™“, an element at position (i,j,k,l) would be denoted as ð™“_{(i,j,k,l)}"
  },
  {
    "objectID": "posts/2022-04-30-linear-algebra-intro.html#tensors",
    "href": "posts/2022-04-30-linear-algebra-intro.html#tensors",
    "title": "Introduction to Linear Algebra",
    "section": "",
    "text": "Tensors are a generalisation of vectors and matrices to any number of dimensions.\n\n\n\nDimension\nName\nDescription\n\n\n\n\n0\nScalar\nmagnitude (value)\n\n\n1\nVector\narray\n\n\n2\nMatrix\n2d array (square)\n\n\n3\n3-Tensor\n3d array (cube)\n\n\nn\nn-Tensor\nmultidimensional\n\n\n\n\n\nSingle value, no dimensions, denoted by lower case italic variable names e.g.Â x\n\n\n\n1-Dimensional ordered array of values, lower case bold italic name e.g.Â \\large \\boldsymbol{x}\nElements are scalars, denoted non-bold lower case x with their index e.g.Â x{_1}\n\n\n\\begin{bmatrix} x{_1} & x{_2} & x{_3} \\end{bmatrix}{^T} = $\n\\begin{bmatrix}\nx{_1} \\\\\nx{_2} \\\\\nx{_3}\n\\end{bmatrix}\n$\nRow vector of shape (1,3) transposed to a column vector shape (3,1).\n\n\n\nNorms are functions that quantify vector magnitude (length).\n\\begin{bmatrix}  x{_1} & x{_2} \\end{bmatrix} = \\begin{bmatrix}  5 & 7 \\end{bmatrix} also represents a magnitude and direction from the origin.\n\\large L{^2} Norm ||\\boldsymbol{x}||{_2} = \\sqrt{\\sum_{\\substack{i}}x{_i}{^2}}\nSquare each element in the vector, sum them, then take the square root. Measures euclidean distance from the origin. Also commonly denoted \\large||\\boldsymbol{x}||.\n\\large L{^1} Norm ||\\boldsymbol{x}||{_1} = \\sum_{\\substack{i}}|x{_i}|\nSum the absolute values in the vector. Varies linearly at all locations (not origin dependent). Used when differences between zero and non-zero are needed.\nSquared \\large L{^2} Norm ||\\boldsymbol{x}||{_2^2} = \\sum_{\\substack{i}}x{_i}{^2}\nSimilar to \\large L{^2} Norm, except we donâ€™t take the root, just return the squared value. Computationally cheaper than \\large L{^2} Norm as sqaured \\large L{^2} Norm = \\boldsymbol{x}{^T}\\boldsymbol{x}. Derivative of element x only requires that element, vs \\large L{^2} Norm which requires the vector. However, squared \\large L{^2} Norm grows slowly near the origin so canâ€™t be used if zero/near-zero distinguishing is required.\n\\large L{^\\infin} Norm (or Max Norm) ||\\boldsymbol{x}||{_\\infin} = max{_i}|x{_i}|\nReturn maximum absolute value of the vector.\nGeneralised \\large L{^p} Norm ||\\boldsymbol{x}||{_p} = (\\sum_{\\substack{i}}|x{_i}|{^p}){^\\frac{1}{p}} for p being a real number \\geqslant 1\nNorms, particularly \\large L{^1} and \\large L{^2} norms, are used to regularise objective functions\n\n\n\nUnit vector is a special case where the L2 norm (length) is equal to one i.e.Â \\boldsymbol{x} is a unit vector with unit norm. \\large||\\boldsymbol{x}|| = 1\n\n\n\nBasis vectors are vectors that can be scaled to represent any vector in a given vector space, typically unit vectors are used along the axes of the vector space. For example, the basis vectors i = (1,0) and j=(0,1), then we can represent a vector \\boldsymbol v as follows: \\boldsymbol v = 0.5 i + 3j\n\n\n\nSets of vectors where the dot product of the vectors are zero: \\boldsymbol{x}{^T}\\boldsymbol{y} = 0 i.e.Â assuming they are of non-zero length, they are at 90\\degree angles i.e.Â perpendicular. For an n-dimensional space, there are a maximum of n mutually orthogonal vectors.\n\n\nOrthonormal vectors are orthogonal and have unit norm e.g.Â basis vectors.\n\n\n\n\n\nDenoted uppercase, italics and bold \\boldsymbol X = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\nNotation is (n_{row},n_{col})\nIndividual scalar elements denoted upper case italics only, e.g. \\boldsymbol X = \\begin{bmatrix} X_{1,1} & X_{1,2} \\\\ X_{2,1} & X_{2,2} \\end{bmatrix}\n\n\n\nUpper-case, bold, italic, sans-serif font e.g.Â ð™“ For a 4-tensor ð™“, an element at position (i,j,k,l) would be denoted as ð™“_{(i,j,k,l)}"
  },
  {
    "objectID": "posts/2022-04-30-tensors.html",
    "href": "posts/2022-04-30-tensors.html",
    "title": "Tensors and Tensor Operations",
    "section": "",
    "text": "Transpose of a scalar is the scalar itself i.e.Â x{^T} = x\n\n\n\nTransposing a vector transofrms columns to rows, and rows to columns i.e.\n\n\\begin{bmatrix} x{_{1,1}} & x{_{1,2}} \\\\ x{_{2,1}} & x{_{2,2}} \\\\ x{_{3,1}} & x{_{3,2}}\n\\end{bmatrix}{^T} =\n\\begin{bmatrix}\nx{_{1,1}} & x{_{2,1}} & x{_{3,1}} \\\\\nx{_{1,2}} & x{_{2,2}} & x{_{3,2}}\n\\end{bmatrix}\n\nScalar and vector transpoition are special cases of matrix transposition, where the axes are â€œflippedâ€ across the main diagonal.\n(\\boldsymbol X^T)_{i,j} = (\\boldsymbol X)_{j,i} \n\n\n\n\n\n\nCalculate the sum across all elements of the vector or matrix.\nVector ${i=1}^n x_i $\nMatrix ${i=1}^m {j=1}^n X{j,i} $\n\n\n\n\nDifferent to the Hadamard product as there is summation in the dot product (or scalar product) to produce a single scalar value.\nCan calculate the dot product if two vectors tensors x and y have the same length n and can be denoted as: \\boldsymbol X\\cdot \\boldsymbol Y ,\\boldsymbol X^T \\boldsymbol Y or \\lang \\boldsymbol X,\\boldsymbol Y \\rang. The product is calculated in an elemnent-wise pattern, then summed across the products to produce a scalar value:\n\\boldsymbol X \\cdot \\boldsymbol Y = \\sum_{i=1}^n x_iy_i"
  },
  {
    "objectID": "posts/2022-04-30-tensors.html#tensor-transposition",
    "href": "posts/2022-04-30-tensors.html#tensor-transposition",
    "title": "Tensors and Tensor Operations",
    "section": "",
    "text": "Transpose of a scalar is the scalar itself i.e.Â x{^T} = x\n\n\n\nTransposing a vector transofrms columns to rows, and rows to columns i.e.\n\n\\begin{bmatrix} x{_{1,1}} & x{_{1,2}} \\\\ x{_{2,1}} & x{_{2,2}} \\\\ x{_{3,1}} & x{_{3,2}}\n\\end{bmatrix}{^T} =\n\\begin{bmatrix}\nx{_{1,1}} & x{_{2,1}} & x{_{3,1}} \\\\\nx{_{1,2}} & x{_{2,2}} & x{_{3,2}}\n\\end{bmatrix}\n\nScalar and vector transpoition are special cases of matrix transposition, where the axes are â€œflippedâ€ across the main diagonal.\n(\\boldsymbol X^T)_{i,j} = (\\boldsymbol X)_{j,i}"
  },
  {
    "objectID": "posts/2022-04-30-tensors.html#tensor-reduction",
    "href": "posts/2022-04-30-tensors.html#tensor-reduction",
    "title": "Tensors and Tensor Operations",
    "section": "",
    "text": "Calculate the sum across all elements of the vector or matrix.\nVector ${i=1}^n x_i $\nMatrix ${i=1}^m {j=1}^n X{j,i} $"
  },
  {
    "objectID": "posts/2022-04-30-tensors.html#dot-product",
    "href": "posts/2022-04-30-tensors.html#dot-product",
    "title": "Tensors and Tensor Operations",
    "section": "",
    "text": "Different to the Hadamard product as there is summation in the dot product (or scalar product) to produce a single scalar value.\nCan calculate the dot product if two vectors tensors x and y have the same length n and can be denoted as: \\boldsymbol X\\cdot \\boldsymbol Y ,\\boldsymbol X^T \\boldsymbol Y or \\lang \\boldsymbol X,\\boldsymbol Y \\rang. The product is calculated in an elemnent-wise pattern, then summed across the products to produce a scalar value:\n\\boldsymbol X \\cdot \\boldsymbol Y = \\sum_{i=1}^n x_iy_i"
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html",
    "href": "posts/2023-04-01-introduction-program-synthesis.html",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Generally, compilers transform source code from the source language to the target language, usually to create an executable program, whereas program synthesisers search for a program that will satisfy some stated requirements. They both take high-level specifications to generate software, but synthesisers can search over a solution space to discover how to generate a program to satisfy the requirements (some compiler optimisations also do this e.g.Â autotuning). A program in this context, is a description of how to perform a computation. It is useful to constrain the solution to narrower notation than programming languages such as Domain Specific Languages (DSL) to avoid using specific constructs, rather build using a limited set of functions. Functional programming notation is useful for synthesis as we avoid side effects which can simplify the reasoning process, and can be expressed concisely.\n## Synthesis Program synthesis will generate a program that will solve the stated problem, within a specified program solution space i.e.Â with control over the space of programs, not just the intended behaviour. An example is flash fill (Excel) which derives and applies a small program to the rest of the data. The order can also be rveersed e.g.Â start with a program and search for the specification i.e.Â reverse engineering e.g.Â Verified Lifting - discovering a high level representation that is provably equivalent to an implementation which can then be uo generate a more efficient version of the program. There are 3 major challenges; intention, invention and adaptation to consider.\n\n\n\nSemantic and syntactic constraints: form of user input will influence the synthesis system e.g.Â input-output examples like flash fill may not be suitable. May need a multimodal approach with some examples or abstract examples combined with logical specifications, to provide enough data about intended behaviour.\nUnder specification: if there are multiple solutions that satisfy requirements, which one should be chosen?\n\n\n\n\n\nDiscovering the code to satisfy the requirements\n\n\n\n\n\nThe application of synthesis to broader software development e.g.Â bug fixing, optimisation and other maintenance or existing codebase tasks.\n\n\n\n\n\nInductive synthesis generates a function that matches provided input-output examples. ### Programming by example (PBE) and Programming by demonstration (PBD) In PBE, just the input-output is provided, which can leave a large potential solution space e.g.Â f(1) = 1 which could be addition, division, multiplication etc.\nIn PBD, a trace of the computation performed is also provided to provide additional specification which makes it easier to infer the intended program, e.g.Â f(1) = 1 \\times 1 = 1\nPBD and PBE can still be under-specificed with a large solution program space; need to address how to find the program that matches the observations, and how to know that the program that the user required is the one that was found. Arbritrary spaces can also be searched e.g.Â a large but highly constrained space, allowing exclusion of undesirable solutions, but would still likely require ranking of solutions.\n\n\n\nNeed to represent code as a data structure - Abstrac Syntax Trees (AST) are commonly used for this as they usually follow the same structure of the parse tree of the program, whilst ignoring additional characters e.g.Â parentheses, hence abstract. Context free grammar notation is often used to describe ASTs, to represent the structure of DSLs.\n\n\n\n\n\nExplicitly construct programs until one satisfies the observations, ideally avoiding the generation of programs that are not viable e.g. cannot satisfy the observations or can be proven redundant from existing found solutions.\n- Bottom-Up: discover low level components then ho to assemble them into programs i.e. leaf nodes up.\n- Top-Down: discover the high level structure first, then enumerate to the low level components i.e. root down.\nThe synthesiser will maintain a partial or completely constructed program that is being evaluated e.g. would evaluate potential solutions until success.\n\n\n\nIn symbolic searching, the synthesiser will maintain a symbolic representation of the space of all programs that are considered valid e.g. Version Space Algebras and Constraint Systems.\nA symbolic search may perform some algebraeic manipulation to deduce results which may perform better in some cases, but not all e.g. binary search being effective in finding solutions where algebraeic manipulation may be too costly.\n\n\n\n\n\n\nOne way to simplify the solution space is to define or use a small DSL, then consider all possible valid programs within this language, using context free grammar to describe the ASTs. This makes it simple to enumerate all (or randomly sample) programs in a small DSL. If the DSL has a type system, this can help eliminate solutions which violate type rules. ### Constraint Based Constraint based approaches utilise parametric representations of the space - different choices of paramters correspond to different forms of the program. Paramtetric representations can be more general than grammars - grammars can be encoded in paramteric representation provided a boundary on the length of the program. Paramteric programs can also be referred to as generative models, particularly when the free parameter choice is associated with probabilities. ### Symmetry If there are many ways to represent the same program, the program space has lots of symmetry. Symmetry can be reduced with reduction of cummtativity e.g.Â forcing right or left associativity. Constraint and enumeration based strategies can benefit from symmetry reduction, though there are additional techniques that are mostly removed from this issue. For example $expr := var N | expr + expr $ has a greater soltuion space than $expr := var N | (var * N) + expr $ which forces right associativity."
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html#compilation-vs-synthesis",
    "href": "posts/2023-04-01-introduction-program-synthesis.html#compilation-vs-synthesis",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Generally, compilers transform source code from the source language to the target language, usually to create an executable program, whereas program synthesisers search for a program that will satisfy some stated requirements. They both take high-level specifications to generate software, but synthesisers can search over a solution space to discover how to generate a program to satisfy the requirements (some compiler optimisations also do this e.g.Â autotuning). A program in this context, is a description of how to perform a computation. It is useful to constrain the solution to narrower notation than programming languages such as Domain Specific Languages (DSL) to avoid using specific constructs, rather build using a limited set of functions. Functional programming notation is useful for synthesis as we avoid side effects which can simplify the reasoning process, and can be expressed concisely.\n## Synthesis Program synthesis will generate a program that will solve the stated problem, within a specified program solution space i.e.Â with control over the space of programs, not just the intended behaviour. An example is flash fill (Excel) which derives and applies a small program to the rest of the data. The order can also be rveersed e.g.Â start with a program and search for the specification i.e.Â reverse engineering e.g.Â Verified Lifting - discovering a high level representation that is provably equivalent to an implementation which can then be uo generate a more efficient version of the program. There are 3 major challenges; intention, invention and adaptation to consider.\n\n\n\nSemantic and syntactic constraints: form of user input will influence the synthesis system e.g.Â input-output examples like flash fill may not be suitable. May need a multimodal approach with some examples or abstract examples combined with logical specifications, to provide enough data about intended behaviour.\nUnder specification: if there are multiple solutions that satisfy requirements, which one should be chosen?\n\n\n\n\n\nDiscovering the code to satisfy the requirements\n\n\n\n\n\nThe application of synthesis to broader software development e.g.Â bug fixing, optimisation and other maintenance or existing codebase tasks."
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html#inductive-synthesis",
    "href": "posts/2023-04-01-introduction-program-synthesis.html#inductive-synthesis",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Inductive synthesis generates a function that matches provided input-output examples. ### Programming by example (PBE) and Programming by demonstration (PBD) In PBE, just the input-output is provided, which can leave a large potential solution space e.g.Â f(1) = 1 which could be addition, division, multiplication etc.\nIn PBD, a trace of the computation performed is also provided to provide additional specification which makes it easier to infer the intended program, e.g.Â f(1) = 1 \\times 1 = 1\nPBD and PBE can still be under-specificed with a large solution program space; need to address how to find the program that matches the observations, and how to know that the program that the user required is the one that was found. Arbritrary spaces can also be searched e.g.Â a large but highly constrained space, allowing exclusion of undesirable solutions, but would still likely require ranking of solutions."
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html#programming",
    "href": "posts/2023-04-01-introduction-program-synthesis.html#programming",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Need to represent code as a data structure - Abstrac Syntax Trees (AST) are commonly used for this as they usually follow the same structure of the parse tree of the program, whilst ignoring additional characters e.g.Â parentheses, hence abstract. Context free grammar notation is often used to describe ASTs, to represent the structure of DSLs."
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html#search-techniques",
    "href": "posts/2023-04-01-introduction-program-synthesis.html#search-techniques",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "Explicitly construct programs until one satisfies the observations, ideally avoiding the generation of programs that are not viable e.g. cannot satisfy the observations or can be proven redundant from existing found solutions.\n- Bottom-Up: discover low level components then ho to assemble them into programs i.e. leaf nodes up.\n- Top-Down: discover the high level structure first, then enumerate to the low level components i.e. root down.\nThe synthesiser will maintain a partial or completely constructed program that is being evaluated e.g. would evaluate potential solutions until success.\n\n\n\nIn symbolic searching, the synthesiser will maintain a symbolic representation of the space of all programs that are considered valid e.g. Version Space Algebras and Constraint Systems.\nA symbolic search may perform some algebraeic manipulation to deduce results which may perform better in some cases, but not all e.g. binary search being effective in finding solutions where algebraeic manipulation may be too costly."
  },
  {
    "objectID": "posts/2023-04-01-introduction-program-synthesis.html#program-solution-space",
    "href": "posts/2023-04-01-introduction-program-synthesis.html#program-solution-space",
    "title": "Introduction to Program Synthesis",
    "section": "",
    "text": "One way to simplify the solution space is to define or use a small DSL, then consider all possible valid programs within this language, using context free grammar to describe the ASTs. This makes it simple to enumerate all (or randomly sample) programs in a small DSL. If the DSL has a type system, this can help eliminate solutions which violate type rules. ### Constraint Based Constraint based approaches utilise parametric representations of the space - different choices of paramters correspond to different forms of the program. Paramtetric representations can be more general than grammars - grammars can be encoded in paramteric representation provided a boundary on the length of the program. Paramteric programs can also be referred to as generative models, particularly when the free parameter choice is associated with probabilities. ### Symmetry If there are many ways to represent the same program, the program space has lots of symmetry. Symmetry can be reduced with reduction of cummtativity e.g.Â forcing right or left associativity. Constraint and enumeration based strategies can benefit from symmetry reduction, though there are additional techniques that are mostly removed from this issue. For example $expr := var N | expr + expr $ has a greater soltuion space than $expr := var N | (var * N) + expr $ which forces right associativity."
  },
  {
    "objectID": "posts/2023-04-03-inductive-program-synthesis.html",
    "href": "posts/2023-04-03-inductive-program-synthesis.html",
    "title": "Inductive Program Synthesis",
    "section": "",
    "text": "A simple implementation would be to construct all possible programs from the terminals of a grammar. This can be inefficient as the space of all expressions can grow quickly. Observationally equivalent programs are discarded (i.e.Â if the same output is generated based on the same input, the primitive can be discarded), which reduces the solution space exponentially.\nfunction synthesise (inputs, outputs):\n  terms := set_of_all_terminals\n  while(true):\n    terms := grow(terms)\n    terms := eliminate_observational_equivalents(terms, inputs);\n    foreach(t in terms):\n      if(is_correct(t, inputs, outputs)):\n        return t;\nOne characteristic is that bottom-up search explores small programs before larger ones, so it can potentially find the smallest program that satisfies the specification. Further heuristics can be applied in the grow or eliminate function level to direct the search, and it copes well with symmetry reduction. However, even with aggressive pruning, it is hard to scale beyond small lists of terms, and whilst it can connect discrete components, it is not effective at determining constants. It also may fail at pruning if the language has context dependant semantics e.g.Â if the same expression had dufferent values in different contexts e.g.Â via mutation.\n\n\n\nBy modularising the search, i.e.Â search for multiple programs that work for different inputs and find a way to combine them that will work for all inputs, the scalability can be improved. The first step is a best-effort synthesis (if it works on all then it is just returned), then it can try to either improve the current program by taking a selection of the currently failing inputs alongside the correctly mapped inputs, or STUN can be called recursively on the inputs where the synthesised program failed. Then the programs can be combined. #### Anti/Unification If there are no top-level branches to allow unification, antiunification can be used. Unification is finding a common structure for two different expressions by replacing variables with expressions. Antiunification is to process of finding the common structure by replacing expressions with variables. For example, if we find 2 expressions a \\times c and b \\times c cover all inputs, antiunification can produce a common expression v \\times c where v stands for a code fragment that can be solved, being a much smaller synthesis problem. When a recursive call to STUN is made, additional constraints can also be passed that the expression must satisfy to avoid situations where, for example, anntiunification cannot be used e.g.Â b \\times c would be chosen over -b as b \\times c can be antiunified with a \\times c. \\bigoplus represents unification.\n\n\n\nModularisation may also be performed if the program can be split into different levels of abstraction, and the search performed independently at each level. A potential optimisation here is if we can eliminate branches where the output is not a superset of the input set.\n\n\n\nTop down search can utilise types to assist in pruning invalid programs efficiently. A basic algorithm will start from the grammar rules and test with the inputs, pruning out programs that are fully expanded which donâ€™t return the correct output or which return incompatible types. Some languages will have infinite types, e.g.Â for supporting nested lists and functions. Typing rules have the following form:  \\frac{premises}{context âŠ¢ expr : \\tau }  âŠ¢ = proves or satisfies or is derived or assuming expr will have type \\tau in a given context as long as all the premises are satisfied. Alternatively:  \\frac{conditions}{Examples, âŠ¢ expression : \\tau } \nAn example typing rule might be:  \\frac{\\bold{\\textit C},x : \\tau_1 âŠ¢ expr : \\tau_2}{\\bold{\\textit C} âŠ¢ \\bold{\\lambda}x.expr : \\tau_1 \\to \\tau_2 }  states the following: \\bold{\\lambda}x.expr will have type \\tau_1 \\to \\tau_2 if it can be shown that expr has type \\tau_2 in a context that is like the context \\bold{\\textit C} but that also has x as having type \\tau_1.\nThe search space can also be further restricted in further iterations/generations for functions like map; if the expected output is an array of integer arrays, the first expression must correspond to a function with a type of integer array, allowing filtering of programs that wonâ€™t have the desired type before evaluating any concrete input values.\n\n\nType rules are a form of deductive rule; information about inputs/outputs are propogated to potential sub-expressions. Additional rules for different constructs in the language can prune the solution space. Given a candidate expression with an unknown subexpression \\textit{\\bold f} and a set of input-outputs, the input-outputs can be propogated to the subexpression or establish that this line is not viable. Rules can inform the search if a candidate is not going to work e.g.Â map will always return a list of the same length so if the input length is different, map alone will not be viable. They can also provide information on how to propogate input-outputs to new expressions. It may not work when expressions involve functions e.g.Â if the same input value is mapped to multiple output values."
  },
  {
    "objectID": "posts/2023-04-03-inductive-program-synthesis.html#explicit",
    "href": "posts/2023-04-03-inductive-program-synthesis.html#explicit",
    "title": "Inductive Program Synthesis",
    "section": "",
    "text": "A simple implementation would be to construct all possible programs from the terminals of a grammar. This can be inefficient as the space of all expressions can grow quickly. Observationally equivalent programs are discarded (i.e.Â if the same output is generated based on the same input, the primitive can be discarded), which reduces the solution space exponentially.\nfunction synthesise (inputs, outputs):\n  terms := set_of_all_terminals\n  while(true):\n    terms := grow(terms)\n    terms := eliminate_observational_equivalents(terms, inputs);\n    foreach(t in terms):\n      if(is_correct(t, inputs, outputs)):\n        return t;\nOne characteristic is that bottom-up search explores small programs before larger ones, so it can potentially find the smallest program that satisfies the specification. Further heuristics can be applied in the grow or eliminate function level to direct the search, and it copes well with symmetry reduction. However, even with aggressive pruning, it is hard to scale beyond small lists of terms, and whilst it can connect discrete components, it is not effective at determining constants. It also may fail at pruning if the language has context dependant semantics e.g.Â if the same expression had dufferent values in different contexts e.g.Â via mutation.\n\n\n\nBy modularising the search, i.e.Â search for multiple programs that work for different inputs and find a way to combine them that will work for all inputs, the scalability can be improved. The first step is a best-effort synthesis (if it works on all then it is just returned), then it can try to either improve the current program by taking a selection of the currently failing inputs alongside the correctly mapped inputs, or STUN can be called recursively on the inputs where the synthesised program failed. Then the programs can be combined. #### Anti/Unification If there are no top-level branches to allow unification, antiunification can be used. Unification is finding a common structure for two different expressions by replacing variables with expressions. Antiunification is to process of finding the common structure by replacing expressions with variables. For example, if we find 2 expressions a \\times c and b \\times c cover all inputs, antiunification can produce a common expression v \\times c where v stands for a code fragment that can be solved, being a much smaller synthesis problem. When a recursive call to STUN is made, additional constraints can also be passed that the expression must satisfy to avoid situations where, for example, anntiunification cannot be used e.g.Â b \\times c would be chosen over -b as b \\times c can be antiunified with a \\times c. \\bigoplus represents unification.\n\n\n\nModularisation may also be performed if the program can be split into different levels of abstraction, and the search performed independently at each level. A potential optimisation here is if we can eliminate branches where the output is not a superset of the input set.\n\n\n\nTop down search can utilise types to assist in pruning invalid programs efficiently. A basic algorithm will start from the grammar rules and test with the inputs, pruning out programs that are fully expanded which donâ€™t return the correct output or which return incompatible types. Some languages will have infinite types, e.g.Â for supporting nested lists and functions. Typing rules have the following form:  \\frac{premises}{context âŠ¢ expr : \\tau }  âŠ¢ = proves or satisfies or is derived or assuming expr will have type \\tau in a given context as long as all the premises are satisfied. Alternatively:  \\frac{conditions}{Examples, âŠ¢ expression : \\tau } \nAn example typing rule might be:  \\frac{\\bold{\\textit C},x : \\tau_1 âŠ¢ expr : \\tau_2}{\\bold{\\textit C} âŠ¢ \\bold{\\lambda}x.expr : \\tau_1 \\to \\tau_2 }  states the following: \\bold{\\lambda}x.expr will have type \\tau_1 \\to \\tau_2 if it can be shown that expr has type \\tau_2 in a context that is like the context \\bold{\\textit C} but that also has x as having type \\tau_1.\nThe search space can also be further restricted in further iterations/generations for functions like map; if the expected output is an array of integer arrays, the first expression must correspond to a function with a type of integer array, allowing filtering of programs that wonâ€™t have the desired type before evaluating any concrete input values.\n\n\nType rules are a form of deductive rule; information about inputs/outputs are propogated to potential sub-expressions. Additional rules for different constructs in the language can prune the solution space. Given a candidate expression with an unknown subexpression \\textit{\\bold f} and a set of input-outputs, the input-outputs can be propogated to the subexpression or establish that this line is not viable. Rules can inform the search if a candidate is not going to work e.g.Â map will always return a list of the same length so if the input length is different, map alone will not be viable. They can also provide information on how to propogate input-outputs to new expressions. It may not work when expressions involve functions e.g.Â if the same input value is mapped to multiple output values."
  }
]